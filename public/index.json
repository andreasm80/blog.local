[{"body":"This post will go through the Antrea-native policy resources and how to use them to secure your Kubernetes environment combined with K8s network policies.\nAbbreviations used in this article:\nContainer Network Interface = CNI Antrea Cluster Network Policies = ACNP Antrea Network Policies = ANP Kubernetes Network Policies = K8s policies or KNP When it comes to securing your K8s infrastructure it can be done in several layers in the infrastructure as a whole. This post will focus on the possibilities within the K8s cluster with features in the Antrea CNI. I will go through the Antrea-native policies (Antrea and K8s policies), with examples of when, how and where to use them. As Antrea-native policy resources can be used together with K8s network policies I will show that also.\nThis post will not cover the additional needs of security in your datacenter before reaching your K8s environment. I will cover this in a later post where I go through the use of NSX Distributed Firewall protecting your k8s clusters together with the security policies in Antrea.\nAntrea-native policy resources - short introduction Antrea comes with a comprehensive policy model. We have the Antrea Cluster Network Policies and Antrea Network Policies. The difference being between those two is that the ACNP applies to all objects on the cluster, where ANP is namespaced meaning its applies to objects within the namespace defined in the policy.\nAntrea policies are tiered, meaning the rules will be following an order of precedence. This makes it very useful to divide the rules into the right categories, having different resources in the organization responsible for the security rules. Sec-ops will have their rules in the beginning setting the \u0026quot;ground\u0026quot; before the application owners can set their rules and finally some block all rules that rules out all that is left. Antrea-native policy resources is working together with K8s network policies where the latter is placed in the Application tier below the ANP and ACNP policies. Antrea comes with a set of default tiers as of installation of Antrea, but there is also possible to add custom tiers. Read more here. Here are the default tiers:\n1Emergency -\u0026gt; Tier name \u0026#34;emergency\u0026#34; with priority \u0026#34;50\u0026#34; 2SecurityOps -\u0026gt; Tier name \u0026#34;securityops\u0026#34; with priority \u0026#34;100\u0026#34; 3NetworkOps -\u0026gt; Tier name \u0026#34;networkops\u0026#34; with priority \u0026#34;150\u0026#34; 4Platform -\u0026gt; Tier name \u0026#34;platform\u0026#34; with priority \u0026#34;200\u0026#34; 5Application -\u0026gt; Tier name \u0026#34;application\u0026#34; with priority \u0026#34;250\u0026#34; 6Baseline -\u0026gt; Tier name \u0026#34;baseline\u0026#34; with priority \u0026#34;253\u0026#34; Take a look at the diagram below and imagine your first rules is placed at the first left tier and more rules in the different tiers all the way to the right:\nMaking use of the Antrea-native policy resources In this section I will describe a demo environment with some namespaces and applications and then go through one way of using Antrea-native policy resources together with K8s network policies. In some of my Antrea-native policies I will use namespace selection based on the actual name of the namespace, but in others I will use selection based on labels. The first example below will make use of labels as selection criteria. I will explain why below. To read more on namespace selection: https://github.com/antrea-io/antrea/blob/main/docs/antrea-network-policy.md#select-namespace-by-name\nMeet the demo environment I will only use one K8s cluster in this example which is based on one master worker and two worker nodes.\nI will create three \u0026quot;environments\u0026quot; by using namespaces and label them according to which \u0026quot;environment\u0026quot; they belong to. The three namespaces will be \u0026quot;test-app\u0026quot;, \u0026quot;dev-app\u0026quot; and \u0026quot;prod-app\u0026quot;. I will then add a label on each namespace with the label env=test, env=dev and env=prod accordingly. Within each namespace I will spin up two pods (an Ubuntu 16.04 and Ubuntu 20.04 pod). What I would like to achieve is that each namespace represents their own environment to simulate scenarios where we do have prod, dev and test environments, where none of the environments are allowed to talk to each other. And by using labels, I can create several namespaces and place them into the correct environment by just \u0026quot;tagging\u0026quot; them with the correct labels (e.g env=dev).\nNow in the next section I will go through how I can isolate, and control those environments with Antrea-native policy resources.\nAntrea Cluster Network Policies (ACNP) The first thing I would like to do is to create some kind of basic separation between those environments/namespaces so they cant communicate with each other, and when that is done I can continue to create more granular application policies within each namespace or environment.\nThe first issue I meet is how to create as few rules as possible to just isolate what I know (the tree namespaces which are labeled with three different labels to create my \u0026quot;environments\u0026quot;) without having to worry about additional namespaces being created and those getting access to the \u0026quot;environments\u0026quot;. In this example I have already created three namespaces named \u0026quot;dev-app\u0026quot;, \u0026quot;prod-app\u0026quot; and \u0026quot;test-app\u0026quot;. I \u0026quot;tag\u0026quot; them in Kubernetes with their corresponding \u0026quot;env\u0026quot; labels: \u0026quot;dev\u0026quot;, \u0026quot;prod\u0026quot; and \u0026quot;test\u0026quot;. The reason I choose that approach is that I then can create several namespaces and choose which environment they belong to instead of doing the selection directly on the name of the namespace. I need to create an Antrea Cluster Network Policy as a \u0026quot;default\u0026quot; rule for each of my known environments so I can at a minimum guarantee that within each namespace or environment \u0026quot;intra-traffic\u0026quot; is allowed (traffic within the namespace or namespaces labeled with the same environment label). Meaning that when I do have a complete Antrea-native policy \u0026quot;framework\u0026quot; in place (with a blocking rule at the end taking care of all that is not specified) I can create new namespaces, but if they are not labeled correctly they will not be allowed to talk to any of my environments. This policy is applied at the SecurityOps tier:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: isolate-dev-env 5spec: 6 priority: 5 7 tier: SecurityOps 8 appliedTo: 9 - namespaceSelector: 10 matchLabels: 11 env: dev 12 ingress: 13 - action: Drop 14 from: 15 - namespaceSelector: 16 matchExpressions: 17 - key: env 18 operator: NotIn 19 values: 20 - dev 21 - action: Allow 22 from: 23 - namespaceSelector: 24 matchLabels: 25 env: dev 26 egress: 27 - action: Allow 28 to: 29 - namespaceSelector: 30 matchLabels: 31 env: dev What I am also doing with this ACNP is saying that if you are member of a namespace with the label \u0026quot;env=dev\u0026quot; you are allowed to ingress the namespace Dev, but not if you are not (\u0026quot;operator: NotIn\u0026quot; in the ingress namespaceSelector).\nAlso note that I am allowing specifically an Action allow to the dev environment within the same policy, the reason being is that when I apply my block-all-else rule later on it will block intra traffic within the same environment if it is not specifically specified that it is allowed in this rule.\nNow I just have to recreate this policy for my other two namespaces.\nAlso note that in the egress part I am only allowing traffic to namespace with the lavel \u0026quot;env=dev\u0026quot;. That does not mean right now that I will only allow traffic to anything else, because I don't have any block rules in my cluster yet. Antrea-native policy resources works a bit different than K8s network policies which only supports creating allow policies. In Antrea one can specify both DROP and ALLOW on both INGRESS and EGRESS. I left this with purpose, because I later in will go ahead create a block all rule. Now lets demonstrate this rule:\nBefore applying ACNP namespace isolation rule:\n1kubectl get pod -n dev-app -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3ubuntu-16-04-7f876959c6-p5nxp 1/1 Running 0 9d 10.162.1.57 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4ubuntu-20-04-6fb66c64cb-9qg2p 1/1 Running 0 9d 10.162.1.56 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 1kubectl get pod -n prod-app -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3ubuntu-16-04-7f876959c6-sfdvf 1/1 Running 0 9d 10.162.1.64 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4ubuntu-20-04-6fb66c64cb-z528m 1/1 Running 0 9d 10.162.1.65 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Above I list out the pods with IP addresses in my two namespaces \u0026quot;dev-app\u0026quot; and \u0026quot;prod-app\u0026quot;\nNow I enter bash of the Ubuntu20.04 pod in \u0026quot;dev-app\u0026quot; namespace and do a ping to the second pod in the same namespace and then ping another pod in the namespace Prod:\n1kubectl exec -it -n dev-app ubuntu-20-04-6fb66c64cb-9qg2p bash 1root@ubuntu-20-04-6fb66c64cb-9qg2p:/# ping 10.162.1.57 2PING 10.162.1.57 (10.162.1.57) 56(84) bytes of data. 364 bytes from 10.162.1.57: icmp_seq=1 ttl=64 time=0.896 ms 464 bytes from 10.162.1.57: icmp_seq=2 ttl=64 time=0.520 ms 564 bytes from 10.162.1.57: icmp_seq=3 ttl=64 time=0.248 ms 6 7root@ubuntu-20-04-6fb66c64cb-9qg2p:/# ping 10.162.1.64 8PING 10.162.1.64 (10.162.1.64) 56(84) bytes of data. 964 bytes from 10.162.1.64: icmp_seq=1 ttl=64 time=1.03 ms 1064 bytes from 10.162.1.64: icmp_seq=2 ttl=64 time=0.584 ms 1164 bytes from 10.162.1.64: icmp_seq=3 ttl=64 time=0.213 ms I have also written about Octant in one of my posts, in Octant there is an Antrea plugin which gives us some graphical features such as traceflow, which is also a powerful tool to showcase/troubleshoot security policies. Below is a screenshot from Octant before the rule is applied:\nAs you can see, this is allowed. Now I apply my \u0026quot;isolation\u0026quot; ACNP rules \u0026quot;prod\u0026quot;, \u0026quot;dev\u0026quot; \u0026amp; \u0026quot;test\u0026quot;. Also note; to list out the applied ACNP policies the command \u0026quot;kubetcl get acnp\u0026quot; can be used, without looking in a specific namespace as ACNP is clusterwide.\n1kubectl apply -f isolate.environment.prod.negated.yaml 2clusternetworkpolicy.crd.antrea.io/isolate-prod-env created 3kubectl apply -f isolate.environment.dev.negated.yaml 4clusternetworkpolicy.crd.antrea.io/isolate-dev-env created 5kubectl get acnp 6NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 7isolate-dev-env SecurityOps 5 1 1 19s 8isolate-prod-env SecurityOps 6 1 1 25s After they are applied I will try to do same as above:\n1ping 10.162.1.57 2PING 10.162.1.57 (10.162.1.57) 56(84) bytes of data. 364 bytes from 10.162.1.57: icmp_seq=1 ttl=64 time=3.28 ms 464 bytes from 10.162.1.57: icmp_seq=2 ttl=64 time=0.473 ms 564 bytes from 10.162.1.57: icmp_seq=3 ttl=64 time=0.190 ms 664 bytes from 10.162.1.57: icmp_seq=4 ttl=64 time=0.204 ms 7 8ping 10.162.1.64 9PING 10.162.1.64 (10.162.1.64) 56(84) bytes of data. Pinging within the same namespace works perfect, but to one of the other namespaces (here the Prod namespace) is not allowed. Works as intended.\nDoing the same traceflow with Octant again:\nSo to recap, this is how it looks like now:\nNow that I have created myself some isolated environments, I also need to allow some basic needs from the environments/namespaces to things such as DNS. So I will go ahead and create such a rule. Also have in mind that I haven't yet applied the last block-all-rule (so they can still reach those services as of now). I will make that rule applied when all the necessary rules are in place beforehand. In a greenfield environment those \u0026quot;baseline\u0026quot; rules would probably be applied as the first thing before the k8s cluster is taken into use.\nGoing down one tier to NetworkOps I will apply this Antrea Policy:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: allow-core-dns 5spec: 6 priority: 10 7 tier: NetworkOps 8 appliedTo: 9 - namespaceSelector: {} 10 egress: 11 - action: Allow 12 to: 13 - namespaceSelector: 14 matchLabels: 15 kubernetes.io/metadata.name: kube-system 16 ports: 17 - protocol: TCP 18 port: 53 19 - protocol: UDP 20 port: 53 This policy is probably for some rather \u0026quot;wide\u0026quot; as it just does a \u0026quot;wildcard\u0026quot; selection of all namespaces available and gives them access to the backend kube-system (where the coredns pods are located) on protocol TCP and UDP port 53. But again, this post is just to showcase Antrea policies and how they can be used and to give some insights in general.\nDNS allowed showed with Octant:\nOctant Traceflow\nFor now I am finished with the Cluster Policies and will head over to the Network Policies. This is the ACNP policies applied so far:\n1kubectl get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 3allow-core-dns NetworkOps 10 3 3 19h 4isolate-dev-env SecurityOps 5 1 1 22h 5isolate-prod-env SecurityOps 6 1 1 22h 6isolate-test-env SecurityOps 7 1 1 19h Antrea Network Policies (ANP) Antrea Network Policies are namespaced. So one of the use cases for ANP could be to create rules specific for the services running in the namespace. It could be allowing ingress on certain selections (e.g label/frontend) which runs the application I want to expose or which makes sense for clients to talk to which is the frontend part of the application. Everything else is backend services which not necessary to expose to clients, but on the other hand it could be that those services needs access to other backend services or services in other namespaces. So with ANP one can create ingress/egress policies by using the different selection options defining what is allowed in and out of the namespace.\nBefore I continue I have now applied my ACNP block-rule in the \u0026quot;Baseline\u0026quot; tier which just blocks all else to make sense of the examples used here in this section. Below is the policy (Note that I have excluded some namespaces in this rule) :\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: block-all-whitelist 5spec: 6 priority: 1000 7 tier: baseline 8 appliedTo: 9 - namespaceSelector: 10 matchExpressions: 11 - key: ns 12 operator: NotIn 13 values: 14 - kube-system 15 - monitoring 16 ingress: 17 - action: Drop 18 from: 19 - namespaceSelector: {} 20 - ipBlock: 21 cidr: 0.0.0.0/0 22 egress: 23 - action: Drop 24 to: 25 - namespaceSelector: {} 26 - ipBlock: 27 cidr: 0.0.0.0/0 Antrea Network Policies Egress rule Now that I have applied my \u0026quot;whitelist\u0026quot; rule I must by now have all my necessary rules in place, otherwise things will stop working (such as access to DNS). I will now apply a policy which is \u0026quot;needed\u0026quot; by the \u0026quot;Prod\u0026quot; environment, which is access to SSH on a remote server. So the policy below is allowing Egress on TCP port 22 to this specific remote SSH server. Lets us apply this policy and test how this works out:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: NetworkPolicy 3metadata: 4 name: allow-prod-env-ssh 5 namespace: prod-app 6spec: 7 priority: 8 8 tier: application 9 appliedTo: 10 - podSelector: {} 11 egress: 12 - action: Allow 13 to: 14 - namespaceSelector: 15 matchLabels: 16 kubernetes.io/metadata.name: prod-app 17 - ipBlock: 18 cidr: 10.100.5.10/32 19 ports: 20 - protocol: TCP 21 port: 22 Just some sanity check before applying the above policy, I am still able to reach all pods within the same namespace due to my \u0026quot;isolation ACNP rules\u0026quot; even though I have my block all rule applied.\nBut I am not allowed to reach anything outside except what is stated in my DNS rule. If I try to reach my remote SSH server from my \u0026quot;Prod\u0026quot; namespace I am not allowed. To illustrate this I have entered \u0026quot;remoted\u0026quot; myself into bash on one of my pods in the Prod namespace and trying to ssh the remote server 10.100.5.10, below is the current result:\n1root@ubuntu-20-04-6fb66c64cb-z528m:/# ssh andreasm@10.100.5.10 2ssh: connect to host 10.100.5.10 port 22: Connection timed out Ok, fine. What does my traceflow say about this also:\nNope cant do it also says Traceflow. Great, everything works out as planned. Now I must apply my policy to allow this.\n1root@ubuntu-20-04-6fb66c64cb-z528m:/# ssh andreasm@10.100.5.10 2andreasm@10.100.5.10\u0026#39;s password: Now I can finally reach my remote SSH server. To confirm again, lets check with Octant:\nThank you very much, that was very kind of you.\nTo summarize so far what we have done. We have applied the ACNP rules/policies to create environment/namespace isolation\n1NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 2allow-core-dns NetworkOps 10 3 3 21h 3block-all-whitelist baseline 20 2 2 25m 4isolate-dev-env SecurityOps 5 1 1 24h 5isolate-prod-env SecurityOps 6 1 1 31m 6isolate-test-env SecurityOps 7 1 1 21h And we have applied a rule to allow some basic \u0026quot;needs\u0026quot; such as DNS with the rule allow-core-dns. And the \u0026quot;block-all-whitelist\u0026quot; policy as a \u0026quot;catch all rule\u0026quot; to block everything not specified in the tiers.\nAnd then we have applied a more application/namespace specific policy with Antrea Network Policy to allow Prod to egress 10.100.5.10 on port 22/TCP. But I have not specified any ingress rules allow access to any services in the namespace Prod coming from outside the namespace. So it is a very lonely/isolated environment for the moment. This is how it looks like now:\nIn the next example I will create an ingress rule to another application that needs to be accessed from the outside.\nAntrea Network Policies Ingress rule To make this section a bit more \u0026quot;understanding\u0026quot; I will use another application as example to easier illustrate the purpose. The example I will be using is a demo application I have been using for several years - Yelb link\nThis application contains of four pods and looks like this:\nYelb diagram\nI already have the application up and running in my environment. But as this application is a bit more complex and contains a frontend which is useless if not exposed or reachable I am exposing this frontend with NSX Advanced Load Balancer. This makes it very easy for me to define the ingress rule as it means I only have to allow the load balancers IPs in my egress rule and not all potential IPs. The load balancers IP's is something I know. Some explanation around the load balancer IP's in my environment is that they are spun up on demand and just pick an IP from a pre-defined IP pool, so instead of pinning the ingress rule to the current IP they have I am bit wide and allow the IP range that is defined. Remember that this is a demo environment and does not represent a production environment. Lets take a look at the policy:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: NetworkPolicy 3metadata: 4 name: allow-yelb-frontend 5 namespace: yelb 6spec: 7 priority: 5 8 tier: application 9 appliedTo: 10 - podSelector: 11 matchLabels: 12 app: yelb-ui 13 ingress: 14 - action: Allow 15 from: 16 - ipBlock: 17 cidr: 10.161.0.0/24 18 ports: 19 - protocol: TCP 20 port: 80 21 endPort: 80 22 name: AllowInYelbFrontend 23 enableLogging: false 24 egress: 25 - action: Allow 26 to: 27 - ipBlock: 28 cidr: 10.161.0.0/24 29 name: AllowOutYelbFrontend 30 enableLogging: false The CIDR in the rule above is the range my load balancers is \u0026quot;living\u0026quot; in. So instead to narrow it down too much in this demo I just allow the range 10.161.0.0/24 meaning I dont have to worry too much if they are getting new IP's within this range making my application inaccessible. When I apply this rule it will be placed in the tier \u0026quot;Application\u0026quot; (See one of the first diagrams in the beginning of this post) with a priority of 5. The basic policies for this application is already in place such as DNS and intra-communication (allowed to talk within the same namespace/environment which in this example is yelb-app/test).\nNow lets see how it is before applying the rule from the perspective of the NSX Advanced Load Balancer which is being asked to expose the frontend of the application:\nFrom NSX Advanced Load Balancer GUI / application showing pool is down\nAs one can see from the above screenshot, the Service Engines (the actual load balancers) are up and running but the application yelb-ui is down because the pool is unreachable. The pool here is the actual pod containing the frontend part of the Yelb app. So I need to apply the Antrea Network Policy to allow the Service Engines to talk to my pod. If I try to access the frontend via the load balancer VIP its also inaccessible:\nLets just apply the rule:\n1kubectl apply -f yelb.frontend.allow.yaml 2networkpolicy.crd.antrea.io/allow-yelb-frontend created 3kubectl get anp -n yelb 4NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 5allow-yelb-frontend application 5 1 1 11s And now check the NSX Advanced Load Balancer status page and try to access the application through the VIP:\nNSX ALB is showing green\nWell that looks promising, green is a wonderful colour in IT.\nAnd the application is available:\nYelb UI frontend\nThe rule above only gives the NSX Advanced Load Balancers access to the frontend pod on port 80 of the application Yelb. All the other pods are protected by the \u0026quot;environment\u0026quot; block rule and the default block rule. There is one catch though, we dont have any rules protecting traffic between the pods. Lets say the frontend pod (which is exposed to the outside world) is compromised, there is no rule stopping any traffic coming from this pod to the others within the same namespace/and or environment. That is something we should apply.\nMicrosegmenting the application What we shall do now is to make sure that the pods that make up the application Yelb is only allowed to talk to each other on the necessary ports and nothing else. Meaning we create a policy that does a selection of the pods and apply specific rules for each pod/service within the application, if one refer to the diagram above over the Yelb application one can also see that there is no need for the fronted pod to be allowed to talk to the redis or db pod at all so that should be completely blocked.\nI will go ahead and apply a rule that does all the selection for me, and only allow what is needed for the application to work. The policy I will make use of here is K8s Native Network Policy kubernetes.io\nHere is the rule:\n1apiVersion: networking.k8s.io/v1 2kind: NetworkPolicy 3metadata: 4 name: yelb-cache 5 namespace: yelb 6spec: 7 podSelector: 8 matchLabels: 9 tier: cache 10 ingress: 11 - from: 12 - podSelector: 13 matchLabels: 14 tier: middletier 15 - namespaceSelector: 16 matchLabels: 17 tier: middletier 18 ports: 19 - protocol: TCP 20 port: 6379 21 policyTypes: 22 - Ingress 23--- 24apiVersion: networking.k8s.io/v1 25kind: NetworkPolicy 26metadata: 27 name: yelb-backend 28 namespace: yelb 29spec: 30 podSelector: 31 matchLabels: 32 tier: backenddb 33 ingress: 34 - from: 35 - podSelector: 36 matchLabels: 37 tier: middletier 38 - namespaceSelector: 39 matchLabels: 40 tier: middletier 41 ports: 42 - protocol: TCP 43 port: 5432 44 policyTypes: 45 - Ingress 46--- 47apiVersion: networking.k8s.io/v1 48kind: NetworkPolicy 49metadata: 50 name: yelb-middletier 51 namespace: yelb 52spec: 53 podSelector: 54 matchLabels: 55 tier: middletier 56 ingress: 57 - from: 58 - podSelector: {} 59 - namespaceSelector: 60 matchLabels: 61 tier: frontend 62 ports: 63 - protocol: TCP 64 port: 4567 65 policyTypes: 66 - Ingress 67--- 68apiVersion: networking.k8s.io/v1 69kind: NetworkPolicy 70metadata: 71 name: yelb-frontend 72 namespace: yelb 73spec: 74 podSelector: 75 matchLabels: 76 tier: frontend 77 ingress: 78 - from: 79 ports: 80 - protocol: TCP 81 port: 80 82 egress: 83 - to: 84 ports: 85 - protocol: TCP 86 port: 30567 87 policyTypes: 88 - Ingress 89 - Egress As I have already illustrated above I will not go through showing that the pods can talk to each other on all kinds of port, as they can because they do not have any restriction within the same namespace/environment. What I will go through though is how the above policy affects my application.\nThe rule applied:\n1kubectl apply -f k8snp_yelb_policy.yaml 2networkpolicy.networking.k8s.io/yelb-cache created 3networkpolicy.networking.k8s.io/yelb-backend created 4networkpolicy.networking.k8s.io/yelb-middletier created 5networkpolicy.networking.k8s.io/yelb-frontend created 6 7kubectl get networkpolicies.networking.k8s.io -n yelb 8NAME POD-SELECTOR AGE 9yelb-backend tier=backenddb 80s 10yelb-cache tier=cache 80s 11yelb-frontend tier=frontend 80s 12yelb-middletier tier=middletier 80s So to illustrate I will paste a diagram with the rules applied, and go ahead an see if I am allowed and not allowed to reach pods on ports not specified.\nYelb diagram with policies\nThe first thing I will try is to see if the frontend pod can reach the appserver on the specified port 4567:\nOctant Antrea Traceflow\nAnd the result is in:\nNow, what if I just change the port to something else, say DNS 53... Will it succeed?\n","link":"https://yikes.guzware.net/2021/07/10/antrea-network-policies/","section":"post","tags":["informational"],"title":"Antrea Network Policies"},{"body":"","link":"https://yikes.guzware.net/tags/antrea/","section":"tags","tags":null,"title":"Antrea"},{"body":"Antrea Egress: What is Egress when we talk about Kubernetes? Well if a pod wants to communicate to the outside world, outside the Kubernetes cluster it runs in, out of the worker node the pod resides on, this is egress traffic (definition \u0026quot;the action of going out of or leaving a place\u0026quot; and in network terminology means the direction is outward from itself).\nWhy does egress matter? Well, usually when the pods communicate out, they will use the IP address of the worker node they currently is deployed on. Thats means the actual pod IP is not the address you should be expecting when doing network inspection, tcpdump, firewall rules etc, it is the Kubernetes worker nodes IP addresses. What we call this network feature is NAT, Network Address Translation. All Kubernetes worker nodes will take the actual POD IP and translate it to its own IP before sending the traffic out of itself. And as we know, we don't know where the pod will be deployed, and the pods can be many and will relocate so in certain environments it can be hard, not granular enough to create firewall rules in the perimeter firewall to allow or block traffic from a certain pod when needed when we only can use the IP addresses of the worker nodes.\nThats where Antrea Egress comes in. With Antrea Egress we have the option to dictate which specific IP address the POD can use when communication out by using an IP address that is not its POD IP address but a valid and allowed IP address in the network. You can read more on the Antrea Egress feature here\nAs the diagram below will illustrate, when pods communicate out, the will all get their POD IP addresses translated into the worker node's IP address. And the firewall between worker node and the SQL server are only able to allow or block the IP address of the worker node. That means we potentially allow or block all pods coming from this node, or nodes if we allow the range of all the worker nodes.\nOfcourse we can use Antrea Native Policies which I have written about here or VMware NSX with NCP, and VMware NSX with Antrea Integration to do fine grained security from source. But still there are environments we need to handle rules in perimeter firewalls.\nSo, this post will show how to enable Antrea Egress in vSphere 8 with Tanzu. With the current release of Antrea there is only support of using the same L2 network as worker nodes for the Antrea Egress IP-Pool.\nAs we can see in the diagram above, Antrea Egress has been configured with an IP-Pool the pods can get if we apply Antrea Egress IPs for them to use. It will then take a free IP from the Egress IP Pool and which is within the same L2 subnet as the workers are configured on. This is very easy to do and achieve. No need to create static routes, Antrea takes care of the IP mapping. With this in place the firewall rule is now very strict, I can allow only the IP 10.10.1.40 (which is the IP the POD got from Antrea Egress Pool) and block the worker node ip address.\nBut.... I wanted to go a bit further and make use of L3 anyway for my Antrea Egress IP-Pool by utilizing BGP. Thats where the fun starts and this article is actually about. What I would like to achieve is that the IP address pool I configfure with Antrea Egress is something completely different from what the workers are using, not even the same L2 subnet but a completely different subnet. That means we need to involve some clever routing, and some configuration done on the worker nodes as its actually their IP addresses that becomes the gateway for our Antrea Egress subnets.\nSomething like this:\nThe diagram above shows a pod getting an IP address from the Egress pool which is something completely different from what subnet the worker node itself has. What Antrea does is creating a virtual interface on the worker node and assigns all the relevant ip addresses that are being used by Antrea Egress on that interface. They will use the default route on the worker node itself when going out, but the only component in the network that does know about this Egress subnet is the worker node itself, so it needs to tell this to his buddy routers out there. Either we create a static route on the router (could be the next hop of the worker node, the closest one, or some other hop in the infrastructure) or use BGP. Static route is more or less useless, too many ip addresses to update each time an egress ip is being applied, it could be on any worker node etc. So BGP is the way to go.\nThe Diagram below illustrates what happens if we dont tell our network routers where this network comes from and where it can be reached. It will egress out, but no one knows the way back.\nAs soon as the routers are informed of the address to this IP address they will be more than happy to deliver it for us, thats their job. Imagine being a postman delivering a packet somewhere in a country without any direction, address etc to narrow down his search field. In the scenario above the return traffic will most likely be sent out via a default route to the Internet and never to be seen again 😄\nSo after we have been so kind to update with the exact delivery address below, we will get our mail again.\nEnough explanation already, get to the actual config of this.\nConfigure Antrea Egress in TKC (vSphere 8) Deploy your TKC cluster, it must be Ubuntu os for this to work:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: wdc-2-tkc-cluster-1 # give your tkc cluster a name 5 namespace: wdc-2-ns-1 # remember to put it in your defined vSphere Namespace 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.10.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.20.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.23.8---vmware.2-tkg.2-zshippable 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 3 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-medium 32 - name: storageClass 33 value: vsan-default-storage-policy Apply the correct Antrea configs, enable the Egress feature:\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: wdc-2-tkc-cluster-1-antrea-package 5 namespace: wdc-2-ns-1 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: false 14 Egress: true #This needs to be enabled 15 NodePortLocal: true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true Log in to your newly created TKC cluster:\n1kubectl-vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-name tkc-cluster-1 --tanzu-kubernetes-cluster-namespace ns-1 Delete the Antrea Controller and Agent pods. Now that we have done the initial config of our TKC cluster its time to test Antrea Egress within same subnet as worker nodes just to verify that it works.\nFrom now on you should stay in the context of your newly created TKC cluster.\nVerify Antrea Egress works with L2 To be able to use Antrea Egress we need to first start with an IP-Pool definition. So I create my definition like this:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: ExternalIPPool 3metadata: 4 name: antrea-ippool-l2 #just a name of this specific pool 5spec: 6 ipRanges: 7 - start: 10.102.6.40 # make sure not to use already used ips 8 end: 10.102.6.50 # should not overlap with worker nodes 9# - cidr: 10.101.112.0/32 # or you can define a whole range with cidr /32, /27 etc 10 nodeSelector: {} # you can remove the brackets and define which nodes you want below by using labels 11# matchLabels: 12# egress-l2: antrea-egress-l2 Apply your yaml definition above:\n1andreasm@linuxvm01:~/antrea/egress$ k apply -f ippool.wdc2.tkc.cluster-1.yaml 2externalippool.crd.antrea.io/antrea-ippool-l2 created Then we need to define the actual Egress itself. What we do with this config is selecting which pod that should get an Egress ip, from wich Antrea Egress IP pool (we can have several). So here is my example:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: Egress 3metadata: 4 name: antrea-egress-l2 #just a name of this specific Egress config 5spec: 6 appliedTo: 7 podSelector: 8 matchLabels: 9 app: ubuntu-20-04 ###Which pods should get Egress IPs 10 externalIPPool: antrea-ippool-l2 ###The IP pool I defined above. Before I apply it I will just make sure that I have a pod running the these labels, if not I will deploy it and then apply the Egress. So before I apply it I will show pinging from my pod to my jumpbox VM to identify which IP it is using before applying the Egress. And the apply the Egress and see if IP changes from the POD.\nMy ubuntu pod is up and running, I have entered the shell on it and initiates a ping from my pod to my jumpbox VM:\nSo here I can see the POD identifies itself with IP 10.102.6.15. Well which worker is that?:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n prod -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3ubuntu-20-04-c9776f965-t8nmf 1/1 Running 0 20h 20.40.1.2 wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide 5NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 6wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 7wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 8wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 9wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 20h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 That is this worker: wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds.\nSo far so good. Now let me apply the Egress on this POD.\n1andreasm@linuxvm01:~/antrea/egress$ k apply -f antrea.egress.l2.yaml 2egress.crd.antrea.io/antrea-egress-l2 created Now how does the ping look like?\nThis was as expected was it not? The POD now identifies itself with the IP 10.102.6.40 which happens to be the first IP in the range defined in the pool. Well, this is cool. Now we now that Antrea Egress works. But as mentioned, I want this to be done with a different subnet than the worker nodes. Se lets see have we can do that as \u0026quot;seemless\u0026quot; as possible, as we dont want to SSH into the worker nodes and do a bunch of manual installation, configuration and so on. No, we use Kubernetes for our needs here also.\nConfigure FRR on worker nodes What I want to achieve is to deploy FRR here on my worker nodes unattended to enable BGP pr worker node to my upstream BGP router (remember, to inform about the Egress network no one knows about). The TKC workers are managed appliances, they can be deleted, scaled up and down (more workers, fewer workers.) And Deploying something manual on them are just waste of time. So we need something that deploy FRR automatically on the worker nodes.\nFRR is easy to deploy and configure, and it is included in the Ubuntu default repo (one reason I wanted to use Ubuntu as worker os). FRR is a very good routing protocol suite in Linux and is deployed easy on Ubuntu with \u0026quot;apt install frr\u0026quot;. FRR can be configured to use BGP which is the routing protocol I want to use. FRR needs two config files, daemons and frr.conf. frr.conf is individual pr node (specific IP addresses) so we need to take that into consideration also. So how can I deploy FRR on the worker nodes with their individal configuration files to automatically establish a BGP neighbourship with my Upstream router, and without logging into the actual worker nodes themselves?\nBelow diagram just illustrating a tkc worker node with FRR installed and BGP configured:\nKubernetes and Daemonset. I have created three Daemonset definition files, one for the actual deployment of FRR on all the nodes:\nThen I have created on Daemonset definition to copy the frr.conf and daemons file for the specific worker nodes and the last definition file is used to uninistall everything on the worker nodes themselves (apt purge frr) if needed.\nLets start by just deploy FRR on the workers themselves.\nHere is the defintion for that:\n1--- 2apiVersion: apps/v1 3kind: DaemonSet 4metadata: 5 namespace: kube-system 6 name: node-custom-setup 7 labels: 8 k8s-app: node-custom-setup 9 annotations: 10 command: \u0026amp;cmd apt-get update -qy \u0026amp;\u0026amp; apt-get install -qy frr 11spec: 12 selector: 13 matchLabels: 14 k8s-app: node-custom-setup 15 template: 16 metadata: 17 labels: 18 k8s-app: node-custom-setup 19 spec: 20 hostNetwork: true 21 initContainers: 22 - name: init-node 23 command: 24 - nsenter 25 - --mount=/proc/1/ns/mnt 26 - -- 27 - sh 28 - -c 29 - *cmd 30 image: alpine:3.7 31 securityContext: 32 privileged: true 33 hostPID: true 34 containers: 35 - name: wait 36 image: pause:3.1 37 hostPID: true 38 hostNetwork: true 39 tolerations: 40 - effect: NoSchedule 41 key: node-role.kubernetes.io/master 42 updateStrategy: 43 type: RollingUpdate Before I apply the above definiton I have logged into one of my TKC worker node and just wants to show that there is no FRR installed:\n1sh-5.0# cd /etc/frr 2sh: cd: /etc/frr: No such file or directory 3sh-5.0# systemctl status frr 4Unit frr.service could not be found. 5sh-5.0# hostname 6wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 7sh-5.0# Now apply:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k apply -f deploy-frr.yaml 2daemonset.apps/node-custom-setup configured 3 4andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n kube-system 5NAME READY STATUS RESTARTS AGE 6antrea-agent-4jrks 2/2 Running 0 20h 7antrea-agent-4khkr 2/2 Running 0 20h 8antrea-agent-4wxb5 2/2 Running 0 20h 9antrea-agent-ccglp 2/2 Running 0 20h 10antrea-controller-56d86d6b9b-hvrtc 1/1 Running 0 20h 11coredns-7d8f74b498-j5sjt 1/1 Running 0 21h 12coredns-7d8f74b498-mgqrm 1/1 Running 0 21h 13docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t 1/1 Running 0 21h 14docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z 1/1 Running 0 21h 15docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 1/1 Running 0 21h 16docker-registry-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 17etcd-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 18kube-apiserver-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 19kube-controller-manager-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 20kube-proxy-44qxn 1/1 Running 0 21h 21kube-proxy-4x72n 1/1 Running 0 21h 22kube-proxy-shhxb 1/1 Running 0 21h 23kube-proxy-zxhdb 1/1 Running 0 21h 24kube-scheduler-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 25metrics-server-6777988975-cxnpv 1/1 Running 0 21h 26node-custom-setup-5rlkr 1/1 Running 0 34s #There they are 27node-custom-setup-7gf2v 1/1 Running 0 62m #There they are 28node-custom-setup-b4j4l 1/1 Running 0 62m #There they are 29node-custom-setup-wjpgz 0/1 Init:0/1 0 1s #There they are Now what has happened on the TKC worker nodes itself:\n1sh-5.0# cd /etc/frr/ 2sh-5.0# pwd 3/etc/frr 4sh-5.0# ls 5daemons frr.conf support_bundle_commands.conf vtysh.conf 6sh-5.0# systemctl status frr 7● frr.service - FRRouting 8 Loaded: loaded (/lib/systemd/system/frr.service; enabled; vendor preset: enabled) 9 Active: active (running) since Mon 2023-02-20 17:53:33 UTC; 1min 36s ago Wow that looks good. But the frr.conf is more or less empty so it doesnt do anything right now.\nA note on FRR config on the worker nodes Before jumping into this section I would like to elaborate a bit around the frr.conf files being copied. If you are expecting that all worker nodes will be on same BGP AS number and your next-hop BGP neighbors are the same ones and in the same L2 as your worker node (illustrated above) you could probably go with the same config for all worker nodes. Then you can edit the same definition used for the FRR deployment to also copy and install the config in the same operation. The steps I do below describes individual config pr worker node. If you need different BGP AS numbers, multi-hop (next-hop is several hops away), individual update-source interfaces is configured then you need individual frr.config pr node.\nIndividual FRR config on the worker nodes I need to \u0026quot;inject\u0026quot; the correct config for each worker node. So I label each and one with their unique label like this: (I map the names node1-\u0026gt;lowest-ip)\nI have already configured my upstream bgp router to accept my workers as soon as they are configured and ready. This is how this looks.\n1router bgp 65802 2 bgp router-id 172.20.0.102 3 redistribute connected 4 neighbor 10.102.6.15 remote-as 66889 5 neighbor 10.102.6.16 remote-as 66889 6 neighbor 10.102.6.17 remote-as 66889 7 neighbor 172.20.0.1 remote-as 65700 8! 9 address-family ipv6 10 exit-address-family 11 exit 12 13 14cpodrouter-nsxam-wdc-02# show ip bgp summary 15BGP router identifier 172.20.0.102, local AS number 65802 16RIB entries 147, using 16 KiB of memory 17Peers 4, using 36 KiB of memory 18 19Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 2010.102.6.15 4 66889 1191 1197 0 0 0 never Active # Node1 not Established yet 2110.102.6.16 4 66889 0 0 0 0 0 never Active # Node2 not Established yet 2210.102.6.17 4 66889 1201 1197 0 0 0 never Active # Node3 not Established yet 23172.20.0.1 4 65700 19228 19172 0 0 0 01w4d09h 65 To verify again, this is the current output of frr.conf on node1:\n1sh-5.0# cat frr.conf 2# default to using syslog. /etc/rsyslog.d/45-frr.conf places the log 3# in /var/log/frr/frr.log 4log syslog informational 5sh-5.0# This is the definition I use to copy the daemons and frr.conf for the individual worker nodes:\n1--- 2apiVersion: apps/v1 3kind: DaemonSet 4metadata: 5 namespace: kube-system 6 name: node-frr-config 7 labels: 8 k8s-app: node-frr-config 9 annotations: 10 command: \u0026amp;cmd cp /tmp/wdc-2.node1.frr.conf /etc/frr/frr.conf \u0026amp;\u0026amp; cp /tmp/daemons /etc/frr \u0026amp;\u0026amp; systemctl restart frr 11spec: 12 selector: 13 matchLabels: 14 k8s-app: node-frr-config 15 template: 16 metadata: 17 labels: 18 k8s-app: node-frr-config 19 spec: 20 nodeSelector: 21 nodelabel: wdc2-node1 #Here is my specific node selection done 22 hostNetwork: true 23 initContainers: 24 - name: copy-file 25 image: busybox 26 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;cp /var/nfs/wdc-2.node1.frr.conf /var/nfs/daemons /data\u0026#39;] 27 volumeMounts: 28 - name: nfs-vol 29 mountPath: /var/nfs # The mountpoint inside the container 30 - name: node-vol 31 mountPath: /data 32 - name: init-node 33 command: 34 - nsenter 35 - --mount=/proc/1/ns/mnt 36 - -- 37 - sh 38 - -c 39 - *cmd 40 image: alpine:3.7 41 securityContext: 42 privileged: true 43 hostPID: true 44 containers: 45 - name: wait 46 image: pause:3.1 47 hostPID: true 48 hostNetwork: true 49 tolerations: 50 - effect: NoSchedule 51 key: node-role.kubernetes.io/master 52 volumes: 53 - name: nfs-vol 54 nfs: 55 server: 10.101.10.99 56 path: /home/andreasm/antrea/egress/FRR/nfs 57 - name: node-vol 58 hostPath: 59 path: /tmp 60 type: Directory 61 updateStrategy: 62 type: RollingUpdate Notice this:\n1 nodeSelector: 2 nodelabel: wdc2-node1 This is used to select the correct node after I have labeled them like this:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide 2NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 3wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 4wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 5wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 6wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 21h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 7 8andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k label node wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds nodelabel=wdc2-node1 9node/wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds labeled So its just about time to apply the configs pr node:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k apply -f wdc2.frr.node1.config.yaml 2daemonset.apps/node-custom-setup configured 3andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n kube-system 4NAME READY STATUS RESTARTS AGE 5antrea-agent-4jrks 2/2 Running 0 21h 6antrea-agent-4khkr 2/2 Running 0 21h 7antrea-agent-4wxb5 2/2 Running 0 21h 8antrea-agent-ccglp 2/2 Running 0 21h 9antrea-controller-56d86d6b9b-hvrtc 1/1 Running 0 21h 10coredns-7d8f74b498-j5sjt 1/1 Running 0 21h 11coredns-7d8f74b498-mgqrm 1/1 Running 0 21h 12docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t 1/1 Running 0 21h 13docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z 1/1 Running 0 21h 14docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 1/1 Running 0 21h 15docker-registry-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 16etcd-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 17kube-apiserver-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 18kube-controller-manager-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 19kube-proxy-44qxn 1/1 Running 0 21h 20kube-proxy-4x72n 1/1 Running 0 21h 21kube-proxy-shhxb 1/1 Running 0 21h 22kube-proxy-zxhdb 1/1 Running 0 21h 23kube-scheduler-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 24metrics-server-6777988975-cxnpv 1/1 Running 0 21h 25node-custom-setup-w4mg5 0/1 Init:0/2 0 4s Now what does my upstream router say:\n1cpodrouter-nsxam-wdc-02# show ip bgp summary 2BGP router identifier 172.20.0.102, local AS number 65802 3RIB entries 149, using 16 KiB of memory 4Peers 4, using 36 KiB of memory 5 6Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 710.102.6.15 4 66889 1202 1209 0 0 0 00:00:55 2 #Hey I am a happy neighbour 810.102.6.16 4 66889 0 0 0 0 0 never Active 910.102.6.17 4 66889 1201 1197 0 0 0 never Active 10172.20.0.1 4 65700 19249 19194 0 0 0 01w4d10h 65 11 12Total number of neighbors 4 13 14Total num. Established sessions 2 15Total num. of routes received 67 Then I just need to deploy on the other two workers.\nMy upstream bgp router is very happy to have new established neighbours:\n1cpodrouter-nsxam-wdc-02# show ip bgp summary 2BGP router identifier 172.20.0.102, local AS number 65802 3RIB entries 153, using 17 KiB of memory 4Peers 4, using 36 KiB of memory 5 6Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 710.102.6.15 4 66889 1221 1232 0 0 0 00:01:01 2 810.102.6.16 4 66889 11 16 0 0 0 00:00:36 2 910.102.6.17 4 66889 1225 1227 0 0 0 00:00:09 2 10172.20.0.1 4 65700 19254 19205 0 0 0 01w4d10h 65 11 12Total number of neighbors 4 Antrea IP Pool outside subnet of worker nodes Lets apply an IP pool which resides outside worker nodes subnet and apply Egress on my test pod again.\nHere is the IP pool config:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: ExternalIPPool 3metadata: 4 name: antrea-ippool-l3 5spec: 6 ipRanges: 7 - start: 10.102.40.41 8 end: 10.102.40.51 9# - cidr: 10.102.40.0/24 10 nodeSelector: {} 11# matchLabels: 12# egress-l3: antrea-egress-l3 And the Egress:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: Egress 3metadata: 4 name: antrea-egress-l3 5spec: 6 appliedTo: 7 podSelector: 8 matchLabels: 9 app: ubuntu-20-04 10 externalIPPool: antrea-ippool-l3 Apply it and check the IP address from the POD....\nWell, how about that?\nI know my workers reside on these ip addresses, but my POD is using a completely different IP address:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide 2NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 3wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 4wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 5wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 6wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 21h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 What about the routing table in my upstream bgp router:\n1*\u0026gt; 10.102.40.41/32 10.102.6.17 0 0 66889 ? Well have you seen..\nObjective accomplished----\u0026gt;\n","link":"https://yikes.guzware.net/2023/02/20/antrea-egress/","section":"post","tags":["Antrea","Kubernetes","Tanzu"],"title":"Antrea Egress"},{"body":"","link":"https://yikes.guzware.net/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://yikes.guzware.net/","section":"","tags":null,"title":"From 0.985mhz... to several Ghz"},{"body":"","link":"https://yikes.guzware.net/categories/kubernetes/","section":"categories","tags":null,"title":"kubernetes"},{"body":"","link":"https://yikes.guzware.net/tags/kubernetes/","section":"tags","tags":null,"title":"Kubernetes"},{"body":"","link":"https://yikes.guzware.net/categories/networking/","section":"categories","tags":null,"title":"Networking"},{"body":"","link":"https://yikes.guzware.net/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://yikes.guzware.net/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://yikes.guzware.net/tags/tanzu/","section":"tags","tags":null,"title":"Tanzu"},{"body":"","link":"https://yikes.guzware.net/tags/k8s/","section":"tags","tags":null,"title":"K8s"},{"body":"","link":"https://yikes.guzware.net/categories/tanzu/","section":"categories","tags":null,"title":"Tanzu"},{"body":"Deploy Tanzu in vSphere 8 with VDS and Avi Loadbalancer: This post will go through how to install Tanzu in vSphere 8 using vSphere VDS networking and Avi as loadbalancer. The goal is to deploy Tanzu by using vSphere Distributed Switch (no NSX this time) and utilize Avi as loadbalancer for Supervisor and workload cluster L4 endpoint (kubernetes API). When that is done I will go through how we also can extend this into L7 (Ingress) by using AKO in our workload clusters.\nThe below diagram is what we should end up with after the basic deployment of Tanzu and Avi:\nAssumptions This post assumes we already have a vSphere environment up and running with vCenter, HA and DRS. Required network to support the basic vSphere stuff like vMotion and shared storage. And the hosts networking has been configured with a Distributed Switch with the corresponding vds portgroups for Management, Frontend network (VIP placement for kubernetes API endpoint) and workload network with corresponding VLANs. In vCenter a content library needs to be created, this is just a local library you give a meaningful name no subscriptions etc.\nAt least one Avi controller is deployed, no cloud added, just deployed and the initial configs done.\nPreparations on the Avi side of things This part of the guide takes place on the newly configured Avi controller(s) which currently only has the initial configuration done\nAvi cloud configurations To prepare Avi for this deployment we need to configure the vCenter cloud. This is done here:\nThere is a Default-Cloud object there we need to convert to a vCenter cloud. This is done by clicking on this button on the far right side: This will bring up the following options: Select VMware vCenter/vSphere NSX Then start populate the relevant vCenter information for your vCenter: When credentials is added slect content library and choose your content library from the list, then click connect and then Save \u0026amp; Relaunch\nWhen the dialog relaunches select the management network and ip address management. I have opted for DHCP (I have DHCP in my mgmt network, if not we can leverage Avi as IPAM provider for the mgmt network also.) This is used for the SE's mgmt interface. Click save for now. Head over to the Template section to add IPAM and DNS.\nWhen using vCenter clouds the different portgroups is automatically added under networks. We need to configure some of them. But for now just create the IPAM and DNS profiles and we configure the networks later accordingly.\nThe DNS Profile (optional, only if you want to use Avi DNS service):\nClick save when done\nThe IPAM profile:\nSelect the Default-Cloud, then select from the list \u0026quot;Usable Networks\u0026quot; the Frontend network vds portgroup corresponding to the frontend network we want to use for our endpoint vips. Click save. You should have your profiles configured now:\nHead back to your cloud again and add your newly created IPAM and DNS profiles.\nAdd the profiles: Before you click finish, make sure you have selected \u0026quot;Prefer Static Routes vs Directly Connected Network\u0026quot; like this: Then click finish...\nAvi network configs Now its time to configure the networks for the SEs (VIP, dataplane). I will go ahead and configure both the Frontend VIP for kubernetes API endpoint, but also the workload network I will use when I add L7 functionality later. Head over to Cloud Resources: Scroll until you find your \u0026quot;Frontend-network\u0026quot; Click edit all the way to the right: In here we need to define the subnet (if not already auto discovered) and add and IP range for the SE's and VIPS. We can decide to create just one range for both, or a range only for SEs, and one for VIPs only. To create a range for both SE and VIP do like this: If you want a specific range for SE and a specific for VIP do like this: Common for both is to deselect the DHCP Enabled option. What we have done now is to tell Avi that Avi is responsible for IP allocation to our SE's when they are deployed and configured to give them each their IP in the Frontend network, and also \u0026quot;carve\u0026quot; out an IP for the VIP when a Virtual Service is created and IP allocation for that service is selected to auto-allocate. The same would go if you decided to not use DHCP for mgmt IP, you would need to defined the network and select only \u0026quot;Use for Service Engine\u0026quot; (no VIPs in the management network)\nAvi service engine group Now its time to prepare the Default Service Engine Group. Head over to Cloud Resources - Service Engine Group\nClick the pencil on the far right side of the Default-Group and make the following changes:\nIn the Advanced tab: Here we select our vSphere cluster for the SE placement, vSphere shared storage, and the Prefix and vCenter folder placement (if you want). Now that is done.\nAvi VRF context Now we need to create a static route for the SE dataplane to know which gateway will take them to the \u0026quot;backend-pool\u0026quot; (the services they are acting as loadbalancer for). This is usually the gateway for the networks in their respective subnet as the dataplane is residing in. Here I prepare the route for the Frontend network, and also the Workload network (so it is already done when moving to the step of enabling L7). Avi controller SSL certificate for Tanzu \u0026quot;integration\u0026quot; The last step is to create a new certificate, or use your own signed certificate, for the Tanzu deployment to use. Head over to Templates - SSL/TLS Certificates. From here we click \u0026quot;Create\u0026quot; in the top right corner: I will go ahead and create a new self-signed certificate: It is important that you use the IP or FQDN of the controller under \u0026quot;Common Name\u0026quot; and under \u0026quot;Subject Alternate Name (SAN)\u0026quot;\nNow head over to Administration - Access Settings: Click edit on the pencil in the top right corner, remove the existing certificates under SSL/TLS Certificate: And replace with the one you created: Now the Avi config is done for this round. Next step is to enable Workload Management in vSphere...\nEnable Workload Management in vSphere This section will cover all the steps to enable Tanzu from vCenter, describing the selections made and the network configs.\nEnable workload management Head over to your vCenter server and click here: (from the the \u0026quot;hamburger menu\u0026quot; top left corner)\nClick the Get Started button:\nStep 1: Select vSphere Distributed Switch Step 2: Select Cluster Deployment, give the supervisor cluster a name and give it a zone name: Step 3: Select your Storage Policy (If VSAN and you dont have created a specific VSAN policy for this use Default Storage Policy): Step 4: Type in the relevant info for your Avi Controller and copy paste the certificate from your Avi controller: The certificate is easily copied from the Avi controller by going to Templates - SSL/TLS Certificates and click the \u0026quot;down arrow\u0026quot;: Then copy the certificate: Paste the content in the Server Certificate field above (step 4) Step 5: Management Network Here we fill in the required information for the Supervisor nodes. Management IP for the nodes themselves (needs connectivity to both vCenter and ESXi hosts, could be in the same mgmt network as vCenter and ESXi). Select the corresponding vds portgroup, select either static or DHCP if you want to use DHCP. Step 6: Workload Network Select the correct vds portgroup for the workload network. The supervisor and the workload nodes will be placed here. Can be static or DHCP. Leave the default \u0026quot;Internal Network for Kubernetes Services\u0026quot;, that is for the internal services (clusterIP etc inside the K8s clusters, they will never be exposed outside). Fill in the necessary config if you go with static. Step 7: Review and Confirm and optionally give the Supervisor endpoint a DNS name which you later can register in your DNS service when we have the L4 IP for the kubernetes API endpoint. Click finish:\nThe whole summary: Now sit back and wait for the creation of the supervisor cluster, it can take a couple of minutes. After a while you can take a look in your Avi controller under Applications and see if something is being created there; You can monitor the process from the Workload management status view by clicking on the \u0026quot;Configuring (View) )\u0026quot;. You can continue work with your vCenter server and go back to this progress bar whenever you want by clicking the hamburger menu Workload management.\nIn your vCenter inventory you should also see the Supervisor VMs and Avi SE's like this: When its done deploying you will see the green status here: Notice the Control Plane Node Address, this is our endpoint IP being served by Avi loadbalancer and the one we use to interact with the Supervisor cluster to create workloads etc..\nEnter the IP in your browser with https://10.102.7.11 and you should see this page: Download the cli tools for your operating system, deploy them so they are in your path. Will use both kubectl and kubectl-vsphere later on Next steps include creating namespace, deploy workload clusters. Lets dive into it.\nvSphere Namespace vSphere with Tanzu workloads, including vSphere Pods, VMs, and Tanzu Kubernetes clusters, are deployed to a vSphere Namespace. You define a vSphere Namespace on a Supervisor and configure it with resource quota and user permissions. Depending on the DevOps needs and workloads they plan to run, you might also assign storage policies, VM classes, and content libraries for fetching the latest Tanzu Kubernetes releases and VM images. source\nCreate a vSphere namespace Now that the Supervisor cluster is ready and running head back to your vCenter and create a vSphere namespace. Click create namespace (above) the select the supervisor to create the namespace on, and give your namespace a name then select the \u0026quot;workload network\u0026quot; you have defined for your workload placement. Now the namespace is being created.\nAdd additional workload networks Sidenote there is also possible to add more \u0026quot;workload networks\u0026quot; after the Supervisor has been configured under Supervisor config if you want to add more \u0026quot;workload networks\u0026quot; for separation etc. To do that head over to Workload Management in vCenter:\nThen select the supervisor tab:\nClick on your supervisor cluster here: Then click the configure tab and go to network and add your additional workload network: After your namespace has been created we need to configure it with access permissions, datastores, content library and vmclasses: Create workload cluster Afte the vSphere Namespace has been configured its time to deploy a workload cluster/TKC cluster (Tanzu Kubernetes Cluster cluster 😄 ). From your workstation/jumphost where you downloaded the cli tools login in to the supervisor with access rights to the Supervisor API. (administrator@vsphere.local will have access).\nCustom role in vCenter I created a specific role in my vCenter with these privileges:\nAdded my \u0026quot;supervisor-manager\u0026quot; user in this role as global and in the top tree of my vCenter with inheritance. Also added it as \u0026quot;editor\u0026quot; in my wdc-2-ns-1 vSphere Namespace.\n1kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net When your are logged in it will give you this output and also put the kubernetes config in your ~/.kube/config file.\n1andreasm@linuxvm01:~$ kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net 2 3 4KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below 5Password: 6Logged in successfully. 7 8You have access to the following contexts: 9 10.102.7.11 10 wdc-2-ns-1 11 12If the context you wish to use is not in this list, you may need to try 13logging in again later, or contact your cluster administrator. 14 15To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` When you are logged in prepare your yaml for your first workload cluster and apply it with kubectl apply -f nameof.yaml\nExample:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: wdc-2-tkc-cluster-1 # give your tkc cluster a name 5 namespace: wdc-2-ns-1 # remember to put it in your defined vSphere Namespace 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.10.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.20.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.23.8---vmware.2-tkg.2-zshippable 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 3 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-medium 32 - name: storageClass 33 value: vsan-default-storage-policy As soon as I apply the above yaml it will deploy the corresponding tkc cluster in your vsphere environment:\nSit back and enjoy while your tkc cluster is being created for you. We can check the status in the vCenter gui:\nor via kubectl:\n1andreasm@linuxvm01:~/tkc-wdc-01-vds$ k get cluster -n wdc-2-ns-1 2NAME PHASE AGE VERSION 3wdc-2-tkc-cluster-1 Provisioned 12m v1.23.8+vmware.2 It is ready, now we need to log into it:\n1andreasm@linuxvm01:~/tkc-wdc-01-vds$ kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-name=wdc-2-tkc-cluster-1 --tanzu-kubernetes-cluster-namespace=wdc-2-ns-1 2 3 4KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below 5Password: 6Logged in successfully. 7 8You have access to the following contexts: 9 10.102.7.11 10 wdc-2-ns-1 11 wdc-2-tkc-cluster-1 12 13If the context you wish to use is not in this list, you may need to try 14logging in again later, or contact your cluster administrator. 15 16To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` Check if you are able to list ns and pods:\n1andreasm@linuxvm01:~/tkc-wdc-01-vds$ k get pods -A 2NAMESPACE NAME READY STATUS RESTARTS AGE 3kube-system antrea-agent-77drs 2/2 Running 0 8m35s 4kube-system antrea-agent-j482r 2/2 Running 0 8m34s 5kube-system antrea-agent-thh5b 2/2 Running 0 8m35s 6kube-system antrea-agent-tz4fb 2/2 Running 0 8m35s 7kube-system antrea-controller-575845467f-pqgll 1/1 Running 0 8m35s 8kube-system coredns-7d8f74b498-ft7rf 1/1 Running 0 10m 9kube-system coredns-7d8f74b498-pqgp7 1/1 Running 0 7m35s 10kube-system docker-registry-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 11kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-6cvz9 1/1 Running 0 8m46s 12kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-rgn29 1/1 Running 0 8m34s 13kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-rsmfw 1/1 Running 0 9m 14kube-system etcd-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 15kube-system kube-apiserver-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 16kube-system kube-controller-manager-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 17kube-system kube-proxy-67xjk 1/1 Running 0 8m46s 18kube-system kube-proxy-6fttt 1/1 Running 0 8m35s 19kube-system kube-proxy-m4wt8 1/1 Running 0 11m 20kube-system kube-proxy-rbsjw 1/1 Running 0 9m1s 21kube-system kube-scheduler-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 22kube-system metrics-server-6f7c489795-scmm6 1/1 Running 0 8m36s 23secretgen-controller secretgen-controller-6966677567-4hngd 1/1 Running 0 8m26s 24tkg-system kapp-controller-55f9977c86-bqppj 2/2 Running 0 9m22s 25tkg-system tanzu-capabilities-controller-manager-cb4bc7978-qh9s8 1/1 Running 3 (87s ago) 7m54s 26vmware-system-auth guest-cluster-auth-svc-r4rk9 1/1 Running 0 7m48s 27vmware-system-cloud-provider guest-cluster-cloud-provider-859b8dc577-8jlth 1/1 Running 0 8m48s 28vmware-system-csi vsphere-csi-controller-6db86b997-l5glc 6/6 Running 0 8m46s 29vmware-system-csi vsphere-csi-node-7bdpl 3/3 Running 2 (7m34s ago) 8m34s 30vmware-system-csi vsphere-csi-node-q7zqr 3/3 Running 3 (7m32s ago) 8m46s 31vmware-system-csi vsphere-csi-node-r8v2c 3/3 Running 3 (7m34s ago) 8m44s 32vmware-system-csi vsphere-csi-node-zl4m2 3/3 Running 3 (7m40s ago) 8m46s 1andreasm@linuxvm01:~/tkc-wdc-01-vds$ k get ns 2NAME STATUS AGE 3default Active 12m 4kube-node-lease Active 12m 5kube-public Active 12m 6kube-system Active 12m 7secretgen-controller Active 9m3s 8tkg-system Active 9m55s 9vmware-system-auth Active 12m 10vmware-system-cloud-provider Active 10m 11vmware-system-csi Active 10m 12vmware-system-tkg Active 12m By default we are not allowed to run anything on our newly created tkc cluster. We need to define some ClusterRoles. I will just apply a global clusterolres on my tkc cluster so I can do what I want with it like this: Apply the psp policy yaml:\n1apiVersion: rbac.authorization.k8s.io/v1 2kind: ClusterRole 3metadata: 4 name: psp:privileged 5rules: 6- apiGroups: [\u0026#39;policy\u0026#39;] 7 resources: [\u0026#39;podsecuritypolicies\u0026#39;] 8 verbs: [\u0026#39;use\u0026#39;] 9 resourceNames: 10 - vmware-system-privileged 11--- 12apiVersion: rbac.authorization.k8s.io/v1 13kind: ClusterRoleBinding 14metadata: 15 name: all:psp:privileged 16roleRef: 17 kind: ClusterRole 18 name: psp:privileged 19 apiGroup: rbac.authorization.k8s.io 20subjects: 21- kind: Group 22 name: system:serviceaccounts 23 apiGroup: rbac.authorization.k8s.io 1kubectl apply -f roles.yaml 2clusterrole.rbac.authorization.k8s.io/psp:privileged created 3clusterrolebinding.rbac.authorization.k8s.io/all:psp:privileged created Now that I am allowed to deploy stuff I am ready to consume the newly cluster. But this blog was how to deploy Tanzu with VDS and Avi Loadbalancer. So far I have only covered the L4 part where Avi is providing me the K8s API endpoints, I will now jump over to the section where I configure both Avi and my tkc cluster to use Ingress (L7) also so I can publish/expose my applications with ingress. That means installing an additional component called AKO in my tkc cluster and configure Avi accordingly.\nConfigure Avi as Ingress controller (L7) For Avi Ingress we need to deploy a component in our TKC cluster called AKO. AKO stands for Avi Kubernetes Operator and introduces the ability to translate our k8s api to the Avi controller so we can make our Avi automatically create vs services for us as soon as we request them from our TKC cluster. To deploy AKO we use Helm. In short we need to add the AKO helm repository, get the ako values, edit them to fit our environment, then install it by using Helm. So let us go through this step-by-step (I have also covered it a while back in an Upstream k8s cluster) but let us do it again here.\nCreate the namespace for the ako pod:\n1k create ns avi-system 2namespace/avi-system created Then add the repo to Helm:\n1helm repo add ako https://projects.registry.vmware.com/chartrepo/ako Check the repo:\n1andreasm@linuxvm01:~$ helm search repo 2NAME CHART VERSION\tAPP VERSION\tDESCRIPTION 3ako/ako 1.8.2 1.8.2 A helm chart for Avi Kubernetes Operator 4ako/ako-operator\t1.3.1 1.3.1 A Helm chart for Kubernetes AKO Operator 5ako/amko 1.8.2 1.8.2 A helm chart for Avi Kubernetes Operator Get the values.yaml:\n1 helm show values ako/ako --version 1.8.2 \u0026gt; values.yaml Now its time to edit the value file. I will go through the values files, update it accordingly and adjust some configurations in Avi controller.\nThe values.yaml for ako chart:\n1# this file has been edited by me to easier reflect the changes I have done. So all default comments have been removed, and contains only my comments. 2replicaCount: 1 3 4image: 5 repository: projects.registry.vmware.com/ako/ako 6 pullPolicy: IfNotPresent 7 8 9AKOSettings: 10 primaryInstance: true 11 enableEvents: \u0026#39;true\u0026#39; 12 logLevel: WARN 13 fullSyncFrequency: \u0026#39;1800\u0026#39; 14 apiServerPort: 8080 15 deleteConfig: \u0026#39;false\u0026#39; 16 disableStaticRouteSync: \u0026#39;false\u0026#39; 17 clusterName: wdc-tkc-cluster-1 # Here we need to define a name for our specific TKC cluster. This must not be the exact names as the cluster itself, but why not, it MUST be unique across all your TKC clusters if you have multiple AKO enabled TKC/K8s clusters on same Avi controller 18 cniPlugin: \u0026#39;antrea\u0026#39; #This needs to be set to Antrea, the reason is that you would like to configure NodePortLocal. 19 enableEVH: false 20 layer7Only: true # This is very important to set to true as we already have an other AKO instance managing L4 for our k8s api endpoints. We will only configure this instance to use L7. 21 22 namespaceSelector: 23 labelKey: \u0026#39;\u0026#39; 24 labelValue: \u0026#39;\u0026#39; 25 servicesAPI: false 26 vipPerNamespace: \u0026#39;false\u0026#39; 27 28NetworkSettings: 29 nodeNetworkList: 30 # nodeNetworkList: 31 - networkName: \u0026#34;vds-tkc-workload-vlan-1026\u0026#34; # this is the VDS portgroup you have for your specific TKC cluster workers running in. In my case the defined portgroup name above. You can see this in vCenter 32 cidrs: 33 - 10.102.6.0/24 # this is the CIDR for your workers 34 enableRHI: false 35 nsxtT1LR: \u0026#39;\u0026#39; 36 bgpPeerLabels: [] 37 # bgpPeerLabels: 38 # - peer1 39 # - peer2 40 vipNetworkList: 41 - networkName: \u0026#34;vds-tkc-frontend-vlan-1027\u0026#34; # This can be the same VIP network you have configured in previously for api endpint or it can be a completely new one. I am going the easy route using the same. It can be shared across multiple cluster (is using NodePortLocal), or can be specific for each tkc cluster. 42 cidr: 10.102.7.0/24 43 44L7Settings: 45 defaultIngController: \u0026#39;true\u0026#39; # Specify if this is the only Ingress controller you have or default if using several others. 46 noPGForSNI: false 47 serviceType: NodePortLocal # Here we select nodeportlocal - verify that Antrea is configured to use NodePortLocal 48 shardVSSize: SMALL # I am setting this to small so I can run more services using same IP. 49 passthroughShardSize: SMALL 50 enableMCI: \u0026#39;false\u0026#39; 51 52L4Settings: 53 defaultDomain: \u0026#39;\u0026#39; 54 autoFQDN: default 55 56 57ControllerSettings: 58 serviceEngineGroupName: Default-Group # If you dont decide to use the same VIP as k8s api endpoint you could decide to create an additional ServiceEngineGroup for your L7 services (IP separation etc). 59 controllerVersion: \u0026#39;22.1.1\u0026#39; # AKO version 1.8.2 supports Avi 22.1.3 60 cloudName: Default-Cloud # The configured cloud name on the Avi controller. 61 controllerHost: \u0026#39;172.21.101.50\u0026#39; # IP address or Hostname of Avi Controller 62 tenantName: admin 63 64nodePortSelector: 65 key: \u0026#39;\u0026#39; 66 value: \u0026#39;\u0026#39; 67 68resources: 69 limits: 70 cpu: 350m 71 memory: 400Mi 72 requests: 73 cpu: 200m 74 memory: 300Mi 75 76podSecurityContext: {} 77 78rbac: 79 pspEnable: false 80 81 82avicredentials: 83 username: \u0026#39;admin\u0026#39; # username for the Avi controller 84 password: \u0026#39;password\u0026#39; # password for the Avi controller 85 authtoken: 86 certificateAuthorityData: 87 88 89persistentVolumeClaim: \u0026#39;\u0026#39; 90mountPath: /log 91logFile: avi.log A word around the VIP network used for the L7/Ingress. As we deploy AKO as standalone we are not restricted to use only the components defined to support the install of Tanzu with vSphere, like service engine groups, vip networks etc. We could decide to create a separate VIP network by using a dedicated SE group for these networks. We could also decide to have the SE's using a separate dataplane network than the VIP itself. If going this path there is some config steps that needs to be taken on the network side. Routing to the VIP addresses, either Avi can be configured by using BGP, or we create static routes in the physical routers. But as the VIPs are coming and going (applications are published, deleted, etc) these IPs change. So BGP would be the best option, or use an already defined VLAN as I am doing in this example. In my other post on using NSX and Avi with Tanzu I will show how to use NSX for BGP. Maybe I will update this post also by adding a section where I use BGP from Avi to my upstream router. But for now I will stick with using my VLAN I have called frontend which already have a gateway and a route defined. So all my VIPs will be reachable through this network.\nAntrea NodePortLocal And another word around NodePortLocal. To be able to utilize NodePortLocal your Antrea config in the TKC cluster must be verified whether it is configured with NPL or not. So let us do instead of just assume something.\n1andreasm@linuxvm01:~/ako/ako_vds$ k get configmaps -n kube-system antrea-config-f5d8g47b88 -oyaml 2apiVersion: v1 3data: 4 antrea-agent.conf: | 5 featureGates: 6 AntreaProxy: true 7 EndpointSlice: true 8 Traceflow: true 9 NodePortLocal: false 10 AntreaPolicy: true 11 FlowExporter: false 12 NetworkPolicyStats: false 13 Egress: false 14 AntreaIPAM: false 15 Multicast: false 16 ServiceExternalIP: false 17 trafficEncapMode: encap 18 noSNAT: false 19 tunnelType: geneve 20 trafficEncryptionMode: none 21 wireGuard: 22 port: 51820 23 egress: {} 24 serviceCIDR: 20.10.0.0/16 Well that was not good. So we need to enable it. Luckily, with Tanzu with vSphere its quite simple actually. Switch context to your vSphere Namespace, edit an antreaconfig, apply it.\n1andreasm@linuxvm01:~/antrea$ k config use-context wdc-2-ns-1 2Switched to context \u0026#34;wdc-2-ns-1\u0026#34;. 3andreasm@linuxvm01:~/antrea$ k get cluster 4NAME PHASE AGE VERSION 5wdc-2-tkc-cluster-1 Provisioned 3h26m v1.23.8+vmware.2 6andreasm@linuxvm01:~/antrea$ k apply -f antreaconfig-wdc-2-nsx-1.yaml 7Warning: resource antreaconfigs/wdc-2-tkc-cluster-1-antrea-package is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. 8antreaconfig.cni.tanzu.vmware.com/wdc-2-tkc-cluster-1-antrea-package configured The antreaconfig I used:\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: wdc-2-tkc-cluster-1-antrea-package # notice the naming-convention tkc cluster name-antrea-package 5 namespace: wdc-2-ns-1 # your vSphere Namespace the TKC cluster is in. 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: false 14 Egress: true 15 NodePortLocal: true # Set this to true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true Lets have a look at my Antrea config in my TKC cluster now:\n1andreasm@linuxvm01:~/antrea$ k config use-context 210.102.7.11 wdc-2-ns-1 wdc-2-tkc-cluster-1 3andreasm@linuxvm01:~/antrea$ k config use-context wdc-2-tkc-cluster-1 4Switched to context \u0026#34;wdc-2-tkc-cluster-1\u0026#34;. 5andreasm@linuxvm01:~/antrea$ k get configmaps -n kube-system antrea-config-f5d8g47b88 -oyaml 6apiVersion: v1 7data: 8 antrea-agent.conf: | 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 Traceflow: true 13 NodePortLocal: true # Yes, there it is 14 AntreaPolicy: true 15 FlowExporter: false 16 NetworkPolicyStats: true 17 Egress: true 18 AntreaIPAM: false 19 Multicast: false 20 ServiceExternalIP: false 21 trafficEncapMode: encap 22 noSNAT: false 23 tunnelType: geneve 24 trafficEncryptionMode: none 25 wireGuard: 26 port: 51820 27 egress: 28 exceptCIDRs: [] 29 serviceCIDR: 20.10.0.0/16 But... Even though our cluster config has been updated we need to delete the Antrea pods so they can restart and read their new configmap again.\n1andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-controller-575845467f-pqgll 2pod \u0026#34;antrea-controller-575845467f-pqgll\u0026#34; deleted 3andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent- 4antrea-agent-77drs antrea-agent-j482r antrea-agent-thh5b antrea-agent-tz4fb 5andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-77drs 6pod \u0026#34;antrea-agent-77drs\u0026#34; deleted 7andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-j482r 8pod \u0026#34;antrea-agent-j482r\u0026#34; deleted 9andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-thh5b 10pod \u0026#34;antrea-agent-thh5b\u0026#34; deleted 11andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-tz4fb 12pod \u0026#34;antrea-agent-tz4fb\u0026#34; deleted Configure Avi as Ingress controller (L7) - continue Now that we have assured NodePortLocal is configured, its time to deploy AKO. I have also verified that I have the VIP network configured in Avi as I am using the existing network that is already defined. So install AKO then 😄\n1helm install ako/ako --generate-name --version 1.8.2 -f ako.vds.wdc-2.values.yaml --namespace=avi-system 2NAME: ako-1675511021 3LAST DEPLOYED: Sat Feb 4 11:43:42 2023 4NAMESPACE: avi-system 5STATUS: deployed 6REVISION: 1 7TEST SUITE: None Verify that the AKO pod is running:\n1andreasm@linuxvm01:~/ako/ako_vds$ k get pods -n avi-system 2NAME READY STATUS RESTARTS AGE 3ako-0 1/1 Running 0 57s Check logs for some immediate messages that needs investigating before trying to deploy a test application.\n1andreasm@linuxvm01:~/ako/ako_vds$ k logs -n avi-system ako-0 22023-02-04T11:43:51.240Z\tINFO\tapi/api.go:52\tSetting route for GET /api/status 32023-02-04T11:43:51.241Z\tINFO\tako-main/main.go:71\tAKO is running with version: v1.8.2 42023-02-04T11:43:51.241Z\tINFO\tapi/api.go:110\tStarting API server at :8080 52023-02-04T11:43:51.242Z\tINFO\tako-main/main.go:81\tWe are running inside kubernetes cluster. Won\u0026#39;t use kubeconfig files. 62023-02-04T11:43:51.265Z\tINFO\tlib/control_config.go:198\tako.vmware.com/v1alpha1/AviInfraSetting enabled on cluster 72023-02-04T11:43:51.270Z\tINFO\tlib/control_config.go:207\tako.vmware.com/v1alpha1/HostRule enabled on cluster 82023-02-04T11:43:51.273Z\tINFO\tlib/control_config.go:216\tako.vmware.com/v1alpha1/HTTPRule enabled on cluster 92023-02-04T11:43:51.290Z\tINFO\tako-main/main.go:150\tKubernetes cluster apiserver version 1.23 102023-02-04T11:43:51.296Z\tINFO\tutils/utils.go:168\tInitializing configmap informer in avi-system 112023-02-04T11:43:51.296Z\tINFO\tlib/dynamic_client.go:118\tSkipped initializing dynamic informers antrea 122023-02-04T11:43:51.445Z\tINFO\tk8s/ako_init.go:455\tSuccessfully connected to AVI controller using existing AKO secret 132023-02-04T11:43:51.446Z\tINFO\tako-main/main.go:261\tValid Avi Secret found, continuing .. 142023-02-04T11:43:51.866Z\tINFO\tcache/avi_ctrl_clients.go:71\tSetting the client version to 22.1.1 152023-02-04T11:43:51.866Z\tINFO\tako-main/main.go:279\tSEgroup name found, continuing .. 162023-02-04T11:43:53.015Z\tINFO\tcache/controller_obj_cache.go:2340\tAvi cluster state is CLUSTER_UP_NO_HA 172023-02-04T11:43:53.176Z\tINFO\tcache/controller_obj_cache.go:2901\tSetting cloud vType: CLOUD_VCENTER 182023-02-04T11:43:53.176Z\tINFO\tcache/controller_obj_cache.go:2904\tSetting cloud uuid: cloud-ae84c777-ebf8-4b07-878b-880be6b201b5 192023-02-04T11:43:53.176Z\tINFO\tlib/lib.go:291\tSetting AKOUser: ako-wdc-2-tkc-cluster-1 for Avi Objects 202023-02-04T11:43:53.177Z\tINFO\tcache/controller_obj_cache.go:2646\tSkipping the check for SE group labels 212023-02-04T11:43:53.332Z\tINFO\tcache/controller_obj_cache.go:3204\tSetting VRF global found from network vds-tkc-frontend-vlan-1027 222023-02-04T11:43:53.332Z\tINFO\trecord/event.go:282\tEvent(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;avi-system\u0026#34;, Name:\u0026#34;ako-0\u0026#34;, UID:\u0026#34;4b0ee7bf-e5f5-4987-b226-7687c5759b4a\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;41019\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;ValidatedUserInput\u0026#39; User input validation completed. 232023-02-04T11:43:53.336Z\tINFO\tlib/lib.go:230\tSetting Disable Sync to: false 242023-02-04T11:43:53.338Z\tINFO\tk8s/ako_init.go:310\tavi k8s configmap created Looks good, let us try do deploy an application and expose it with Ingress\nI have two demo applications, banana and apple. Yaml comes below. I deploy them\n1andreasm@linuxvm01:~/ako$ k create ns fruit 2namespace/fruit created 3andreasm@linuxvm01:~/ako$ k apply -f apple.yaml -f banana.yaml 4pod/apple-app created 5service/apple-service created 6pod/banana-app created 7service/banana-service created yaml for banana and fruit\n1kind: Pod 2apiVersion: v1 3metadata: 4 name: banana-app 5 labels: 6 app: banana 7 namespace: fruit 8spec: 9 containers: 10 - name: banana-app 11 image: hashicorp/http-echo 12 args: 13 - \u0026#34;-text=banana\u0026#34; 14 15--- 16 17kind: Service 18apiVersion: v1 19metadata: 20 name: banana-service 21 namespace: fruit 22spec: 23 selector: 24 app: banana 25 ports: 26 - port: 5678 # Default port for image 1kind: Pod 2apiVersion: v1 3metadata: 4 name: apple-app 5 labels: 6 app: apple 7 namespace: fruit 8spec: 9 containers: 10 - name: apple-app 11 image: hashicorp/http-echo 12 args: 13 - \u0026#34;-text=apple\u0026#34; 14 15--- 16 17kind: Service 18apiVersion: v1 19metadata: 20 name: apple-service 21 namespace: fruit 22spec: 23 selector: 24 app: apple 25 ports: 26 - port: 5678 # Default port for image Then I apply the Ingress:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 namespace: fruit 6# annotations: 7# ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 8 9spec: 10 ingressClassName: avi-lb 11 rules: 12 - host: fruit-tkgs.you-have.your-domain.here 13 http: 14 paths: 15 - path: /apple 16 pathType: Prefix 17 backend: 18 service: 19 name: apple-service 20 port: 21 number: 5678 22 - path: /banana 23 pathType: Prefix 24 backend: 25 service: 26 name: banana-service 27 port: 28 number: 5678 1k apply -f ingress-example.yaml You should more or less instantly notice a new virtual service in your Avi controller: And let us check the ingress in k8s:\n1andreasm@linuxvm01:~/ako$ k get ingress -n fruit 2NAME CLASS HOSTS ADDRESS PORTS AGE 3ingress-example avi-lb fruit-tkgs.you-have.your-domain.here 10.102.7.15 80 2m49s There it is, with the actual VIP it gets from Avi.\nHeres is the view of the application from the Dashboard view in Avi: Also notice that the SE's now also places itself in the same network as the worker nodes, but still creates the VIP in the frontend-network.\nMeaning our network diagram will now look like this: Now, AKO comes with its own CRDs that one can work with. I will go through these in a separate post.\n","link":"https://yikes.guzware.net/2022/10/26/vsphere-8-with-tanzu-using-vds-and-avi-loadbalancer/","section":"post","tags":["K8s","Tanzu"],"title":"vSphere 8 with Tanzu using VDS and Avi Loadbalancer"},{"body":"","link":"https://yikes.guzware.net/tags/avi/","section":"tags","tags":null,"title":"Avi"},{"body":"","link":"https://yikes.guzware.net/tags/nsx/","section":"tags","tags":null,"title":"NSX"},{"body":"Deploy Tanzu in vSphere 8 with NSX and Avi Loadbalancer: This post will go through how to install Tanzu in vSphere 8 using NSX networking (including built in L4 loadbalancer) and Avi as L7 loadbalancer. The goal is to deploy Tanzu by using NSX for all networking needs, including the Kubernetes Api endpoint (L4) and utilize Avi as loadbalancer for all L7 (Ingress). The deployment of Tanzu with NSX is an automated process, but it does not include L7 loadbalancing. This post will quickly go through how to configure NSX and Avi to support this setup and also the actual configuration/deployment steps of Tanzu. The following components will be touched upon in this post: NSX, Tanzu, TKC, AKO, NCP, vCenter, AVI and Antrea. All networks needed for this deployment will be handled by NSX, except vCenter, NSX manager and Avi controller but including the management network for the supervisor cluster and Avi SE's. In the end we will also have a quick look at how to use Antrea Egress in one of the TKC clusters.\nWe should end up with the following initial network diagram for this deployment (will update it later in the post reflecting several network for our TKC cluster with and without NAT (without NAT when using Egress): Preparations - NSX config This post assumes a working vSphere environment with storage configured, vMotion network, vSAN (if using vSAN), HA, DRS enabled and configured. So this step will cover the basic NSX config for this use-case. NSX will need some network configured in the physical environment like the Geneve Tunnel VLAN, Uplink VLAN(s) for our T0 in addition to the most likely already defined management network for the placement of NSX managers, NSX edges and Avi controller and/or SE's. So lets jump in.\nInitial configs in the NSX manager The first NSX manager is already deployed. Accept the EULA and skip the NSX tutorial:\nWhen done, head over to System -\u0026gt; Licenses and add your license key. Then, still under system, head over to Appliances and add a cluster IP. Even though you only have 1 NSX manager for test/poc it can make sense to use cluster ip adding, removing nsx managers etc and still point to the same IP.\nClick on Set Virtual IP and type in your wanted cluster ip. Out of the box its the same layer 2 subnet as your controllers are placed in (it possible to use L3 also but that involves an external LoadBalancer, not the built in for this purpose).\nClick save and wait. After some minutes, try to log in via your new cluster IP. All the configs I will do will be used with this IP. It does not matter if you go directly to the NSX manager itself or the cluster IP for this post.\nAfter cluster IP is done, we need to add a Compute Manager which in our case is the vCenter server (not that you have any option besides vCenter). Still under System, go to Fabric expand and find Compute Manager. From there click Add Compute Manager:\nFill in the necessary information for your vCenter and make sure Service Account and Enable Trust is enabled. Next message will ask you to use a Thumprint the vCenter says it has. You could either just say ADD or actually go to vCenter and grab the thumbprint from there and verify or paste it in the SHA-256 field before clicking add.\nHere is how to get the thumbprint from vCenter:\n1root@vcsa [ ~ ]# openssl x509 -in /etc/vmware-vpx/ssl/rui.crt -fingerprint -sha256 -noout 2SHA256 Fingerprint=A1:F2:11:0F:47:D8:7B:02:D1:C9:B6:87:19:C0:65:15:B7:6A:6E:23:67:AD:0C:41:03:13:DA:91:A9:D0:B2:F6 Now when you are absolutely certain, add and wait.\nNSX Profiles: Uplink, Transport Zones and Transport Node Profiles In NSX there is a couple of profiles that needs to be configured, profiles for the Transport nodes, Edge transport nodes, transport zones. Instead of configuring things individually NSX uses profiles so we have a consistent and central place to configure multiple components from. Let start with the Uplink profiles: Under Fabric head over to Profiles.\nHere we need to create two uplink profiles (one for the ESXi transport nodes and one for the NSX edge transport nodes). These profile will dictate the number of uplinks used, mtu size (only for the edges after NSX 3.1) ,vlan for the geneve tunnel and nic teaming. Here we also define multiple teaming policies if we want to dictate certain uplinks to be used for deterministic traffic steering. Which I will do.\nHost uplink:\nIn the \u0026quot;host\u0026quot; uplink profile we define the logical uplinks, (uplink-1 and uplink-2, but we could name them Donald-Duck-1 and 2 if we wanted). We define the default teaming-policy to be Load Balance Source as we want two vmkernels for the host-tep, and default active/active if we create a vlan segment without specifying a teaming policy in the segment. Then we add two more teaming policies with Failover Order and specify one uplink pr policy. The reason for that is because we will go on and create a VLAN segment later where we will place the Edge VM uplinks, and we need to have some control over wich uplink-\u0026gt;nic on the ESXi host the T0 Uplinks go, and we dont want to use teaming on these, and we dont want a standby uplink as BGP is supposed to handle failover for us. This way we steer T0 Uplink 1 out via uplink-1 and T0 Uplink 2 out via uplink-2 and the ESXi hosts has been configured to map uplink-1 to VDS uplink-1 which again is mapping to pNIC0 and uplink-2 to VDS uplink-2 to pNIC1 respectively. In summary we create a vlan segment on the host for the edge VM, then we create a vlan segment for the logical T0 later.\nThen we define the VLAN number for the Geneve tunnel. Notice we dont specify MTU size as this will adjust after what we have in our VDS which we will map to later, so our VDS must have minumum MTU 1700 defined (it works with 1600 also, but in NSX-T 3.2 and later there is a greenfield min MTU of 1700). We will use the same VLAN for both host and edge tep wich was supported from NSX-T 3.1 and forward. But to make that work we cant use VDS portgroups for the T0, it needs to be a NSX VLAN segment. More on that later.\nClick save.\nNext up is the \u0026quot;Edge\u0026quot; Uplink Profile, almost same procedure:\nThe biggest difference is the name of the specific teaming policies, and we specify a MTU size of 1700 as this is what I use in my VDS.\nNow over to Transport Zones\nHere we create three Transport Zones: 1 vlan TZ for host, 1 vlan TZ for edge and 1 overlay which is common for both Edge and Host transport nodes.\nCreate host-vlan-tz:\nIts probably not so obvious in the GUI, but we also define our teaming policies defined in our respective uplinks earlier. Here I enter manually the uplink policy names for my host transport zone so they can be available later when I create a VLAN segment for my T0 uplinks (on the host). Click save.\nWhen done its the edge-vlan-tz:\nCommon for both Transport Zones is VLAN.\nNow the last Transport Zone for now - the Overlay TZ (this one is easy):\nThe next step would be to create the Transport Node Profile, but first we need to create an IP-Pool for the TEP addresses. Head over to Networking and IP Address Pools :\nAdd IP address pool:\nIts only necessary to define the CIDR, IP Range and Gateway IP. Please make sure the range is sufficient to support the max amount of ESXi Transport nodes and Edge nodes you will have. 2 IPs pr device.\nNow back to System -\u0026gt; Fabric again and create the Transport Node Profile under Profile:\nHere we select the vCenter we added earlier, point to the correct VDS we want to use, select the host-profile we created and under IP assignment we use our newly created IP pool and map the uplinks to the corresponding VDS uplinks. Then ADD\nInstall NSX components on the ESXi hosts To enable NSX after our initial configs has been done is fairly straight-forward. Head over to Nodes under System-\u0026gt;Fabric and select the first tab Host Transport Nodes. Select your vCenter under Managed by and select your cluster and click Configure NSX:\nSelect your (only?) transport-node profile your created above:\nClick apply and wait...\nStatus in vCenter - the only status we will see.\nWhen everything is up and green as below, NSX is installed in our ESXi hosts and we are ready to create networks 😄\nDeploy NSX Edges Deploying a Edge is quite straight forward, and is done from the NSX manager under System -\u0026gt;Fabric -\u0026gt;Nodes -\u0026gt;Edge Transport Nodes\nBut I need to create two VLAN segments for Edge \u0026quot;data-path/uplink interfaces. As I want to use the same VLAN for both Host Tep and Edge TEP I need to do that. These two segments will only be used for the actual Edge VMs, not the T0 I am going to create later. In my lab I am using VLAN 1013 for TEP and VLAN 1014 and 1015 for T0 Uplink 1 and 2. So that means the first VLAN segment I create will have the VLAN Trunk range 1013-1014 and the second VLAN segment will use VLAN trunk 1013,1015 Head over to the Networking section/Segments in the NSX UI click Add Segment:\nSelect host-uplink-1 under Uplink Teaming Policy and add your VLAN ID under VLAN. Click save\nSame procedure again for the second segment:\nselect host-uplink-2 and the correct vlan trunk accordingly.\nThe result should be two VLAN segments, created in our host-vlan-tz (the host vlan transport zone created earlier)\nNow we can deploy our Edge(s).\nHead over to System -\u0026gt;Fabric -\u0026gt;Nodes -\u0026gt;Edge Transport Nodes\nClick add edge node and start the edge deployment wizard:\nGive the edge a name, then fill in a FQDN name. Not sure if that part have to be actually registered in DNS, but it probably does do any harm if you decide to. Choose a form factor. When using Tanzu it can be potentially many virtual services so you should at least go with Large. You can find more sizing recommendation on the VMware official NSX docs page. Next\nFill inn your username and passwords (if you want easier access to SSH shell for troubleshooting purposes enable SSH now).\nSelect your vCenter, cluster and datastore. The rest default.\nConfigure the basics... This the part of the Edge that communicates with the NSX manager. Next we will configure the networking part that will be used for the T0 uplinks and TEP.\nHere we select the transport zones for the Edge to be part of. Note, it should be part of your overlay transport zone but not part of the host vlan transport zone. Here we have defined a Edge vlan transportzone to be used. This is the transport zone where the segment for the T0 to be created in. One of the reason is that we dont want the segment for the T0 to be visible for the host for potentially other workloads, and the segment is actually created in the Edge, thats where the T0 is realised (The SR part of the T0). Then we select the edge-profile we created earlier, the same IP pool as the hosts. Under uplinks we select the respective vlan segments uplink 1 and 2 created earlier.\nThen finish. It should take a couple of minutes to report ready in the NSX manager ui.\nStatus when ready for duty:\nThere should also be some activity in vCenter deploying the edge. When ready head over to Edge Clusters and create a cluster to put the Edge in. We need an Edge cluster and the edges in an edge cluster before we can do anything with them, even if we only deploy one edge (labs etc).\nThe T0 Now that at least one Edge is up, we should create a T0 so we can make some external connectivity happen (even though NSX have its own networking components and we can create full L3 topology, we cant talk outside NSX from overlay without the Edges). Head over to Network and create a VLAN segment. This time the segment should be placed in the edge-vlan-tz as it is use for T0 uplinks only. Select teaming policy and correct vlan for the T0 uplink 1. I will only use 1 uplink in my lab so I will only create 1 segment for this, I only have 1 upstream router to peer to also.\nNext is heading over to Tier-0 and create a T0:\nThe selection is very limited at first, so give it a name and select HA mode, and edge cluster (the one that we created above).\nClick save and yes to continue edit:\nNow we need to add the interface(s) to the T0. The actual interfaces will be residing on the Edges, but we need to define them in the T0. Click on the 0 under Interfaces (its already two interfaces in the screenshot below).\nGive the interface a name, choose type External, give it the correct IP address to peer with the upstream router, and select the Edge VLAN segment created earlier which maps to the correct uplink (1 or 2). Then select the Edge node that shall have this interface configured.\nClick save. Now as an optional step SSH into the Edge selected above, go the correct vrf and ping the upstream router.\nGet the correct VRF (we are looking for the SR T0 part of the T0)\nEnter the vrf by typing vrf and the number, here it is vrf 1.\nListing the interface with get interfaces one should see the interface we configured above, and we can ping the upstream router to verify L2 connectivity.\nGood, now configure BGP. Expand BGP in your T0 settings view (same place as we configure the interface) adjust your BGP settings accordingly. Click save, enter again and add your BGP peers/neighbors bly clicking on neighbors.\nAdd the IP to the BGP peer you should use and adjust accordingly, like AS number. Click Save\nIt will become green directly, then if you click refresh it will become red, then refresh again it should be green again if everything is correct BGP config wise on both sides. Clicking on the (i) will give you the status also:\nFrom your upstream routes you should see a new neighbor established:\nThe last step on the T0 now is to configure it which networks it should advertise on BGP. That is done under Route re-distribution. In a Tanzu setup we need to advertise NAT, connected and LB VIP from our T1s. That is because Tanzu or NCP creates NAT rules, it creates some LB VIPS and we should also be able to reach our other Overlay segments we create under our T1 (which we have not created yet).\nNow that T0 is configured and peering with the upstream router, I can create segments directly under the T0, or create T1s and then segments connected to the T1 instead. If you do create segments directly attached to the T0 one must configure route advertisement accordingly. As the config above is not advertising any networks from T0, only T1.\nIn NSX there is a neat map over the Network Topology in NSX:\nDeploy Tanzu with NSX Now that networking with NSX is configured and the foundation is ready. Its time to deploy the Supervisor cluster, or enable WCP, Workload Management. Head over to the hamburger menu in top left corner in your vCenter and select * Workload Management*\nClick on Get Started\nThen follow the wizard below:\nSelect NSX and Next\nSelect Cluster Deployment, choose your vCenter cluster, give the supervisor a name and below (not in picture above) enter a zone name.\nNext\nSelect your storage policies, Next\nIn Step 4 - Management Network we configure the network for the Supervisor Control Plane VMs. In my lab I have already created an overlay segment I call ls-mgmt with cidr 10.101.10.0/24 and gateway 10.101.10.1. So I will place my Supervisors also there. Not using DHCP, but just defining a start IP. The Supervisors will consist of three VMs, and a cluster IP. But it will use 5 IP addresses in total in this network. DNS, Search Domains and NTP should be your internal services. In screenshot above I have used an external NTP server. NEXT\nAbove I define the workload network, which the supervisor control plane vms also will be part of, but also for the vSphere Pods. This a default workload network where your TKC cluster can be deployed in (if not overriden when creating a vSphere namespace (later on that topic) ). Select the VDS you have used and configured for NSX. Select your Edge cluster. Add your DNS server(s), select the T0 router you created earlier in NSX. Then leave NAT-Mode enabled, we can create vSphere namespaces later where we override these settings. Then you define the Namespace Network. This is the network your vSphere pods will use, the workload network interface of the Supervisor ControlPlane Nodes, and your TKC cluster nodes. The CIDR size define how many IPs you will have available for your TKC cluster nodes, meaning also the total amount of nodes in this workload network. But dont despair, we can create additional vSphere namespaces and add more networks. So in the above example I give it a /20 cidr (unnecessary big actually, but why not). This Namespace Network will not be exposed to the outside world as NCP creates route-map rules on the T0 not allowing these to be advertised (we have NAT enabled). The Service CIDR is Kubernetes internal network for services. When we deploy a TKC cluster later we define other Kubernetes cluster and pod cidrs. Define the Ingress CIDR, this is the IP address range NCP will use to carve out LoadBalancer VIPs for the Kubernetes API endpoints, for the Supervisor Control Plane, the TKC clusters K8s Api endpoint and even the Service Type LoadBalancer services you decide to created. So all access TO the Supervisor Cluster API endpoint will be accessed through the IP address assigned from this CIDR. When we have NAT enabled it will also ask you to define a Egress CIDR which will be used by NSX to create SNAT rules for the worker nodes to use when communicate OUT. These NAT rules will be created automatically in NSX-T.\nNEXT\nSelect the size of your SVCP and give the SVCP API endpoint a name. This is something that can be registered in DNS when deployment is finished and we know the IP it gets.\nFinish and wait. Its the same (waiting) process as explained here\nIf everything goes well we should have a Supervisor cluster up and running in not that many minutes. 20-30 mins?\nWhen its done deploying you will see the green status here: Notice the Control Plane Node Address, this is our endpoint IP being served by Avi loadbalancer and the one we use to interact with the Supervisor cluster to create workloads etc..\nEnter the IP in your browser with https://10.101.11.2 and you should see this page: Download the cli tools for your operating system, deploy them so they are in your path. Will use both kubectl and kubectl-vsphere later on Next steps include creating namespace, deploy workload clusters.\nNSX components configured by WCP/NCP Before heading over to next section, have a look in NSX and see what happended there:\nUnder Networks -\u0026gt; Segments you will notice networks like this:\nNotice the gateway ip and CIDR. These are created for each vSphere Namespace, the cidr 10.101.96.1/28 is carved out of the CIDR defined in the \u0026quot;default\u0026quot; network when deploying WCP.\nUnder Networks -\u0026gt; T-1 Gateways you will notice a couple of new T1 routers being created:\nUnder Networks -\u0026gt; Load Balancing:\nHere is all the L4 K8s API endpoints created, also the other Service Type Loadbalancer services you choose to expose in your TKC clusters.\nUnder Networks -\u0026gt; NAT and the \u0026quot;domain\u0026quot; T1 there will be auto-created NAT rules, depending on whether NAT is enabled or disabled pr Namespace.\nThen under Security -\u0026gt; Distributed Firewall there will be a new section:\nvSphere Namespace When Supervisor is up and running next step is to create a vSphere Namespace. I will go ahead and create that, but will also use the \u0026quot;override network\u0026quot; to create a separate network for this Namespace and also disable NAT as I want to use this cluster for Antrea Egress explained here.\nA vSphere is a construct in vSphere to adjust indidivual access settings/permissions, resources, network settings or different networks for IP separation. Click on Create Namespace and fill in relevant info. I am choosing to Override Supervisor network settings.\nNotice when I disable NAT Mode there will no longer be a Egress IP to populate. Thats because the TKC nodes under this namespace will not get a NAT rule applied to them in NSX and will be communicating \u0026quot;externally\u0026quot; with their own actual IP address. Ingress will still be relevant as the NSX-T Loadbalancer will create the K8s API endpoint to reach the control plane endpoint your respective TKC clusters.\nThis option to adjust the networks in such a degree is a great flexibility with NSX. Tanzu with VDS does give you the option to select different VDS portgroups on separate vlans, but must be manually created, does not give you NAT, Ingress and Egress and VLAN/Routing must be in place in the physical environment. With NSX all the networking components are automatically created and it includes the NSX Distributed Firewall.\nAfter the vSphere Namespace has been created it is the same procedure to deploy TKC clusters regardless of using VDS or NSX. So instead of me repeating myself and saving the environment for digital ink I will refere to the process I have already described here\nConfigure Avi as Ingress controller (L7) with NSX as L4 LB When using Antrea as the CNI in your TKC cluster (default in vSphere with Tanzu) make sure to enable NodePortLocal. This gives much better control and flexibility, follow how here and NodePortLocal explained here\nConfigure NSX cloud in Avi - network preparations As this guide is using NSX as the underlaying networking platform instead of VDS as this article is using, we also have the benefit of configuring the NSX cloud in Avi instead of the vCenter Cloud. This cloud do come with some additional benefits like automatic Security Group creation in NSX, VIP advertisement through the already configured T0, SE placement dataplane/data network on separate network and VRF context.\nBut before we can consume the NSX cloud we need to configure it. Assuming the Avi controller has already been deployed and initial config is done (username and password, dns, etc) log in to the controller and head over to Administration -\u0026gt; User Credentials:\nAdd the credentials you want to use for both vCenter and NSX-T. The next step is involving NSX. Head over to NSX and create two new networks. One for SE management and one for SE dataplane (the network it will use to reach the backend servers they are loadbalancing). I will now just show a screenshot of three networks already created. One segment ls-avi-se-mgmt for SE mgmt, one segment ls-avi-generic-se-data for SE communication to backend pools, and one segment ls-avi-dns-se-data (I will get back to the last network later when enabling the Avi DNS service).\nThen head over to vCenter and create a local Content Library and call it what you want.\nWhen networks and credentials have been created, back in the Avi gui head over to Infrastructure -\u0026gt; Clouds:\nClick Create and select NSX-T\nStart by giving the NSX cloud a meaningfull name and give a prefix name for the objects being created in NSX by Avi. If you have several Avi controllers using same NSX manager etc. Easier to identify when looking in the NSX manager ui:\nThen proceed (scroll down if needed) to connect to you NSX manager:\nEnter the IP of your NSX manager, if you created a Cluster IP in the NSX manager use that one, and select the credentials for NSX manager create in the Avi earlier.\nConnect.\n!Note Before being able to continue this step we need to have defined some networks in NSX as explained above...\nFill in the Transport zone where you have defined the segment for the management network, select the T1 this segment is attached to and then the segment. This network is created in NSX for the SE management interface. Then go to Data Networks and select the Overlay Transport Zone where the ls-avi-generic-se-data segment is created the ADD the T1 router for this segment from the dropdown list and the corresponding segment. (Ignore the second netowork in the screenshot above.) Then under vCenter Server click add and fill in relevant info to connect the vCenter server your NSX manager is using.\nGive it a meaningful name, it will either already have selected your NSX cloud otherwise select it from the list. Then the credentials and the Content Library from the dropdown.\nThe last section IPAM/DNS we will fill out later (click save):\nNow the cloud should be created.\nSE Groups Head over to Cloud Resources -\u0026gt; Service Engine Groups and create a custom SE-group.\nSelect your newly created cloud from the dropdown list above next to Select Cloud. Click create blue button right side.\nChange the fields marked by a red line. And optionally adjust the Max. Number of Service Engines. This is just to restrict Avi to not deploy too many SE's if it sees fit. Adjust the Virtual Services pr Service Engine to a higher number than 10 (if that is default). This all comes down to performance.\nJump to advanced:\nChange the Service Engine Name Prefix to something useful, then select the vCenter, cluster and datastore. Save\nAvi Networks Now head over to Infrastructure -\u0026gt; VRF Contexts (below Service engine Groups) select correct cloud from dropdown and add a default gateway for the SE dataplane network.\nClick the pencil\nAdd a default route under Static Route and point to the gateway used for the SE dataplane ls-avi-generic-se-data . This is used for the SE's to know where to go to reach the different pools it will loadbalance.\nNext we need to define the different networks for the SE's to use (the dataplane network). Head over to Infrastructure -\u0026gt; Networks (again select correct cloud from dropdown)\nIf there is workload running in these networks they can already be auto-detected, if not you need to create them. In my lab I rely on Avi as the IPAM provider for my different services, a very useful feature in Avi. So the two networks we need to create/and or edit is the ls-avi-se-mgmt and the ls-avi-generic-se-data\nFirst out is the ls-avi-se-mgmt\nDeselect DHCP, we want to use Avi IPAM instead (and I dont have DHCP in these networks). Then fill in the CIDR (IP Subnet), deselect Use Static IP Address for VIPs and Service Engine add a range for the SE to be allow to get an IP from. Then select the Use for Service Engines. This will configure the IPAM pool to only be used for the dataplane for the SE's in the management network. We dont want to have any VIPs here as it will only be used for the SE's to talk to the Avi controller.\nThen it is the ls-avi-generic-se-data network\nSame as above just a different subnet, using the same T1 router defined in NSX.\nThe two above networks will only be used as dataplane network. Meaning they will not be used for any Virtual Service VIP. We also need to define 1 or more VIP networks. Create one more network:\nHere I specify a network which is only used for VS VIP\nAvi IPAM/DNS template Now we need to inform our cloud which VIP networks we can use, that is done under Templates -\u0026gt; IPAM/DNS Profiles\nWhile we are here we create two profiles, one for DNS (for the DNS service) and IPAM profile for the VIP networks. Lets start with the IPAM profile. Click create right corner and select IPAM Profile:\nFill in name, select allocate IP in VRF and select your NSX cloud. Then click add and select your VIP networks defined above. Screenshot below also include a DNS-VIP network which I will use later. Click save.\nNow click create again and select DNS profile:\nGive it a name and type in the domain name you want Avi to use.\nNow go back to Infrastructure -\u0026gt; Clouds and edit your NSX cloud and add the newly added Profiles here:\nSave\nNow Avi is prepared and configured to handle requests from AKO on L7 service Ingress. Next step will be to deploy configure and deploy AKO in our TKC cluster. But first some short words around Avi DNS service.\nAvi DNS service Getting a virtual service with a VIP is easy with Avi, but often we need a DNS record on these VS'es. Avi has a built in DNS service which automatically register a DNS record for your services. The simplest way to make this work out of the box is to create a Forward Zone in your DNS server to the DNS Service IP for a specific subdomain or domain. Then Avi will handle the DNS requests for these specific domains. To make use of Avi DNS service we should dedicate a SE group for this service, and create a dedicated VIP network for it. As we should use a dedicated SE group for the DNS service it would be nice to also have a dedicated SE dataplane network for these SE's. So follow the steps I have done above to create a SE network for the SE service SE's and add this to your cloud. The VIP network also needs to be added to the IPAM Profile created earlier. A note on the additional SE network, this also requires a dedicated T1 router in the NSX environment. So in your NSX environment create an additional T1 router, create segment for the DNS SE datanetwork. This is how to enable the DNS service in Avi after you have prepared the networks, and IPAM profile:\nHead over to Administration -\u0026gt; Settings -\u0026gt; DNS Service:\nThen create virtual service:\nSelect your cloud and configure the DNS service:\nThe VS VIP is configured with a static IP (outside of the DNS VIP IPAM range you have created)\nUnder advanced select the SE group:\nSave. Now the DNS VS is configured, go to templates and add a DNS profile:\nGive it a name, add your domain(s) here. Save\nHead over to the cloud and add your DNS profile.\nNow you just need to configure your backend DNS server to forward the requests for these domains to the Avi DNS VS IP. Using bind this can be done like this:\n1zone \u0026#34;you-have.your-domain.here\u0026#34; { 2 type forward; 3 forward only; 4 forwarders { 10.101.211.9; }; 5}; AKO in TKC I have already deployed a TKC cluster, which is described here\nAlso make sure Antrea is configured with NodePortLocal as described also in the link above.\nSo for Avi to work as Ingress controller we need to deploy AKO (Avi Kubernetes Operator). I have also explained these steps here the only difference is how the value.yaml for AKO is configured. Below is how I have configured it to work in my NSX enabled environment with explanations:\n1# this file has been edited by me to easier reflect the changes I have done. So all default comments have been removed, and contains only my comments. 2replicaCount: 1 3 4image: 5 repository: projects.registry.vmware.com/ako/ako 6 pullPolicy: IfNotPresent 7 8 9AKOSettings: 10 primaryInstance: true 11 enableEvents: \u0026#39;true\u0026#39; 12 logLevel: WARN 13 fullSyncFrequency: \u0026#39;1800\u0026#39; 14 apiServerPort: 8080 15 deleteConfig: \u0026#39;false\u0026#39; 16 disableStaticRouteSync: \u0026#39;false\u0026#39; 17 clusterName: wdc-tkc-cluster-1-nsx # Here we need to define a name for our specific TKC cluster. This must not be the exact names as the cluster itself, but why not, it MUST be unique across all your TKC clusters if you have multiple AKO enabled TKC/K8s clusters on same Avi controller 18 cniPlugin: \u0026#39;antrea\u0026#39; #This needs to be set to Antrea, the reason is that you would like to configure NodePortLocal. 19 enableEVH: false 20 layer7Only: true # This is very important to set to true as we already have an other AKO instance managing L4 for our k8s api endpoints. We will only configure this instance to use L7. 21 22 namespaceSelector: 23 labelKey: \u0026#39;\u0026#39; 24 labelValue: \u0026#39;\u0026#39; 25 servicesAPI: false 26 vipPerNamespace: \u0026#39;false\u0026#39; 27 28NetworkSettings: 29 nodeNetworkList: 30 # nodeNetworkList: 31 - networkName: \u0026#34;vnet-domain-c8:dd5825a9-8f62-4823-9347-a9723b6800d5-ns-wdc-1-tkc-cluste-62397-0\u0026#34; # this is the NSX segment created for your specific TKC cluster workers running in. In my case the defined portgroup name above. You can see this in vCenter 32 cidrs: 33 - 10.101.112.32/27 # this is the CIDR for your current TKC cluster (make sure you are using right CIDR, seen from NSX) 34 enableRHI: false 35 nsxtT1LR: \u0026#39;Da-Tier-1\u0026#39; #The T1 router in NSX you have defined for the avi-se-dataplane network 36 bgpPeerLabels: [] 37 # bgpPeerLabels: 38 # - peer1 39 # - peer2 40 vipNetworkList: 41 - networkName: \u0026#34;vip-tkc-cluster-1-nsx-wdc-l7\u0026#34; # This can be the same VIP network you have configured in previously for api endpint or it can be a completely new one. I am going the easy route using the same. It can be shared across multiple cluster (is using NodePortLocal), or can be specific for each tkc cluster. 42 cidr: 10.101.210.0/24 43 44L7Settings: 45 defaultIngController: \u0026#39;true\u0026#39; # Specify if this is the only Ingress controller you have or default if using several others. 46 noPGForSNI: false 47 serviceType: NodePortLocal # Here we select nodeportlocal - verify that Antrea is configured to use NodePortLocal 48 shardVSSize: SMALL # I am setting this to small so I can run more services using same IP. 49 passthroughShardSize: SMALL 50 enableMCI: \u0026#39;false\u0026#39; 51 52L4Settings: 53 defaultDomain: \u0026#39;\u0026#39; 54 autoFQDN: default 55 56 57ControllerSettings: 58 serviceEngineGroupName: nsx-se-generic-group # If you dont decide to use the same VIP as k8s api endpoint you could decide to create an additional ServiceEngineGroup for your L7 services (IP separation etc). 59 controllerVersion: \u0026#39;22.1.1\u0026#39; # AKO version 1.8.2 supports Avi 22.1.2 60 cloudName: wdc-1-nsx # The configured cloud name on the Avi controller. 61 controllerHost: \u0026#39;172.21.101.50\u0026#39; # IP address or Hostname of Avi Controller 62 tenantName: admin 63 64nodePortSelector: 65 key: \u0026#39;\u0026#39; 66 value: \u0026#39;\u0026#39; 67 68resources: 69 limits: 70 cpu: 350m 71 memory: 400Mi 72 requests: 73 cpu: 200m 74 memory: 300Mi 75 76podSecurityContext: {} 77 78rbac: 79 pspEnable: false 80 81 82avicredentials: 83 username: \u0026#39;admin\u0026#39; # username for the Avi controller 84 password: \u0026#39;password\u0026#39; # password for the Avi controller 85 authtoken: 86 certificateAuthorityData: 87 88 89persistentVolumeClaim: \u0026#39;\u0026#39; 90mountPath: /log 91logFile: avi.log Install AKO with this command:\n1helm install ako/ako --generate-name --version 1.8.2 -f values.yaml --namespace=avi-system Check the logs of the AKO pod if it encountered some issues or not by issuing the command:\n1kubectl logs -n avi-system ako-o If there is no errors there its time to deploy a couple of test applications and the Ingress itself. This is already described here\nThats it. Now L7 is enabled on your TKC cluster with Avi as Ingress controller. There is much that can be configured with AKO CRDs. I will try to update my post here to go through the different possibilites. In the meantime much information is described here\nAviInfraSetting If you need to have separate VIPs/different subnets for certain applications we can use AviInfraSetting to override the \u0026quot;default\u0026quot; settings configured in our values.yaml above. This is a nice feature to override some settings very easy. There is also the option to run several AKO instances pr TKC/k8s cluster like described here which I will go through another time. But now quickly AviInfraSetting\nLets say I want to enable BPG on certain services or adjust my Ingress to be exposed on a different VIP network.\nCreate a yaml definition for AviInfraSetting:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: AviInfraSetting 3metadata: 4 name: enable-bgp-fruit 5spec: 6 seGroup: 7 name: Default-Group 8 network: 9 vipNetworks: 10 - networkName: vds-tkc-frontend-l7-vlan-1028 11 cidr: 10.102.8.0/24 12 nodeNetworks: 13 - networkName: vds-tkc-workload-vlan-1026 14 cidrs: 15 - 10.102.6.0/24 16 enableRhi: true 17 bgpPeerLabels: 18 - cPodRouter In the example above I define the VIP network (here I can override the default confgured from value.yaml), the nodNetwork. Enable RHI, and define a label to be used for BGP (label is from BGP settings here):\nPeer:\nApply the above yaml. To use it, create an additional IngressClass like this:\n1apiVersion: networking.k8s.io/v1 2kind: IngressClass 3metadata: 4 name: avi-lb-bgp #name the IngressClass 5spec: 6 controller: ako.vmware.com/avi-lb #default ingressclass from ako 7 parameters: 8 apiGroup: ako.vmware.com 9 kind: AviInfraSetting #refer to the AviInfraSetting 10 name: enable-bgp-fruit #the name of your AviInfraSetting applied Apply it, then when you apply your Ingress or update it refer to this ingressClass like this:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 namespace: fruit 6 7spec: 8 ingressClassName: avi-lb-bgp #Here you choose your specific IngressClass 9 rules: 10 - host: fruit-tkgs.you-have.your-domain.here 11 http: 12 paths: 13 - path: /apple 14 pathType: Prefix 15 backend: 16 service: 17 name: apple-service 18 port: 19 number: 5678 20 - path: /banana 21 pathType: Prefix 22 backend: 23 service: 24 name: banana-service 25 port: 26 number: 5678 ","link":"https://yikes.guzware.net/2022/10/26/vsphere-8-with-tanzu-using-nsx-t-avi-loadbalancer/","section":"post","tags":["Tanzu","Kubernetes","NSX","Avi"],"title":"vSphere 8 with Tanzu using NSX-T \u0026 Avi LoadBalancer"},{"body":"What is AKO? AKO is an operator which works as an ingress controller and performs Avi-specific functions in an OpenShift/Kubernetes environment with the Avi Controller. It runs as a pod in the cluster and translates the required OpenShift/Kubernetes objects to Avi objects and automates the implementation of ingresses/routes/services on the Service Engines (SE) via the Avi Controller. ref: link\nHow to install AKO AKO is very easy installed with Helm. Four basic steps needs to be done.\nCreate a namespace for AKO in your kubernetes cluster: kubectl create ns avi-system Add AKO Helm reposistory: helm repo add ako https://projects.registry.vmware.com/chartrepo/ako Get the current values for the versions you want: helm show values ako/ako --version 1.9.1 \u0026gt; values.yaml Deploy (after values have been edited to suit your environment): helm install ako/ako --generate-name --version 1.9.1 -f values.yaml -n avi-system AKO Helm values explained Before deploying AKO there are some parameters that should be configured, or most likely the deployment will fail. Below is an example file where the different fields are explained:\n1# Default values for ako. 2# This is a YAML-formatted file. 3# Declare variables to be passed into your templates. 4 5replicaCount: 1 6 7image: 8 repository: projects.registry.vmware.com/ako/ako #If using your own registry update accordingly 9 pullPolicy: IfNotPresent 10 11### This section outlines the generic AKO settings 12AKOSettings: 13 primaryInstance: true # Defines AKO instance is primary or not. Value `true` indicates that AKO instance is primary. In a multiple AKO deployment in a cluster, only one AKO instance should be primary. Default value: true. 14 enableEvents: \u0026#39;true\u0026#39; # Enables/disables Event broadcasting via AKO 15 logLevel: WARN # enum: INFO|DEBUG|WARN|ERROR 16 fullSyncFrequency: \u0026#39;1800\u0026#39; # This frequency controls how often AKO polls the Avi controller to update itself with cloud configurations. 17 apiServerPort: 8080 # Internal port for AKO\u0026#39;s API server for the liveness probe of the AKO pod default=8080 18 deleteConfig: \u0026#39;false\u0026#39; # Has to be set to true in configmap if user wants to delete AKO created objects from AVI 19 disableStaticRouteSync: \u0026#39;false\u0026#39; # If the POD networks are reachable from the Avi SE, set this knob to true. 20 clusterName: my-cluster # A unique identifier for the kubernetes cluster, that helps distinguish the objects for this cluster in the avi controller. // MUST-EDIT 21 cniPlugin: \u0026#39;\u0026#39; # Set the string if your CNI is calico or openshift. enum: calico|canal|flannel|openshift|antrea|ncp 22 enableEVH: false # This enables the Enhanced Virtual Hosting Model in Avi Controller for the Virtual Services 23 layer7Only: false # If this flag is switched on, then AKO will only do layer 7 loadbalancing.Must be true if used in a TKC cluster / Tanzu with vSphere 24 # NamespaceSelector contains label key and value used for namespacemigration 25 # Same label has to be present on namespace/s which needs migration/sync to AKO 26 namespaceSelector: 27 labelKey: \u0026#39;\u0026#39; 28 labelValue: \u0026#39;\u0026#39; 29 servicesAPI: false # Flag that enables AKO in services API mode: https://kubernetes-sigs.github.io/service-apis/. Currently implemented only for L4. This flag uses the upstream GA APIs which are not backward compatible 30 # with the advancedL4 APIs which uses a fork and a version of v1alpha1pre1 31 vipPerNamespace: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to create Parent VS per Namespace in EVH mode 32 istioEnabled: false # This flag needs to be enabled when AKO is be to brought up in an Istio environment 33 # This is the list of system namespaces from which AKO will not listen any Kubernetes or Openshift object event. 34 blockedNamespaceList: [] 35 # blockedNamespaceList: 36 # - kube-system 37 # - kube-public 38 ipFamily: \u0026#39;\u0026#39; # This flag can take values V4 or V6 (default V4). This is for the backend pools to use ipv6 or ipv4. For frontside VS, use v6cidr 39 40 41### This section outlines the network settings for virtualservices. 42NetworkSettings: 43 ## This list of network and cidrs are used in pool placement network for vcenter cloud. 44 ## Node Network details are not needed when in nodeport mode / static routes are disabled / non vcenter clouds. 45 nodeNetworkList: [] 46 # nodeNetworkList: 47 # - networkName: \u0026#34;network-name\u0026#34; 48 # cidrs: 49 # - 10.0.0.1/24 50 # - 11.0.0.1/24 51 enableRHI: false # This is a cluster wide setting for BGP peering. 52 nsxtT1LR: \u0026#39;\u0026#39; # T1 Logical Segment mapping for backend network. Only applies to NSX-T cloud. 53 bgpPeerLabels: [] # Select BGP peers using bgpPeerLabels, for selective VsVip advertisement. 54 # bgpPeerLabels: 55 # - peer1 56 # - peer2 57 vipNetworkList: [] # Network information of the VIP network. Multiple networks allowed only for AWS Cloud. 58 # vipNetworkList: 59 # - networkName: net1 60 # cidr: 100.1.1.0/24 61 # v6cidr: 2002:🔢abcd:ffff:c0a8:101/64 # Setting this will enable the VS networks to use ipv6 62### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO. 63L7Settings: 64 defaultIngController: \u0026#39;true\u0026#39; 65 noPGForSNI: false # Switching this knob to true, will get rid of poolgroups from SNI VSes. Do not use this flag, if you don\u0026#39;t want http caching. This will be deprecated once the controller support caching on PGs. 66 serviceType: ClusterIP # enum NodePort|ClusterIP|NodePortLocal. NodePortLocal can only be used if Antrea is the CNI 67 shardVSSize: LARGE # Use this to control the layer 7 VS numbers. This applies to both secure/insecure VSes but does not apply for passthrough. ENUMs: LARGE, MEDIUM, SMALL, DEDICATED 68 passthroughShardSize: SMALL # Control the passthrough virtualservice numbers using this ENUM. ENUMs: LARGE, MEDIUM, SMALL 69 enableMCI: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to start processing multi-cluster ingress objects. 70 71### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO. 72L4Settings: 73 defaultDomain: \u0026#39;\u0026#39; # If multiple sub-domains are configured in the cloud, use this knob to set the default sub-domain to use for L4 VSes. 74 autoFQDN: default # ENUM: default(\u0026lt;svc\u0026gt;.\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), flat (\u0026lt;svc\u0026gt;-\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), \u0026#34;disabled\u0026#34; If the value is disabled then the FQDN generation is disabled. 75 76### This section outlines settings on the Avi controller that affects AKO\u0026#39;s functionality. 77ControllerSettings: 78 serviceEngineGroupName: Default-Group # Name of the ServiceEngine Group. 79 controllerVersion: \u0026#39;\u0026#39; # The controller API version 80 cloudName: Default-Cloud # The configured cloud name on the Avi controller. 81 controllerHost: \u0026#39;\u0026#39; # IP address or Hostname of Avi Controller 82 tenantName: admin # Name of the tenant where all the AKO objects will be created in AVI. 83 84nodePortSelector: # Only applicable if serviceType is NodePort 85 key: \u0026#39;\u0026#39; 86 value: \u0026#39;\u0026#39; 87 88resources: 89 limits: 90 cpu: 350m 91 memory: 400Mi 92 requests: 93 cpu: 200m 94 memory: 300Mi 95 96securityContext: {} 97 98podSecurityContext: {} 99 100rbac: 101 # Creates the pod security policy if set to true 102 pspEnable: false 103 104 105avicredentials: 106 username: \u0026#39;\u0026#39; 107 password: \u0026#39;\u0026#39; 108 authtoken: 109 certificateAuthorityData: 110 111 112persistentVolumeClaim: \u0026#39;\u0026#39; 113mountPath: /log 114logFile: avi.log More info here\n","link":"https://yikes.guzware.net/2022/10/26/ako-explained/","section":"post","tags":["Kubernetes","Ingress","Loadbalancing"],"title":"AKO Explained"},{"body":"","link":"https://yikes.guzware.net/tags/ingress/","section":"tags","tags":null,"title":"Ingress"},{"body":"","link":"https://yikes.guzware.net/categories/ingress/","section":"categories","tags":null,"title":"Ingress"},{"body":"","link":"https://yikes.guzware.net/tags/loadbalancing/","section":"tags","tags":null,"title":"Loadbalancing"},{"body":"","link":"https://yikes.guzware.net/tags/ako/","section":"tags","tags":null,"title":"AKO"},{"body":"","link":"https://yikes.guzware.net/tags/amko/","section":"tags","tags":null,"title":"AMKO"},{"body":"","link":"https://yikes.guzware.net/categories/gslb/","section":"categories","tags":null,"title":"GSLB"},{"body":"Global Server LoadBalancing in VMware Tanzu with AMKO This post will go through how to configure AVI (NSX ALB) with GSLB in vSphere with Tanzu (TKGs) and an upstream k8s cluster in two different physical locations. I have already covered AKO in my previous posts, this post will assume knowledge of AKO (Avi Kubernetes Operator) and extend upon that with the use of AMKO (Avi Multi-Cluster Kubernetes Operator). The goal is to have the ability to scale my k8s applications between my \u0026quot;sites\u0026quot; and make them geo-redundant. For more information on AVI, AKO and AMKO head over here\nPreparations and diagram over environment used in this post This post will involve a upstream Ubuntu k8s cluster in my home-lab and a remote vSphere with Tanzu cluster. I have deployed one Avi Controller in my home lab and one Avi controller in the remote site. The k8s cluster in my home-lab is defined as the \u0026quot;primary\u0026quot; k8s cluster, the same goes for the Avi controller in my home-lab. There are some networking connectivity between the AVI controllers that needs to be in place such as 443 (API) between the controllers, and the AVI SE's needs to reach the GSLB VS vips on their respective side for GSLB health checks. Site A SE's dataplane needs connectivity to the vip that is created for the GSLB service on site B and vice versa. The primary k8s cluster also needs connectivity to the \u0026quot;secondary\u0026quot; k8s clusters endpoint ip/fqdn, k8s api (port 6443). AMKO needs this connectivity to listen for \u0026quot;GSLB\u0026quot; enabled services in the remote k8s clusters which triggers AMKO to automatically put them in your GSLB service. More on that later in the article. When all preparations are done the final diagram should look something like this:\n(I will not cover what kind of infrastructure that connects the sites together as that is a completely different topic and can be as much). But there will most likely be a firewall involved between the sites, and the above mentioned connectivity needs to be adjusted in the firewall. In this post the following ip subnets will be used:\nSE Dataplane network home-lab: 10.150.1.0/24 (I only have two se's so there will be two addresses from this subnet) (I am running the all services on the same two SE's which is not recommended, one should atleast have dedicated SE's for the AVI DNS service) SE Dataplane network remote-site: 192.168.102.0/24 (Two SE's here also, in remote site I do have dedicated SE's for the AVI DNS Service but they will not be touched upon in this post only the SE's responsible for the GSLB services being created) VIP subnet for services exposed in home-lab k8s cluster: 10.150.12.0/24 (a dedicated vip subnet for all services exposed from this cluster) VIP subnet for services exposed in remote-site tkgs cluster: 192.168.151.0/24 (a dedicated vip subnet for all services exposed from this cluster) For this network setup to work one needs to have routing in place, either with BGP enabled in AVI or static routes. Explanation: The SE's have their own dataplane network, they are also the ones responsible for creating the VIPs you define for your VS. So, if you want your VIPs to be reachable you have to make sure there are routes in your network to the VIPS where the SEs are next hops either with BGP or static routes. The VIP is what it is, a Virtual IP meaning it dont have its own VLAN and gateway in your infrastructure. It is created and realised by the SE's. The SE's are then the gateways for your VIPS. A VIP address could be anything. At the same time the SEs dataplane network needs connectivity to the backend servers it is supposed to loadbalance, so this dataplane network also needs routes to reach those. In this post that means the SE's dataplane network will need reachability to the k8s worker nodes where your apps are running in the home-lab site and in the remote site it needs reachability to the TKGs workers. On a sidenote I am not running routable pods, they are nat-ed trough my workers, and I am using Antrea as CNI with NodePortLocal configured. I also prefer to have a different network for the SE dataplane, different VIP subnets as it is easier to maintain control, isolation, firewall rules etc.\nThe diagram above is very high level, as it does not go into all networking details, firewall rules etc but it gives an overview of the communication needed.\nWhen one have an clear idea of the connectivity requirements we need to form the GSLB \u0026quot;partnership\u0026quot; between the AVI controllers. I was thinking back and forth whether I should cover these steps also but instead I will link to a good friends blog site here that does this brilliantly. Its all about saving the environment of unnecessary digital ink 😄. This also goes for AKO deployment. This is also covered here or from the AVI docs page here\nIt should look like this on both controllers when everything is up and ready for GSLB: It should be reflected on the secondary controller as well, except there will be no option to edit.\nTime to deploy AMKO in K8s AMKO can be deployed in two ways. It can be sufficient with only one instance of AMKO deployed in your primary k8s cluster, or you can go the federation approach and deploy AMKO in all your clusters that you want to use GSLB on. Then you will end up with one master instance of AMKO and \u0026quot;followers\u0026quot; or federation member on the others. One of the benefit is that you can promote one of the follower members if the primary is lost. I will go with the simple approach, deploy AMKO once, in my primary k8s cluster in my home-lab.\nAMKO preparations before deploy with Helm AMKO will be deployed by using Helm, so if Helm is not installed do that. To successfully install AMKO there is a couple of things to be done. First, decide which is your primary cluster (where to deploy AMKO). When you have decided that (the easy step) then you need to prepare a secret that contains the context/clusters/users for all the k8s clusters you want to use GSLB on. An example file can be found here. Create this content in a regular file and name the file gslb-members. The naming of the file is important, if you name it differently AMKO will fail as it cant find the secret. I have tried to find a variable that is able override this in the value.yaml for the Helm chart but has not succeeded, so I went with the default naming. When that is populated with the k8s clusters you want, we need to create a secret in our primary k8s cluster like this: kubectl create secret generic gslb-config-secret --from-file gslb-members -n avi-system. The namespace here is the namespace where AKO is already deployed in.\nThis should give you a secret like this:\n1gslb-config-secret Opaque 1 20h A note on kubeconfig for vSphere with Tanzu (TKGs) When logging into a guest cluster in TKGs we usually do this through the supervisor with either vSphere local users or AD users defined in vSphere and we get a timebased token. Its not possible to use this approach. So what I went with was to grab the admin credentials for my TKGs guest cluster and used that context instead. Here is how to do that. This is not a recommended approach, instead one should create and use a service account. Maybe I will get back to this later and update how.\nBack to the AMKO deployment...\nThe secret is ready, now we need to get the value.yaml for the AMKO version we will install. I am using AMKO 1.8.1 (same for AKO). The Helm repo for AMKO is already added if AKO has been installed using Helm, the same repo. If not, add the repo:\n1helm repo add ako https://projects.registry.vmware.com/chartrepo/ako Download the value.yaml:\n1 helm show values ako/amko --version 1.8.1 \u0026gt; values.yaml (there is a typo in the official doc - it points to just amko) Now edit the values.yaml:\n1# This is a YAML-formatted file. 2# Declare variables to be passed into your templates. 3 4replicaCount: 1 5 6image: 7 repository: projects.registry.vmware.com/ako/amko 8 pullPolicy: IfNotPresent 9 10# Configs related to AMKO Federator 11federation: 12 # image repository 13 image: 14 repository: projects.registry.vmware.com/ako/amko-federator 15 pullPolicy: IfNotPresent 16 # cluster context where AMKO is going to be deployed 17 currentCluster: \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name - for your leader/primary cluster 18 # Set to true if AMKO on this cluster is the leader 19 currentClusterIsLeader: true 20 # member clusters to federate the GSLBConfig and GDP objects on, if the 21 # current cluster context is part of this list, the federator will ignore it 22 memberClusters: 23 - \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name 24 - \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; #####use the context name 25# Configs related to AMKO Service discovery 26serviceDiscovery: 27 # image repository 28 # image: 29 # repository: projects.registry.vmware.com/ako/amko-service-discovery 30 # pullPolicy: IfNotPresent 31 32# Configs related to Multi-cluster ingress. Note: MultiClusterIngress is a tech preview. 33multiClusterIngress: 34 enable: false 35 36configs: 37 gslbLeaderController: \u0026#39;172.18.5.51\u0026#39; ##### MGMT ip leader/primary avi controller 38 controllerVersion: 22.1.1 39 memberClusters: 40 - clusterContext: \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name 41 - clusterContext: \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; #####use the context name 42 refreshInterval: 1800 43 logLevel: INFO 44 # Set the below flag to true if a different GSLB Service fqdn is desired than the ingress/route\u0026#39;s 45 # local fqdns. Note that, this field will use AKO\u0026#39;s HostRule objects\u0026#39; to find out the local to global 46 # fqdn mapping. To configure a mapping between the local to global fqdn, configure the hostrule 47 # object as: 48 # [...] 49 # spec: 50 # virtualhost: 51 # fqdn: foo.avi.com 52 # gslb: 53 # fqdn: gs-foo.avi.com 54 useCustomGlobalFqdn: true ####### set this to true if you want to define custom FQDN for GSLB - I use this 55 56gslbLeaderCredentials: 57 username: \u0026#39;admin\u0026#39; ##### username/password AVI Controller 58 password: \u0026#39;password\u0026#39; ##### username/password AVI Controller 59 60globalDeploymentPolicy: 61 # appSelector takes the form of: 62 appSelector: 63 label: 64 app: \u0026#39;gslb\u0026#39; #### I am using this selector for services to be used in GSLB 65 # Uncomment below and add the required ingress/route/service label 66 # appSelector: 67 68 # namespaceSelector takes the form of: 69 # namespaceSelector: 70 # label: 71 # ns: gslb \u0026lt;example label key-value for namespace\u0026gt; 72 # Uncomment below and add the reuqired namespace label 73 # namespaceSelector: 74 75 # list of all clusters that the GDP object will be applied to, can take any/all values 76 # from .configs.memberClusters 77 matchClusters: 78 - cluster: \u0026#39;k8slab-admin@k8slab\u0026#39; ####use the context name 79 - cluster: \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; ####use the context name 80 81 # list of all clusters and their traffic weights, if unspecified, default weights will be 82 # given (optional). Uncomment below to add the required trafficSplit. 83 # trafficSplit: 84 # - cluster: \u0026#34;cluster1-admin\u0026#34; 85 # weight: 8 86 # - cluster: \u0026#34;cluster2-admin\u0026#34; 87 # weight: 2 88 89 # Uncomment below to specify a ttl value in seconds. By default, the value is inherited from 90 # Avi\u0026#39;s DNS VS. 91 # ttl: 10 92 93 # Uncomment below to specify custom health monitor refs. By default, HTTP/HTTPS path based health 94 # monitors are applied on the GSs. 95 # healthMonitorRefs: 96 # - hmref1 97 # - hmref2 98 99 # Uncomment below to specify a Site Persistence profile ref. By default, Site Persistence is disabled. 100 # Also, note that, Site Persistence is only applicable on secure ingresses/routes and ignored 101 # for all other cases. Follow https://avinetworks.com/docs/20.1/gslb-site-cookie-persistence/ to create 102 # a Site persistence profile. 103 # sitePersistenceRef: gap-1 104 105 # Uncomment below to specify gslb service pool algorithm settings for all gslb services. Applicable 106 # values for lbAlgorithm: 107 # 1. GSLB_ALGORITHM_CONSISTENT_HASH (needs a hashMask field to be set too) 108 # 2. GSLB_ALGORITHM_GEO (needs geoFallback settings to be used for this field) 109 # 3. GSLB_ALGORITHM_ROUND_ROBIN (default) 110 # 4. GSLB_ALGORITHM_TOPOLOGY 111 # 112 # poolAlgorithmSettings: 113 # lbAlgorithm: 114 # hashMask: # required only for lbAlgorithm == GSLB_ALGORITHM_CONSISTENT_HASH 115 # geoFallback: # fallback settings required only for lbAlgorithm == GSLB_ALGORITHM_GEO 116 # lbAlgorithm: # can only have either GSLB_ALGORITHM_ROUND_ROBIN or GSLB_ALGORITHM_CONSISTENT_HASH 117 # hashMask: # required only for fallback lbAlgorithm as GSLB_ALGORITHM_CONSISTENT_HASH 118 119serviceAccount: 120 # Specifies whether a service account should be created 121 create: true 122 # Annotations to add to the service account 123 annotations: {} 124 # The name of the service account to use. 125 # If not set and create is true, a name is generated using the fullname template 126 name: 127 128resources: 129 limits: 130 cpu: 250m 131 memory: 300Mi 132 requests: 133 cpu: 100m 134 memory: 200Mi 135 136service: 137 type: ClusterIP 138 port: 80 139 140rbac: 141 # creates the pod security policy if set to true 142 pspEnable: false 143 144persistentVolumeClaim: \u0026#39;\u0026#39; 145mountPath: /log 146logFile: amko.log 147 148federatorLogFile: amko-federator.log When done, its time to install AMKO like this:\n1helm install ako/amko --generate-name --version 1.8.1 -f /path/to/values.yaml --set configs.gslbLeaderController=\u0026lt;leader_controller_ip\u0026gt; --namespace=avi-system ####There is a typo in the official docs - its pointing to amko only If everything went well you should se a couple of things in your k8s cluster under the namespace avi-system.\n1k get pods -n avi-system 2NAME READY STATUS RESTARTS AGE 3ako-0 1/1 Running 0 25h 4amko-0 2/2 Running 0 20h 5 6k get amkocluster amkocluster-federation -n avi-system 7NAME AGE 8amkocluster-federation 20h 9 10k get gc -n avi-system gc-1 11NAME AGE 12gc-1 20h 13 14k get gdp -n avi-system 15NAME AGE 16global-gdp 20h AMKO is up and running. Time create a GSLB service\nCreate GSLB service You probably already have a bunch of ingress services running, and to make them GSLB \u0026quot;aware\u0026quot; there is not much to be done to achieve that. If you noticed in our value.yaml for the AMKO Helm chart we defined this:\n1globalDeploymentPolicy: 2 # appSelector takes the form of: 3 appSelector: 4 label: 5 app: \u0026#39;gslb\u0026#39; #### I am using this selector for services to be used in GSLB So what we need to in our ingress service is to add the below, and then a new section where we define our gslb fqdn.\nHere is my sample ingress applied in my primary k8s cluster:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 labels: #### This is added for GSLB 6 app: gslb #### This is added for GSLB - Using the selector I chose in the value.yaml 7 namespace: fruit 8 9spec: 10 ingressClassName: avi-lb 11 rules: 12 - host: fruit-global.guzware.net #### Specific for this site (Home Lab) 13 http: 14 paths: 15 - path: /apple 16 pathType: Prefix 17 backend: 18 service: 19 name: apple-service 20 port: 21 number: 5678 22 - path: /banana 23 pathType: Prefix 24 backend: 25 service: 26 name: banana-service 27 port: 28 number: 5678 29--- #### New section to define a host rule 30apiVersion: ako.vmware.com/v1alpha1 31kind: HostRule 32metadata: 33 namespace: fruit 34 name: gslb-host-rule-fruit 35spec: 36 virtualhost: 37 fqdn: fruit-global.guzware.net #### Specific for this site (Home Lab) 38 enableVirtualHost: true 39 gslb: 40 fqdn: fruit.gslb.guzware.net ####This is common for both sites As soon as it is applied, and there are no errors in AMKO or AKO, it should be visible in your AVI controller GUI: If you click on the name it should take you to next page where it show the GSLB pool members and the status: Screenshot below is when both sites have applied their GSLB services: \u0026quot;\nNext we need to apply gslb settings on the secondary site also:\nThis is what I have deployed on the secondary site (note the difference in domain names specific for that site)\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 labels: #### This is added for GSLB 6 app: gslb #### This is added for GSLB - Using the selector I chose in the value.yaml 7 namespace: fruit 8 9spec: 10 ingressClassName: avi-lb 11 rules: 12 - host: fruit-site-2.lab.guzware.net #### Specific for this site (Remote Site) 13 http: 14 paths: 15 - path: /apple 16 pathType: Prefix 17 backend: 18 service: 19 name: apple-service 20 port: 21 number: 5678 22 - path: /banana 23 pathType: Prefix 24 backend: 25 service: 26 name: banana-service 27 port: 28 number: 5678 29--- #### New section to define a host rule 30apiVersion: ako.vmware.com/v1alpha1 31kind: HostRule 32metadata: 33 namespace: fruit 34 name: gslb-host-rule-fruit 35spec: 36 virtualhost: 37 fqdn: fruit-site-2.lab.guzware.net #### Specific for this site (Remote Site) 38 enableVirtualHost: true 39 gslb: 40 fqdn: fruit.gslb.guzware.net ##### Common for both sites When this is applied Avi will go ahead and put this into the same GSLB service as above, and the screenshot above will be true.\nNow I have the same application deployed in both sites, but equally available whether I am sitting in my home-lab or at the remote-site. There is a bunch of parameters that can be tuned, which I will not go into now (maybe getting back to this and update with further possibilities with GSLB). But one of them can be LoadBalancing algorithms such as Geo Location Source. Say I want the application to be accessed from clients as close to the application as possible. And should one of the sites become unavailable it will still be accessible from one of the sites that are still online. Very cool indeed. For the sake of the demo I am about to show the only thing I change in the default GSLB settings is the TTL, I am setting it to 2 seconds so I can showcase that the application is being load balanced between both sites. Default algorithm is Round-Robin so it should balance between them regardless of the latency difference (accessing the application from my home network in my home lab vs from my home network in the remote-site which has several ms in distance). Heres where I am setting these settings: With a TTL of 2 seconds it should switch faster so I can see the balancing between the two sites. Let me try to access the application from my browser using the gslb fqdn: fruit.gslb.guzware.net/apple\nA refresh of the page and now: To even illustrate more I will run a curl command against the gslb fqdn: Now a ping against the FQDN to show the ip of the corresponding site that answer on the call: Notice the change in ip address but also the latency in ms\nNow I can go ahead and disable one of the site to simulate failover, and the application is still available on the same FQDN. So many possibilities with GSLB.\nThats it then. NSX ALB, AKO with AMKO between two sites, same application available in two physical location, redundancy, scale-out, availability. Stay tuned for more updates in advanced settings - in the future 😄\n","link":"https://yikes.guzware.net/2022/10/23/gslb-with-ako-amko-nsx-advanced-loadbalancer/","section":"post","tags":["NSX Advanced LoadBalancer","AKO","AMKO"],"title":"GSLB With AKO \u0026 AMKO - NSX Advanced LoadBalancer"},{"body":"","link":"https://yikes.guzware.net/tags/nsx-advanced-loadbalancer/","section":"tags","tags":null,"title":"NSX Advanced LoadBalancer"},{"body":"","link":"https://yikes.guzware.net/tags/authentication/","section":"tags","tags":null,"title":"Authentication"},{"body":"","link":"https://yikes.guzware.net/tags/cert-manager/","section":"tags","tags":null,"title":"cert-manager"},{"body":"","link":"https://yikes.guzware.net/tags/rbac/","section":"tags","tags":null,"title":"RBAC"},{"body":"AKO settings: What happens if we need to to this\nWhat happens if I need passthrough\nHow does AKO work\n","link":"https://yikes.guzware.net/2022/10/23/we-take-a-look-at-the-ako-crds/","section":"post","tags":["cert-manager","Authentication","RBAC"],"title":"We Take a Look at the AKO Crds"},{"body":"TOPICS: ","link":"https://yikes.guzware.net/2022/10/23/running-the-unifi-controller-in-kubernetes/","section":"post","tags":["cert-manager","Authentication","RBAC"],"title":"Running the Unifi Controller in Kubernetes"},{"body":"vSphere with Tanzu (TKGs) With or without NSX-T Deploy workload clusters NSX-T Loadbalancer and AKO as Ingress ","link":"https://yikes.guzware.net/2022/10/23/how-to-deploy-tanzu-with-vsphere/","section":"post","tags":["cert-manager","Authentication","RBAC"],"title":"How to Deploy Tanzu With vSphere"},{"body":"","link":"https://yikes.guzware.net/tags/grafana/","section":"tags","tags":null,"title":"grafana"},{"body":"","link":"https://yikes.guzware.net/categories/logging/","section":"categories","tags":null,"title":"logging"},{"body":"","link":"https://yikes.guzware.net/tags/loki/","section":"tags","tags":null,"title":"loki"},{"body":"","link":"https://yikes.guzware.net/categories/monitoring/","section":"categories","tags":null,"title":"monitoring"},{"body":"Logging and metrics monitoring I wanted to visualize performance metrics in Grafana, and getting the logs from my Kubernetes clusters available centrally. So i chose to go with Grafana as my \u0026quot;dashboard\u0026quot; for visualizing, Prometheus for metrics and Loki for logs. I did fiddle some to get this up and running. But after I while I managed to get it sorted the way I wanted.\nSources used in this article: Bitnami, Grafana and Kube-Prometheus-Stack\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n","link":"https://yikes.guzware.net/2022/10/19/monitoring-with-prometheus-loki-promtail-and-grafana/","section":"post","tags":["loki","grafana","promtail","prometheus"],"title":"Monitoring With Prometheus, Loki, Promtail and Grafana"},{"body":"","link":"https://yikes.guzware.net/tags/prometheus/","section":"tags","tags":null,"title":"prometheus"},{"body":"","link":"https://yikes.guzware.net/tags/promtail/","section":"tags","tags":null,"title":"promtail"},{"body":"","link":"https://yikes.guzware.net/categories/docker/","section":"categories","tags":null,"title":"docker"},{"body":"","link":"https://yikes.guzware.net/categories/harbor/","section":"categories","tags":null,"title":"harbor"},{"body":"","link":"https://yikes.guzware.net/categories/helm/","section":"categories","tags":null,"title":"helm"},{"body":"","link":"https://yikes.guzware.net/tags/howto/","section":"tags","tags":null,"title":"howto"},{"body":"","link":"https://yikes.guzware.net/tags/informational/","section":"tags","tags":null,"title":"informational"},{"body":"This post will briefly go through how to deploy (using Helm), configure and use VMware Harbor registry in Kubernetes.\nQuick introduction to Harbor Harbor is an open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted. Harbor, a CNCF Graduated project, delivers compliance, performance, and interoperability to help you consistently and securely manage artifacts across cloud native compute platforms like Kubernetes and Docker. link\nI use myself Harbor in many of my own projects, including the images I make for my Hugo blogsite (this).\nDeploy Harbor with Helm Add helm chart:\n1helm repo add harbor https://helm.goharbor.io 2helm fetch harbor/harbor --untar Before you perform the default helm install of Harbor you want to grab the helm values for the Harbor charts so you can edit some settings to match your environment:\n1helm show values harbor/harbor \u0026gt; harbor.values.yaml The default values you get from the above command includes all available parameter which can be a bit daunting to go through. In the values file I use I have only picked the parameters I needed to set, here:\n1expose: 2 type: ingress 3 tls: 4 enabled: true 5 certSource: secret 6 secret: 7 secretName: \u0026#34;harbor-tls-prod\u0026#34; # certificates you have created with Cert-Manager 8 notarySecretName: \u0026#34;notary-tls-prod\u0026#34; # certificates you have created with Cert-Manager 9 ingress: 10 hosts: 11 core: registry.example.com 12 notary: notary.example.com 13 annotations: 14 kubernetes.io/ingress.class: \u0026#34;avi-lb\u0026#34; 15 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 16externalURL: https://registry.example.com 17harborAdminPassword: \u0026#34;PASSWORD\u0026#34; 18persistence: 19 enabled: true 20 # Setting it to \u0026#34;keep\u0026#34; to avoid removing PVCs during a helm delete 21 # operation. Leaving it empty will delete PVCs after the chart deleted 22 # (this does not apply for PVCs that are created for internal database 23 # and redis components, i.e. they are never deleted automatically) 24 resourcePolicy: \u0026#34;keep\u0026#34; 25 persistentVolumeClaim: 26 registry: 27 # Use the existing PVC which must be created manually before bound, 28 # and specify the \u0026#34;subPath\u0026#34; if the PVC is shared with other components 29 existingClaim: \u0026#34;\u0026#34; 30 # Specify the \u0026#34;storageClass\u0026#34; used to provision the volume. Or the default 31 # StorageClass will be used (the default). 32 # Set it to \u0026#34;-\u0026#34; to disable dynamic provisioning 33 storageClass: \u0026#34;nfs-client\u0026#34; 34 subPath: \u0026#34;\u0026#34; 35 accessMode: ReadWriteOnce 36 size: 50Gi 37 annotations: {} 38 database: 39 existingClaim: \u0026#34;\u0026#34; 40 storageClass: \u0026#34;nfs-client\u0026#34; 41 subPath: \u0026#34;postgres-storage\u0026#34; 42 accessMode: ReadWriteOnce 43 size: 1Gi 44 annotations: {} 45 46portal: 47 tls: 48 existingSecret: harbor-tls-prod When you have edited the values file its time to install:\n1helm install -f harbor.values.yaml harbor-deployment harbor/harbor -n harbor Explanation: \u0026quot;-f\u0026quot; is telling helm to read the values from the specified file after, then the name of your helm installation (here harbor-deployment) then the helm repo and finally the namespace you want it deployed in. A couple of seconds later you should be able to log in to the GUI of Harbor through your webbrowser if everything has been set up right, Ingress, pvc, secrets.\nCertificate You can either use Cert-manager as explained here or bring your own ca signed certificates.\nHarbor GUI To log in to the GUI for the first time open your browser and point it to the externalURL you gave it in your values file and the corresponding harborAdminPassword you defined. From there on you create users and projects and start exploring Harbor.\nUsers: Projects: Docker images To push your images to Harbor execute the following commands:\n1docker login registry.example.com #log in with the user/password you have created in the GUI 2docker tag image-name:tag registry.example.com/project/image-name:tag 3docker push registry.example.com/project/image-name:tag ","link":"https://yikes.guzware.net/2022/10/13/vmware-harbor-registry/","section":"post","tags":["informational","howto"],"title":"VMware Harbor Registry"},{"body":"","link":"https://yikes.guzware.net/categories/blog/","section":"categories","tags":null,"title":"Blog"},{"body":"","link":"https://yikes.guzware.net/categories/hugo/","section":"categories","tags":null,"title":"hugo"},{"body":"This blog post will cover how I wanted to deploy Hugo to host my blog-page.\nPreparations To achieve what I wanted, deploy an highly available Hugo hosted blog page, I decided to run Hugo in Kubernetes. For that I needed\nKubernetes cluster, obviously, consisting of several workers for the the \u0026quot;hugo\u0026quot; pods to run on (already covered here. Persistent storage (NFS in my case, already covered here) An Ingress controller (already covered here) A docker image with Hugo, nginx and go (will be covered here) Docker installed so you can build the image A place to host the docker image (Docker hub or Harbor registry will be covered here) Create the Docker image Before I can deploy Hugo I need to create an Docker image that contains the necessary bits. I have already created the Dockerfile here:\n1#Install the container\u0026#39;s OS. 2FROM ubuntu:latest as HUGOINSTALL 3 4# Install Hugo. 5RUN apt-get update -y 6RUN apt-get install wget git ca-certificates golang -y 7RUN wget https://github.com/gohugoio/hugo/releases/download/v0.104.3/hugo_extended_0.104.3_Linux-64bit.tar.gz \u0026amp;\u0026amp; \\ 8 tar -xvzf hugo_extended_0.104.3_Linux-64bit.tar.gz \u0026amp;\u0026amp; \\ 9 chmod +x hugo \u0026amp;\u0026amp; \\ 10 mv hugo /usr/local/bin/hugo \u0026amp;\u0026amp; \\ 11 rm -rf hugo_extended_0.104.3_Linux-64bit.tar.gz 12# Copy the contents of the current working directory to the hugo-site 13# directory. The directory will be created if it doesn\u0026#39;t exist. 14COPY . /hugo-site 15 16# Use Hugo to build the static site files. 17RUN hugo -v --source=/hugo-site --destination=/hugo-site/public 18 19# Install NGINX and deactivate NGINX\u0026#39;s default index.html file. 20# Move the static site files to NGINX\u0026#39;s html directory. 21# This directory is where the static site files will be served from by NGINX. 22FROM nginx:stable-alpine 23RUN mv /usr/share/nginx/html/index.html /usr/share/nginx/html/old-index.html 24COPY --from=HUGOINSTALL /hugo-site/public/ /usr/share/nginx/html/ 25 26# The container will listen on port 80 using the TCP protocol. 27EXPOSE 80 Credits for the Dockerfile as it was initially taken from here. I have updated it, and done some modifications to it.\nBefore building the image with docker, install docker by following this guide.\nBuild the docker image I need to place myself in the same directory as my Dockerfile and execute the following command (Replace \u0026quot;name-you-want-to-give-the-image:\u0026lt;tag\u0026gt;\u0026quot; with something like \u0026quot;hugo-image:v1\u0026quot;):\n1docker build -t name-you-want-to-give-the-image:\u0026lt;tag\u0026gt; . #Note the \u0026#34;.\u0026#34; important Now the image will be built and hosted locally on my \u0026quot;build machine\u0026quot;.\nIf anything goes well it should be listed here:\n1$ docker images 2REPOSITORY TAG IMAGE ID CREATED SIZE 3hugo-image v1 d43ee98c766a 10 secs ago 70MB 4nginx stable-alpine 5685937b6bc1 7 days ago 23.5MB 5ubuntu latest 216c552ea5ba 9 days ago 77.8MB Place the image somewhere easily accessible Now that I have my image I need to make sure it is easily accessible for my Kubernetes workers so they can download the image and deploy it. For that I can use the local docker registry pr control node and worker node. Meaning I need to load the image into all workers and control plane nodes. Not so smooth way to to do it. This is the approach for such a method:\n1docker save -o \u0026lt;path for generated tar file\u0026gt; \u0026lt;image name\u0026gt; #needs to be done on the machine you built the image. Example: docker save -o /home/username/hugo-image.v1.tar hugo-image:v1 This will \u0026quot;download\u0026quot; the image from the local docker repository and create tar file. This tar file needs to be copied to all my workers and additional control plane nodes with scp or other methods I find suitable. When that is done I need to upload the tar to each of their local docker repository with the following command:\n1docker -i load /home/username/hugo-image.v1.tar It is ok to know about this process if you are in non-internet environments etc, but even in non-internet environment we can do this with a private registry. And thats where Harbor can come to the rescue link.\nWith Harbor I can have all my images hosted centrally but dont need access to the internet as it is hosted in my own environment.\nI could also use Docker hub. Create an account there, and use it as my repository. I prefer the Harbor registry, as it provides many features. The continuation of this post will use Harbor, the procedure to upload/download images is the same process as with Docker hub but you log in to your own Harbor registry instead of Docker hub.\nUploading my newly created image is done like this:\n1docker login registry.example.com #FQDN to my selfhosted Harbor registry, and the credentials for an account I have created there. 2docker tag hugo-image:v1 https://registry.example.com/hugo/hugo-image:v1 #\u0026#34;/hugo/\u0026#34; name of project in Harbor 3docker push registry.example.com/hugo/hugo-image:v1 #upload it Thats it. Now I can go ahead and create my deployment.yaml definition file in my Kubernetes cluster, point it to my image hosted at my local Harbor registry (e.g registry.example.com/hugo/hugo-image:v1). But let me go through how I created my Hugo deployment in Kubernetes, as I am so close to see my newly image in action 😄 (Will it even work).\nDeploy Hugo in Kubernetes To run my Hugo image in Kubernetes the way I wanted I need to define a Deployment (remember I wanted a highly available Hugo deployment, meaning more than one pod and the ability to scale up/down). The first section of my hugo-deployment.yaml definition file looks like this:\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: hugo-site 5 namespace: hugo-site 6spec: 7 replicas: 3 8 selector: 9 matchLabels: 10 app: hugo-site 11 tier: web 12 template: 13 metadata: 14 labels: 15 app: hugo-site 16 tier: web 17 spec: 18 containers: 19 - image: registry.example.com/hugo/hugo-image:v1 20 name: hugo-site 21 imagePullPolicy: Always 22 ports: 23 - containerPort: 80 24 name: hugo-site 25 volumeMounts: 26 - name: persistent-storage 27 mountPath: /usr/share/nginx/html/ 28 volumes: 29 - name: persistent-storage 30 persistentVolumeClaim: 31 claimName: hugo-pv-claim In the above I define name of deployment, specify number of pods with the replica specification, labels, point to my image hosted in Harbor and then what the container mountPath and the peristent volume claim. mountPath is inside the container, and the files/folders mounted is read from the content it sees in the persistent volume claim \u0026quot;hugo-pv-claim\u0026quot;. Thats where Hugo will find the content of the Public folder (after the content has been generated).\nI also needed to define a Service so I can reach/expose the containers contents (webpage) on port 80. This is done with this specification:\n1apiVersion: v1 2kind: Service 3metadata: 4 name: hugo-service 5 namespace: hugo-site 6 labels: 7 svc: hugo-service 8spec: 9 selector: 10 app: hugo-site 11 tier: web 12 ports: 13 - port: 80 Can be saved as a separate \u0026quot;service.yaml\u0026quot; file or pasted into one yaml file. But instead of pointing to my workers IP addresses to read the content each time I wanted to expose it with an Ingress by using AKO and Avi LoadBalancer. This is how I done that:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: hugo-ingress 5 namespace: hugo-site 6 labels: 7 app: hugo-ingress 8 annotations: 9 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 10spec: 11 ingressClassName: avi-lb 12 rules: 13 - host: yikes.guzware.net 14 http: 15 paths: 16 - pathType: Prefix 17 path: / 18 backend: 19 service: 20 name: hugo-service 21 port: 22 number: 80 I define my ingressClassName, the hostname for my Ingress controller to listen for requests on and the Service the Ingress should route all the request to yikes.guzware.net to, which is my hugo-service defined earlier. Could also be saved as a separe yaml file. I have chosen to put all three \u0026quot;kinds\u0026quot; in one yaml file. Which then looks like this:\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: hugo-site 5 namespace: hugo-site 6spec: 7 replicas: 3 8 selector: 9 matchLabels: 10 app: hugo-site 11 tier: web 12 template: 13 metadata: 14 labels: 15 app: hugo-site 16 tier: web 17 spec: 18 containers: 19 - image: registry.example.com/hugo/hugo-image:v1 20 name: hugo-site 21 imagePullPolicy: Always 22 ports: 23 - containerPort: 80 24 name: hugo-site 25 volumeMounts: 26 - name: persistent-storage 27 mountPath: /usr/share/nginx/html/ 28 volumes: 29 - name: persistent-storage 30 persistentVolumeClaim: 31 claimName: hugo-pv-claim 32--- 33apiVersion: v1 34kind: Service 35metadata: 36 name: hugo-service 37 namespace: hugo-site 38 labels: 39 svc: hugo-service 40spec: 41 selector: 42 app: hugo-site 43 tier: web 44 ports: 45 - port: 80 46--- 47apiVersion: networking.k8s.io/v1 48kind: Ingress 49metadata: 50 name: hugo-ingress 51 namespace: hugo-site 52 labels: 53 app: hugo-ingress 54 annotations: 55 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 56spec: 57 ingressClassName: avi-lb 58 rules: 59 - host: yikes.guzware.net 60 http: 61 paths: 62 - pathType: Prefix 63 path: / 64 backend: 65 service: 66 name: hugo-service 67 port: 68 number: 80 Now before my Deployment is ready to be applied I need to create the namespace I have defined in the yaml file above: kubectl create ns hugo-site.\nNow when that is done its time to apply my hugo deployment. kubectl apply -f hugo-deployment.yaml\nI want to check the state of the pods:\n1$ kubectl get pod -n hugo-site 2NAME READY STATUS RESTARTS AGE 3hugo-site-7f95b4644c-5gtld 1/1 Running 0 10s 4hugo-site-7f95b4644c-fnrh5 1/1 Running 0 10s 5hugo-site-7f95b4644c-hc4gw 1/1 Running 0 10s Ok, so far so good. What about my deployment:\n1$ kubectl get deployments.apps -n hugo-site 2NAME READY UP-TO-DATE AVAILABLE AGE 3hugo-site 3/3 3 3 35s Great news. Lets check the Service, Ingress and persistent volume claim.\nService:\n1$ kubectl get service -n hugo-site 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3hugo-service ClusterIP 10.99.25.113 \u0026lt;none\u0026gt; 80/TCP 46s Ingress:\n1$ kubectl get ingress -n hugo-site 2NAME CLASS HOSTS ADDRESS PORTS AGE 3hugo-ingress avi-lb yikes.guzware.net x.x.x.x 80 54s PVC:\n1NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE 2hugo-pv-claim Bound pvc-b2395264-4500-4d74-8a5c-8d79f9df8d63 10Gi RWO nfs-client 59s Well that looks promising. Will I be able to access my hugo page on yikes.guzware.net ... well yes, otherwise you wouldnt read this.. 🤣\nCreating and updating content A blog page without content is not so interesting. So just some quick comments on how I create content, and update them.\nI use Typora creating and editing my *.md files. While working with the post (such as now) I run hugo in \u0026quot;server-mode\u0026quot; whith this command: hugo server. If I run this command on of my linux virtual machines through SSH I want to reach the server from my laptop so I add the parameter --bind=ip-of-linux-vm and I can access the page from my laptop on the ip of the linux VM and port 1313. When I am done with the article/post for the day I generated the web-page with the command hugo -D -v. The updated content of my public folder after I have generated the page is mirrored to the NFS path that is used in my PVC shown above and my containers picks up the updated content instantly. Thats how I do, it works and I find it easy to maintain and operate. And, if one of my workers fails, I have more pods still available on the remaining workers. If a pod fails Kubernetes will just take care of that for me as I have declared a set of pods(replicas) that should run. If I run my Kubernetes environment in Tanzu and one of my workers fails, that will also be automatically taken care of.\n","link":"https://yikes.guzware.net/2022/10/12/hugo-in-kubernetes/","section":"post","tags":["howto","just for fun"],"title":"Hugo in Kubernetes"},{"body":"","link":"https://yikes.guzware.net/tags/just-for-fun/","section":"tags","tags":null,"title":"just for fun"},{"body":"","link":"https://yikes.guzware.net/categories/pinniped/","section":"categories","tags":null,"title":"Pinniped"},{"body":"How to use Pinniped as the authentication service in Kubernets with OpenLDAP\nGoal: Deploy an authentication service to handle RBAC in Kubernetes Purpose: User/access management in Kubernetes\nPinniped introduction ","link":"https://yikes.guzware.net/2022/10/11/pinniped-authentication-service/","section":"post","tags":["howto"],"title":"Pinniped Authentication Service"},{"body":"This article will quickly go through how to create wildcard certificates and automatically renew them with Lets Encrypt and Cert-Manager\nCert-Manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.\nIt can issue certificates from a variety of supported sources, including Let's Encrypt, HashiCorp Vault, and Venafi as well as private PKI.\nIt will ensure certificates are valid and up to date, and attempt to renew certificates at a configured time before expiry. link\nInstall Cert-Manager I prefer the Helm way so lets add the cert-manager helm chart:\n1helm repo add jetstack https://charts.jetstack.io 2helm repo update Then we need to deploy cert-manager. This can be done out-of-the-box with the commands given from the official docs (this also installed the necessary CRDs):\n1helm install \\ 2 cert-manager jetstack/cert-manager \\ 3 --namespace cert-manager \\ 4 --create-namespace \\ 5 --version v1.9.1 \\ 6 --set installCRDs=true Or if you need to customize some settings, as I needed to do, I used this command:\n1helm install -f /path/to/cert-manager.values.yaml cert-manager jetstack/cert-manager --namespace cert-manager --version v1.9.1 --set installCRDs=true --set \u0026#39;extraArgs={--dns01-recursive-nameservers-only,--dns01-recursive-nameservers=xx.xx.xx.xx:53\\,xx.xx.xx:53}\u0026#39; The above command takes care of the cert-manager installation including the necessary CRDs, but it will also adjust the DNS servers Cert-Manager will use to verify the ownership of my domain.\nDNS01 - Wildcard certificate In this post I will go with wildcard certificate creation. I find it easier to use instead of having a separate cert for everthing I do, as long as they are within the same subdomain. So if I have my services in *.example.com they can use the same certificate. But if I have services in *.int.example.com I can not use the same certificate as LetsEncrypt certificates dont support that. Then you need to create a separate wildcard cert for each subdomain. But Cert-manager will handle that for you very easy.\nThe offiicial Cert-Manager supported DNS01 providers are:\nACMEDNS Akamai AzureDNS CloudFlare Google Route53 DigitalOcean RFC2136 There is also an option to use Webhooks. I did try that as my previous DNS registrar were not on the DNS01 supported list. I did not succeed with using the webhook approach. It could be an issue with the specific webhooks I used or even with my registrar so I decided to migrate over to CloudFlare which is on the supported list, \u0026quot;out of the box\u0026quot;.\nIssuer - CloudFlare and LetsEncrypt The first we need to do is to create a secret for Cert-Manager to use when \u0026quot;interacting\u0026quot; with CloudFlare. I went with API Token. So head over to your CloudFlare control panel and create a token for Cert Manager like this: Here is the permissions: Now use the tokens to create your secret:\n1apiVersion: v1 2kind: Secret 3metadata: 4 name: cloudflare-api-token-secret 5 namespace: cert-manager 6type: Opaque 7stringData: 8 api-token: Apply it kubect apply -f name.of.yaml\nNow create your issuer. LetsEncrypt have two repos, one called staging and one production. Start out with staging until everything works so you dont hit the LetsEncrypt limit. In regards to this I created two issuers, one for staging and one for production. When everything was working and I have verified the certificates etc I deployed the certs using the prod-issuer.\nIssuer-staging:\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: letsencrypt-staging 5spec: 6 acme: 7 # ACME Server 8 # prod : https://acme-v02.api.letsencrypt.org/directory 9 # staging : https://acme-staging-v02.api.letsencrypt.org/directory 10 server: https://acme-staging-v02.api.letsencrypt.org/directory 11 # ACME Email address 12 email: xxx.xxx@xxx.xxx 13 privateKeySecretRef: 14 name: letsencrypt-key-staging # staging or production 15 solvers: 16 - dns01: 17 cloudflare: 18 apiTokenSecretRef: 19 name: cloudflare-api-token-secret ## created and applied above 20 key: api-token Issuer-production:\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: letsencrypt-prod 5 namespace: cert-manager 6spec: 7 acme: 8 # ACME Server 9 # prod : https://acme-v02.api.letsencrypt.org/directory 10 # staging : https://acme-staging-v02.api.letsencrypt.org/directory 11 server: https://acme-v02.api.letsencrypt.org/directory 12 # ACME Email address 13 email: xxx.xxx@xxx.xxx 14 privateKeySecretRef: 15 name: letsencrypt-key-prod # staging or production 16 solvers: 17 - dns01: 18 cloudflare: 19 apiTokenSecretRef: 20 name: cloudflare-api-token-secret # created and applied above 21 key: api-token Request certificate Now that the groundwork for cert-manager has been setup, its time to \u0026quot;print\u0026quot; some certificates. Prepare your yamls for both the staging key and production key.\nWildcard-staging:\n1apiVersion: cert-manager.io/v1 2kind: Certificate 3metadata: 4 name: name-tls-test 5 namespace: namespace-you-want-the-cert-in 6spec: 7 secretName: name-tls-staging 8 issuerRef: 9 name: letsencrypt-staging 10 kind: ClusterIssuer 11 duration: 2160h # 90d 12 renewBefore: 720h # 30d before SSL will expire, renew it 13 dnsNames: 14 - \u0026#34;*.example.com\u0026#34; Wildcard-production:\n1apiVersion: cert-manager.io/v1 2kind: Certificate 3metadata: 4 name: name-tls-production 5 namespace: namespace-you-want-the-cert-in 6spec: 7 secretName: name-tls-prod 8 issuerRef: 9 name: letsencrypt-prod 10 kind: ClusterIssuer 11 duration: 2160h # 90d 12 renewBefore: 720h # 30d before SSL will expire, renew it 13 dnsNames: 14 - \u0026#34;*.example.com\u0026#34; Apply the staging request first. Check your certificate status with this command:\n1$ kubectl get certificate -n namespace-you-wrote 2NAME READY SECRET AGE 3name-tls-staging True name-tls-staging 8d Please note that it can take a couple of minutes before the certificate is ready. This applies for production also.\nIf everything went well, delete your staging certificate and apply your production certificate with the production yaml. Thats it. Now Cert-Manager will take care of updating your certificate for your, sit back and enjoy your applications with your always up to date certificates.\nTroubleshooting tips, commands If something should fail there is a couple of commands you can use to figure out whats going on.\n1$ kubectl get issuer 2$ kubectl get clusterissuer 3$ kubectl describe issuer 4$ kubectl describe clusterissuer 5$ kubectl describe certificaterequest 6$ kubectl describe order 7$ kubectl get challenges 8$ kubectl describe challenges For more detailed explanation go here\n","link":"https://yikes.guzware.net/2022/10/11/cert-manager-and-letsencrypt/","section":"post","tags":["howto"],"title":"Cert Manager and Letsencrypt"},{"body":"","link":"https://yikes.guzware.net/categories/antrea/","section":"categories","tags":null,"title":"antrea"},{"body":"","link":"https://yikes.guzware.net/tags/cni/","section":"tags","tags":null,"title":"cni"},{"body":"","link":"https://yikes.guzware.net/tags/network-policies/","section":"tags","tags":null,"title":"network policies"},{"body":"","link":"https://yikes.guzware.net/categories/nsx/","section":"categories","tags":null,"title":"nsx"},{"body":"What is the NSX Antrea integration Assumptions: I assume that the reader is aware of what Antrea is, and what a CNI is and also what NSX is. If not head over here to read more on Antrea and here to read more on NSX.\nFor many years VMware NSX has help many customer secure their workload by using the NSX Distributed Firewall. As NSX has evolved over the years the different platform it supports has also broadened, from virtual machines, bare metal server, cloud workload and kubernetes pods. NSX has had support for security policies in Kubernetes for a long time also with the CNI NCP Read about NCP here recently (almost a year ago since I wrote this article, so not so recent in the world of IT) it also got support for using the Antrea CNI. What does that mean then. Well, it mean we can now \u0026quot;add\u0026quot; our Antrea enabled clusters to NSX. With that Antrea will report their inventory, such as nodes, pods, services, ip addresses, k8s labels into the NSX manager. Another important feature is that we can also create and publish security policies from the NSX manager to the Antrea enabled clusters. Antrea is supported in almost all kinds of Kubernetes platforms, our own Tanzu solutions, upstream k8s, ARM, public cloud etc so it is very flexible. And with the rich information NSX gets from Antrea we can create more clever security policies by using the native kubernetes labels to form security group membership based on these labels. Will get into more of that later. Now lets move on to the components involved.\nAntrea NSX Adapter To understand a bit more how this works, we need to go through a couple of components that is in involved to get this integration in place.\nsdfsdfsdf\nsdf\nsdf\nsdf\nsdf\n","link":"https://yikes.guzware.net/2022/10/11/nsx-antrea-integration/","section":"post","tags":["security","network policies","cni"],"title":"NSX Antrea Integration"},{"body":"","link":"https://yikes.guzware.net/tags/security/","section":"tags","tags":null,"title":"security"},{"body":"This week I was fortunate to get hold of a VMC on AWS environment and wanted to test out the possibility of managing my K8s security policies from my on-prem NSX manager by utilizing the integration of Antrea in NSX. I haven't covered that specific integration part in a blog yet, but in short: by using Antrea as your CNI and you are running NSX-T 3.2 you can manage all your K8s policies from the NSX manager GUI. Thats a big thing. Manage your k8s policies from the same place where you manage all your other critical security policies. Your K8s clusters does not have to be in the same datacenter as your NSX manager. You can utilize VMC on AWS as your scale-out, prod/test/dev platform and still manage your K8s security policies centrally from the same NSX manager.\nIn this post I will go through how this is done and how it works.\nVMC on AWS VMC on AWS comes with NSX, but it is not yet on the version that has the NSX-T integration. So what I wanted to do was to use the VMC NSX manager to cover all the vm-level microsegmentation and let my on-prem NSX manager handle the Antrea security policies. To illustrate want I want to achieve:\nVMC on AWS to on-prem connectivity VMC on AWS supports a variety of connectivity options to your on-prem environment. I have gone with IPSec VPN. Where I configure IPsec on the VMC NSX manager to negotiate with my on-prem firewall to terminate the VPN connection. In VMC I have two networks: Management and Workload. I configured both subnets in my IPsec config as I wanted the flexibility to reach both subnets from my on-prem environment. To get the integration working I had to make sure that the subnet on my on-prem NSX manager resided on also was configured. So the IPsec configurations were done accordingly to support that: Two subnets from VMC and one from on-prem (where my NSX managers resides).\nIPsec config from my VMC NSX manager\nIPsec config from my on-prem firewall\nWhen IPsec tunnel was up I logged on to the VMC NSX manager and configured the \u0026quot;North/South\u0026quot; security policies allowing my Workload segment to any. I created a NSX Security Group (\u0026quot;VMs\u0026quot;) with membership criteria in place to grab my Workload Segment VMs (workload). This was just to make it convenient for myself during the test. We can of course (and should) be more granular in making these policies. But we also have the NSX Distributed Firewall which I will come to later.\nNorth/South policies\nNow I had the necessary connectivity and security policies in place for me to log on to the VMC vCenter from my on-prem management jumpbox and deploy my k8s worker nodes.\nVMC on AWS K8s worker nodes In VMC vCenter I deployed three Ubuntu worker nodes, and configured them to be one master worker and two worker nodes by following my previous blog post covering these steps:\nhttp://yikes.guzware.net/2020/10/08/ako-with-antrea-on-native-k8s-cluster/#Deploy_Kubernetes_on_Ubuntu_2004\nThree freshly deployed VMs in VMC to form my k8s cluster\n1NAME STATUS ROLES AGE VERSION 2vmc-k8s-master-01 Ready control-plane,master 2d22h v1.21.8 3vmc-k8s-worker-01 Ready \u0026lt;none\u0026gt; 2d22h v1.21.8 4vmc-k8s-worker-02 Ready \u0026lt;none\u0026gt; 2d22h v1.21.8 After the cluster was up I needed to install Antrea as my CNI.\nDownload the Antrea release from here: https://customerconnect.vmware.com/downloads/info/slug/networking_security/vmware_antrea/1_0\nAfter it has been downloaded, unpack it and upload the image to your k8s master and worker nodes by issuing the docker load -i antrea-advanced-debian-v1.2.3_vmware.3.tar.gz\nThen apply it by using the manifest antrea-advanced-v1.2.3+vmware.3.yml found under the /antrea-advanced-1.2.3+vmware.3.19009828/manifests folder. Like this: kubectl apply -f antrea-advanced-v1.2.3+vmware.3.yml and Antrea should spin right up and you have a fully working K8s cluster:\n1NAME READY STATUS RESTARTS AGE 2antrea-agent-2pdcr 2/2 Running 0 2d22h 3antrea-agent-6glpz 2/2 Running 0 2d22h 4antrea-agent-8zzc4 2/2 Running 0 2d22h 5antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d20h 6coredns-558bd4d5db-2j7jf 1/1 Running 0 2d22h 7coredns-558bd4d5db-kd2db 1/1 Running 0 2d22h 8etcd-vmc-k8s-master-01 1/1 Running 0 2d22h 9kube-apiserver-vmc-k8s-master-01 1/1 Running 0 2d22h 10kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 2d22h 11kube-proxy-rvzs6 1/1 Running 0 2d22h 12kube-proxy-tnkxv 1/1 Running 0 2d22h 13kube-proxy-xv77f 1/1 Running 0 2d22h 14kube-scheduler-vmc-k8s-master-01 1/1 Running 0 2d22h Antrea NSX integration Next up is to configure the Antrea NSX integration. This is done by following this guide:\nhttps://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-DFD8033B-22E2-4D7A-BD58-F68814ECDEB1.html\nIts very well described and easy to follow. So instead of me rewriting it here I just point to it. But in general it makes up of a couple of steps needed.\n1. Download the necessary Antrea Interworking parts, which is included in the Antrea-Advanced zip above\n2. Create a certificate to use for the Principal ID User in your on-prem NSX manager.\n3. Import image to your master and workers (interworking-debian-0.2.0.tar)\n4. Edit the bootstrap-config.yaml (https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-1AC65601-8B35-442D-8613-D3C49F37D1CC.html)\n5. Apply the bootstrap-config-yaml and you should end up with this result in your k8s cluster and in your on-prem NSX manager:\n1vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 17 2d22h My VMC k8s cluster: vmc-ant-cluster (in addition to my other on-prem k8s-cluster)\nInventory view from my on-prem NSX manager\nOne can immediately see useful information in the on-prem NSX manager about the \u0026quot;remote\u0026quot; VMC K8s cluster such as Nodes, Pods and Services in the Inventory view. If I click on the respective numbers I can dive into more useful information. By clicking on \u0026quot;Pods\u0026quot;\nPod state, ips, nodes they are residing on etc\nEven in this view I can click on the labels links to get the lables NSX gets from kubernetes and Antrea:\nBy clicking on Services I get all the services running in my VMC k8s cluster\nAnd the service labels:\nAll this information is very useful, as they can be used to create the security groups in NSX and use those groups in security policies.\nClicking on the Nodes I also get very useful information:\nVMC on AWS NSX distributed firewall As I mentioned earlier VMC on AWS also comes with NSX and one should utilize this to segment/create security polices on your worker nodes there. I have just created some simple rules allowing the \u0026quot;basic\u0026quot; needs for my workers, and then created some specific rules for what they are allowed to where all unspecified traffic is blocked by a default block rule.\nBare in mind that this is a demo environment and not representing any production environment as such, but some rules are in place to showcase that I am utilizing the NSX distributed firewall in VMC to microsegment my workload there.\nThe \u0026quot;basic\u0026quot; needs rules are the following: Under \u0026quot;Infrastructure\u0026quot; I am allowing my master and worker nodes to \u0026quot;consume\u0026quot; NTP and DNS. In this environment I do not have any local DNS and NTP servers as they are all public. DNS I am using Google's public DNS servers 8.8.8.8 and 8.8.4.4 and NTP the workers are using \u0026quot;time.ubuntu.com\u0026quot;. I have created a security group consisting of the known DNS servers ip, as I know what they are. But the NTP server's IP I do not know so I have created a security group with members only consisting of RFC1918 subnets and created a negated policy indicating that they are only allowed to reach NTP servers if they not reside on any RFC1918 subnet.\nNTP and DNS allow rules under Infrastructure\nUnder \u0026quot;Environment\u0026quot; I have created a Jump to Application policy that matches my K8s master/worker nodes\nJump to Application\nUnder \u0026quot;Application\u0026quot; I have a rule that is allowing internet access (could be done on the North/South Gateway Firewall section also) by indication that HTTP/HTTPS is allowed as long as it is not any RFC1918 subnet.\nAllow \u0026quot;internet access\u0026quot;\nFurther under Application I am specifying a bit more granular rules for what the k8s cluster is allowed in/out. Again, this is just some simple rules restricting the k8s cluster to not allow any-any by utilizing the NSX DFW already in VMC on AWS. One can and should be more granular, but its to give you an idea.\nIn the K8s-Backbone security policy section below I am allowing HTTP in to the k8s cluster as I am planning to run an k8s application there that uses HTTP where I allow a specific IP subnet/range as source and my loadbalancer IP range as the destination.\nThen I allow SSH to the master/workers for management purposes. Then I am creating some specific rules allowing the necessary ports needed for the Antrea control-plane to communicate with my on-prem NSX manager, which are: TCP 443, 1234 and 1235. Then I create an \u0026quot;Intra\u0026quot; rule allowing the master/workers to talk freely between each other. This should and can also be much more tightened down. When those rules are done processed they will hit a default block rule.\nVMC k8s cluster policy\nDefault drop\nAntrea policies from on-prem NSX manager Now when the \u0026quot;backbone\u0026quot; is ready configured and deployed its time to spin up some applications in my \u0026quot;VMC K8s cluster\u0026quot; and apply some Antrea security policies. Its now time for the magic to begin ;-)\nIn my on-prem environment I already have a couple of Antrea enabled K8s clusters running. On them a couple of demo applications are already running and protected by Antrea security policies created from my on-prem NSX manager. I like to use an application called Yelb (which I have used in previous blog posts here). This application consist of 4 pods. All pods doing their separate thing for the application to work. I have a frontend pod which is hosting the web-page for the application, I have an application pod, db pod and a cache pod. The necessary connectivity between looks like this:\nYelb pods connectivity\nTo make security policy creation easy I make use of all the information I get from Antrea and Kubernetes in form of \u0026quot;Labels\u0026quot;. These labels are translated into tags in NSX. Which makes it very easy to use, and for \u0026quot;non\u0026quot; developers to use as \u0026quot;human-readable\u0026quot; elements instead of IP adresses, pods unique names etc. In this example I want to microsegment the pods that makes up the application \u0026quot;Yelb\u0026quot;.\nCreating NSX Security Groups for Antrea Security Policy Before I create the actual Antrea Security Policies I will create a couple of security groups based on the tags I use in K8s for the application Yelb. The Yelb manifest looks like this:\n1apiVersion: v1 2kind: Service 3metadata: 4 name: redis-server 5 labels: 6 app: redis-server 7 tier: cache 8 namespace: yelb 9spec: 10 type: ClusterIP 11 ports: 12 - port: 6379 13 selector: 14 app: redis-server 15 tier: cache 16--- 17apiVersion: v1 18kind: Service 19metadata: 20 name: yelb-db 21 labels: 22 app: yelb-db 23 tier: backenddb 24 namespace: yelb 25spec: 26 type: ClusterIP 27 ports: 28 - port: 5432 29 selector: 30 app: yelb-db 31 tier: backenddb 32--- 33apiVersion: v1 34kind: Service 35metadata: 36 name: yelb-appserver 37 labels: 38 app: yelb-appserver 39 tier: middletier 40 namespace: yelb 41spec: 42 type: ClusterIP 43 ports: 44 - port: 4567 45 selector: 46 app: yelb-appserver 47 tier: middletier 48--- 49apiVersion: v1 50kind: Service 51metadata: 52 name: yelb-ui 53 labels: 54 app: yelb-ui 55 tier: frontend 56 namespace: yelb 57spec: 58 type: LoadBalancer 59 ports: 60 - port: 80 61 protocol: TCP 62 targetPort: 80 63 selector: 64 app: yelb-ui 65 tier: frontend 66--- 67apiVersion: v1 68kind: ReplicationController 69metadata: 70 name: yelb-ui 71 namespace: yelb 72spec: 73 replicas: 1 74 template: 75 metadata: 76 labels: 77 app: yelb-ui 78 tier: frontend 79 spec: 80 containers: 81 - name: yelb-ui 82 image: mreferre/yelb-ui:0.3 83 ports: 84 - containerPort: 80 85--- 86apiVersion: apps/v1 87kind: Deployment 88metadata: 89 name: redis-server 90 namespace: yelb 91spec: 92 selector: 93 matchLabels: 94 app: redis-server 95 replicas: 1 96 template: 97 metadata: 98 labels: 99 app: redis-server 100 tier: cache 101 spec: 102 containers: 103 - name: redis-server 104 image: redis:4.0.2 105 ports: 106 - containerPort: 6379 107--- 108apiVersion: apps/v1 109kind: Deployment 110metadata: 111 name: yelb-db 112 namespace: yelb 113spec: 114 selector: 115 matchLabels: 116 app: yelb-db 117 replicas: 1 118 template: 119 metadata: 120 labels: 121 app: yelb-db 122 tier: backenddb 123 spec: 124 containers: 125 - name: yelb-db 126 image: mreferre/yelb-db:0.3 127 ports: 128 - containerPort: 5432 129--- 130apiVersion: apps/v1 131kind: Deployment 132metadata: 133 name: yelb-appserver 134 namespace: yelb 135spec: 136 selector: 137 matchLabels: 138 app: yelb-appserver 139 replicas: 1 140 template: 141 metadata: 142 labels: 143 app: yelb-appserver 144 tier: middletier 145 spec: 146 containers: 147 - name: yelb-appserver 148 image: mreferre/yelb-appserver:0.3 149 ports: 150 - containerPort: 4567 As we can see there is a couple of labels that distinguish the different components in the application which I can map to the application topology above. I only want to allow the frontend to talk to the \u0026quot;app-server\u0026quot;, and the app-server to the \u0026quot;db-server\u0026quot; and \u0026quot;cache-server\u0026quot;. And only on the needed ports. All else should be dropped. On my on-prem NSX manager I have created these groups for the already running on-prem Yelb application. I have created four 5 groups for the Yelb application in total. One group for the frontend (\u0026quot;ui-server\u0026quot;), one for the middletier (app server), one for the backend-db (\u0026quot;db-server\u0026quot;), one for the cache-tier (\u0026quot;cache server\u0026quot;) and one last for all the pods in this application:\nSecurity groups filtering out the Yelb pods\nThe membership criteria inside those groups are made up like this, where I am using the labels in my Yelb manifest (these labels are autopopulated so you dont have to guess). Tag equals label frontend and scope equals label dis:k8s:tier:\nGroup definition for the frontend\nThe same goes for the other groups just using their respective labels. The members should then look like this:\nOnly the frontend pod\nThen I have created a security group that selects all pods in my namespace Yelb by using the label Yelb like this:\nWhich then selects all my pods in the namespace Yelb:\nNow I have my security groups and can go on and create my security policies.\nAntrea security policies from NSX manager Head over to Security, Distributed Firewall section in the on-prem NSX manager to start creating security policies based on your security groups. These are the rules I have created for my application Yelb:\nAntrea Security Policies from the NSX manager\nFirst rule allows traffic from my Avi SE's that are being used to create the service loadbalancer for my application Yelb to the Yelb frontend on HTTP only. Notice that the source part here is in the \u0026quot;Applied to field\u0026quot; (goes for all rules in this example). Thee second rule allows traffic from the frontend to the middletier (\u0026quot;app-server\u0026quot;) on port 4567 only. The third rule allows traffic from middletier to backend-db (\u0026quot;db-server\u0026quot; on port 5432 only. The fourth rule allows traffic from middletier to cache (redis cache) on port 6379 only. All rules according to the topology maps above. Then the last rule is where I am using the namespace selection to select all pods in the namespace Yelb to drop all else not specified above.\nTo verify this I can use the Traceflow feature in Antrea from the NSX manager like this:\n(Head over to Plan \u0026amp; Troubleshoot, Traffic Analysis, Traceflow in your NSX manager)\nChoose Antrea Traceflow, choose the Antrea cluster where your application resides, then select TCP under Protocol type, type in Destination Port (4567) and choose where your pods are from the source and destination. In the screenshot above I want to verify that the needed ports are allowed between Frontend and middletier (application pod).\nClick trace:\nWell that worked, now if I change to port to something else like 4568, am I then still allowed to do that?\nNo, I am not. That is because I have my drop rule in place remember:\nI could go on and test all pod to pod connectivity (I have), but you can trust me their are doing their job. Just to save some screenshots. So that is it, I have microsegmented my Yelb application. But what if I want to scale out this application to my VMC environment. I want to achieve the same thing there. Why not, all our groundwork has already been done so lets head out and spin up the same applicaion on our VMC K8s cluster. Whats going to happen in my on-prem NSX manager. This is cool!\nAntrea security policies in my VMC k8s cluster managed by my on-prem NSX manager Before I deploy my Yelb application in my VMC K8s cluster I want to refresh the memory by showing what my NSX manager knows about the VMC K8s cluster inventory. Lets take a look again. Head over to Inventory in my on-prem NSX manager and take a look at my VMC-ANT-CLUSTER:\n3 nodes you say, and 18 pods you say... Are any of them my Yelb pods?\nNo Yelb pods here...\nNo, there are no yelb pods here. Lets make that a reality. Nothing reported in k8s either:\nandreasm@vmc-k8s-master-01:~/pods$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-2pdcr 2/2 Running 0 3d kube-system antrea-agent-6glpz 2/2 Running 0 3d kube-system antrea-agent-8zzc4 2/2 Running 0 3d kube-system antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d22h kube-system coredns-558bd4d5db-2j7jf 1/1 Running 0 3d kube-system coredns-558bd4d5db-kd2db 1/1 Running 0 3d kube-system etcd-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-apiserver-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-proxy-rvzs6 1/1 Running 0 3d kube-system kube-proxy-tnkxv 1/1 Running 0 3d kube-system kube-proxy-xv77f 1/1 Running 0 3d kube-system kube-scheduler-vmc-k8s-master-01 1/1 Running 0 3d metallb-system controller-7dcc8764f4-6n49s 1/1 Running 0 2d23h metallb-system speaker-58s5v 1/1 Running 0 2d23h metallb-system speaker-7tnhr 1/1 Running 0 2d23h metallb-system speaker-lcq4n 1/1 Running 0 2d23h vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 18 2d23h\nSpin up the Yelb application in my VMC k8s cluster by using the same manifest:\nkubectl apply -f yelb-lb.yaml\nThe result in my k8s cluster:\nandreasm@vmc-k8s-master-01:~/pods$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-2pdcr 2/2 Running 0 3d kube-system antrea-agent-6glpz 2/2 Running 0 3d kube-system antrea-agent-8zzc4 2/2 Running 0 3d kube-system antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d22h kube-system coredns-558bd4d5db-2j7jf 1/1 Running 0 3d kube-system coredns-558bd4d5db-kd2db 1/1 Running 0 3d kube-system etcd-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-apiserver-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-proxy-rvzs6 1/1 Running 0 3d kube-system kube-proxy-tnkxv 1/1 Running 0 3d kube-system kube-proxy-xv77f 1/1 Running 0 3d kube-system kube-scheduler-vmc-k8s-master-01 1/1 Running 0 3d metallb-system controller-7dcc8764f4-6n49s 1/1 Running 0 2d23h metallb-system speaker-58s5v 1/1 Running 0 2d23h metallb-system speaker-7tnhr 1/1 Running 0 2d23h metallb-system speaker-lcq4n 1/1 Running 0 2d23h vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 18 2d23h yelb redis-server-74556bbcb7-fk85b 1/1 Running 0 4s yelb yelb-appserver-6b6dbbddc9-9nkd4 1/1 Running 0 4s yelb yelb-db-5444d69cd8-dcfcp 1/1 Running 0 4s yelb yelb-ui-f74hn 1/1 Running 0 4s\nWhat does my on-prem NSX manager reports?\nHmm 22 pods\nAnd a lot of yelb pods\nInstantly my NSX manager shows them in my inventory that they are up.\nNow, are they being protected by any Antrea security policies? Lets us do the same test as above by using Antrea Traceflow from the on-prem NSX manager with same ports as above (frontend to app 4567 and 4568).\nTraceflow from my on-prem NSX manager in the VMC k8s cluster\nNotice the selection I have done, everything is the VMC k8s cluster.\nNeed port is allowed\nThat is allowed, what about 4568 (which is not needed):\nAlso allowed\nThat is also allowed. I cant have that. How can I make use of my already created policy for this application as easy as possible instead of creating all the rules all over?\nWhell, lets test that. Head over to Security, Distributed firewall in my on-prem NSX manager.\nNotice the Applied to field here:\nWhat happens if I click on it?\nIt only shows me my local Antrea k8s cluster. That is also visible if I list the members in the groups being used in the rules:\nOne pod from my local k8s cluster\nWhat if I add the VMC Antrea cluster?\nCick on the pencil and select your remote VMC K8s cluster:\nApply and Publish:\nNow lets have a look inside our groups being used by our security policy:\nMore pods\nThe Yelb namespace group:\nInstantly my security groups are being updated with more members!!!\nRemember how I created the membership criteria of the groups? Labels from my manifest, and labels for the namespace? Antrea is cluster aware, and dont have to specify a specific namespace to select labels from one specific namespace, it can select from all namespaces as long as the label matches. This is really cool.\nNow what about my security policies in my VMC k8s cluster? Is it enforcing anything?\nLets check, doing a traceflow again. Now only on a disallowed port 4568:\nResult:\nDropped\nThe Security policy is in place, enforcing what it is told to do. The only thing I did in my local NSX manager was to add the VMC Antrea cluster in my Security Applied to section\n","link":"https://yikes.guzware.net/2022/03/13/managing-your-antrea-k8s-clusters-running-in-vmc-from-your-on-prem-nsx-manager/","section":"post","tags":["informational"],"title":"Managing your Antrea K8s clusters running in VMC from your on-prem NSX Manager"},{"body":"","link":"https://yikes.guzware.net/categories/vmware-cloud/","section":"categories","tags":null,"title":"vmware cloud"},{"body":"","link":"https://yikes.guzware.net/categories/vmware-nsx/","section":"categories","tags":null,"title":"vmware nsx"},{"body":"","link":"https://yikes.guzware.net/tags/configuration/","section":"tags","tags":null,"title":"configuration"},{"body":"","link":"https://yikes.guzware.net/tags/deployment/","section":"tags","tags":null,"title":"deployment"},{"body":"","link":"https://yikes.guzware.net/categories/nsx-application-platform/","section":"categories","tags":null,"title":"nsx application platform"},{"body":"VMware NSX 3.2 is out and packed with new features. One of them is the NSX Application Platform which runs on Kubernetes to provide the NSX ATP (Advanced Threat Protection) functionality such as NSX Intelligence (covered in a previous post), NSX Network Detection and Response (NDR) and NSX Malware. This post will go through how to spin up a K8s cluster for this specific scenario covering the pre-reqs from start to finish. After that the features itself will be covered in separate posts. Through this post NSX Application Platform will be abbreviated into NAPP.\nGetting started To get started with NAPP its important that one has read the prerequisites needed to be in place on the K8s cluster that is hosting NAPP. In short NAPP is currently validated to run on upstream K8s version 1.7 all the way up to version 1.21, VMware Tanzu (TKC) versions 1.17.17 to 1.21.2. In addtion to a K8s cluster itself NAPP also needs a Registry supporting images/helm charts. In this walkthrough Harbor will be used. I will also go with an upstream K8s cluster version 1.21.8 running on Ubuntu nodes.\nSources being used to cover this post is mainly from VMware's official documentation. So I list them here as an easy way to reference as they are providing the necessary and important information on how, requirements and all the steps detailed to get NAPP up and running. The purpose of this post is to just go through the steps as a more step by step guide. If more information is needed, head over to our official documentation. Below are the links for the software/tools I have used in this post:\nDeploying and Managing the VMware NSX Application Platform\nHarbor registry\nVMware vSphere Container Storage Plugin\nMetalLB LetsEncrypt\nPrerequisites/Context First some context. In this post all K8s nodes are running as virtual machines on VMware vSphere. NSX-T is responsible for the underlaying network connectivity for the nodes and the persistent storage volumes in my K8s cluster is VMFS exposed through the vSphere Container Storage Plugin (CSI). The NSX manager cluster consists of 3 NSX managers and a cluster VIP address.\nThe vSphere environment consists of two ESXi hosts with shared storage from a FC SAN and vCenter managing the ESXi hosts.\nA quick summary of what is needed, tools/software and what I have used in my setup:\nA upstream kubernetes cluster running any of the supported versions stated in the official documentation. I am using 1.21.8 A working CNI, I am using Antrea. A registry supporting images/helm charts using a signed trusted certificate (no support for self-signed certificates). I am using Harbor and LetsEncrypt certificates. A load balancer to expose the NAPP endpoint with a static ip. I am using MetalLB Persistent storage in K8s I am using the vSphere Container Storage Plugin (if you are running the nodes on vSphere). NSX 3.2 of course Required resources for the NAPP form factor you want to go with. The required resources for the K8s cluster supporting NAPP is outlined below (from the official docs page): This post will cover the Advanced form factor where I went with the following node configuration:\n1 master worker/control plane node with 4 vCPUs, 8GB RAM and 200GB local disk (ephemeral storage). For the worker nodes: 3 worker nodes with 16vCPUs, 64GB RAM and 200GB (ephemeral storage) for the persistent storage (the 1TB disk requirement) I went with vSphere CSI to expose a VMFS datastore from vSphere for the persistent volume requirement.\nThe next chapters will go trough the installation of Harbor (registry) and the configuration done there, then the K8s configuration specifically the CSI and MetalLB part as I am following the generic K8s installation covered earlier here: Deploy Kubernetes on Ubuntu 20.04 .\nLets get to it.\nHarbor (registry requirement) Harbor can be run as a pod in Kubernetes or a pod in Docker. I went with the Docker approach and spun up a VM for this sole purpose.\nThe VM is Ubuntu 20.4 with 4vCPU, 8GB RAM and 200GB disk.\nAfter Ubuntu is installed I installed the necessary dependencies to deploy Harbor in Docker by following the Harbor docs here: Harbor Getting Started\nI find it better to just link the steps below instead of copy/paste too much as the steps are very well documented and could also change over time.\nDocker Engine (latest Stable): docker.com/ubuntu Docker Compose (latest Stable): docker.com/linux Prepare the signed cert (if its not already done) NB! The certificate needs to contain the full chain otherwise the Docker client will not accept it. Download the Harbor installer: Harbor installer (I went with 2.4.1) Extract the online installer: tar -zxvf harbor-online-installer-v2.4.1.tgz Edit the harbor.yaml: Go to the folder harbor (result of the extract above) cp the harbor.yml.tmpl to harbor.yml and use your favourite editor and change the following (snippet from the harbor.yml file): Run the installer: sudo ./install.sh --with-chartmuseum The --with-chartmuseum flag is important (The installer is in the same folder as above and if it is not executable make it executable with chmod +x install.sh check/validate whether you are able to log in to your Harbor registry with the following command: sudo docker login FQDNofHarbor --username admin --password (password defined in harbor.yml). It should not complain about the certificate if the certificate is valid. Log in to the Harbor UI by opening a browser and enter your FQDN of your harbor with the use of admin/password. Create a project: I made a public project\nDownload the NAPP images from your my.vmware.com page: VMware-NSX-Application-Platform-3.2.0.0.0.19067744.tgz upload it to your Harbor VM or to another endpoint where you have a Docker client. \u0026quot;Untar\u0026quot; the tgz file Find and edit the upload_artifacts_to_private_harbor.sh by changing the following: DOCKER_REPO=harborFQDN/napp (after the / is the project name you created) DOCKER_USERNAME=admin DOCKER_PASSWORD=Password-Defined-In-Harbor.yml\nSave and run it sudo ./upload_artifacts_to_private_harbor.sh (same here make it executable with chmod +x if its not executable. This takes a long time, so sit back and enjoy or go do something useful like taking a 3km run in about 15 minutes. The end result shoul look something like this in the Harbor GUI: Thats it for Harbor, next up the K8s cluster\nThe K8s cluster where NAPP is deployed As stated in the official documentation for NAPP, K8s needs to be a specific version, it can be upstream K8s, VMware Tanzu (TKC) or other K8s managed platforms such as OpenShift. But the currently validated platforms are at the moment the above two mentioned platforms: upstream K8s and TKC.\nAs I wrote initially I will go with upstream K8s for this. To get this going I prepared 4 VMs where I dedicate one master worker/control-plane node with the above given specifications and 3 worker nodes with the above given specifications. I follow my previous guide for preparing the Ubuntu os and installing K8s here: K8s on Ubuntu so I will not cover this here but just continue from this with the specifics I did to get the CSI driver up, Antrea CNI and MetalLB. First out is the Antrea CNI.\nAssuming the K8s cluster is partially up, due to no CNI is installed. I download the downstream version of Antrea (one can also use the upstream version from the Antrea github repository) from my.vmware.com. One of the reason I want to use the downstream version from my.vmware.com version is that I want to integrate it to my NSX management plane (more on that in an separate post covering NSX-T with Antrea integration). Download the VMware Container Networking with Antrea (Advanced) from your my.vmware.page to your master worker. Unzip the zip file. Copy the antrea-advanced-debian-v1.2.3_vmware.3.tar.gz to all your worker nodes (with scp for example). Found under the folder antrea-advanced-1.2.3+vmware.3.19009828/images (result of the extract previously). Load the antrea-advanced-debian-v1.2.3_vmware.3.tar.gz image on all nodes, including the master worker, with the command sudo docker load -i antrea-advanced-debian-v1.2.3_vmware.3.tar.gz Apply the antrea-advanced-v1.2.3+vmware.3.yml found under the folder antrea-advanced-1.2.3+vmware.3.19009828/manifests from your master worker. A second or two later you should have a fully working Antrea CNI in your K8s cluster. Notice that your CoreDNS pods decided to go into a running state. Thats it for the Antrea CNI. Next up is MetalLB When the CNI is up, its time for MetalLB. Installation of MetalLB is easy and well explained here: Install MetalLB. Next is the CSI for persistent volume. For step by step config of vSphere Container Storage Plugin head over the the following link (Getting Started with VMware vSphere Container Storage Plug-in section) and follow the instructions there which are very well described. That is, if you are running the VMs on vSphere and want to utilize VMFS as the underlying storage for your persistent volumes. Works great and is fairly easy to deploy. I might come back later and write up a short summary on this one. When you are done with the step above and have your Storage Class defined its over to NSX for deployment of NAPP - Yes! Deploy NAPP - NSX Application Platform Its finally time to head over to the NSX manager and start deployment of NAPP\nTo get this show started, I again must refer to the prerequisites page for NAPP. I will paste below and make some comments:\nFirst requirement: NSX version 3.2 (First release with NAPP). Pr now 3.2 is the one and only NSX-T release that supports NAPP. License ---------- Certificate: The first statement with CA-signed certificates is ok to follow. But the second one could be something that needs to be checked. This is valid if you are using NSX-T Self-Signed certificates. Image that you started out with one NSX manager, enabled the VIP cluster address, then it may well be that this cluster IP gets the certificate of your first NSX manager. So its important to verify that all three NSX managers are using their own unique certificate and the VIP uses its own unique certificate. If not, one must update the certificates accordingly. In my environment I had unique certificates on all NSX manager nodes, but my VIP was using NSX manager 1's certificate. So I had to update the certificate on the VIP. I generated a new certificate from the NSX manager here: And followed the instructions here to replace the VIP certificate: Replace Certificates The currently validated platforms NAPP is supported to run on, been through that earlier. Harbor is covered previously with a dedicated section. Harbor is not a strict requirement though. One can BYO registry if it supports image/helm charts When using upstream K8s, the config does not have a default token expiry. If using TKC one must generate a long-lived token so NAPP wont log out from the K8s cluster. Described in the docs Service Name FQDN is a dns record that is mapped to the IP the the endpoint service gets when deployed. Thats were I use MetalLB for this purpose. Just to initiate a type LoadBalancer. If there is a firewall between your NSX manager and the NAPP K8s cluster, one must do firewall openings accordingly. Time sync is important here also. The K8s cluster must be synced to the NSX Manager. Going through the deployment of NAPP from the NSX manager GUI Log in to the NSX manager GUI. Head over to System and find the new section on the left side called: NSX Application Platform\nFrom there the first thing thats needed to populate is the urls to your Harbor registry (or other BYO registry). The urls goes like this:\nHelm Repository: https://harbor.guzware.net/chartrepo/napp -\u0026gt; FQDN for the Harbor instance, then chartrepo and then the name of the projecy you created in Harbor Docker Registry: harbor.guzware.net/napp/clustering without HTTPS, almost the same url just swap places on napp (project in Harbor) and clustering Click save url and it should validate ok and present you with this and the option to continue in the bottom right corner: Next is the Form factor and kubeconfig:\nThe first thing is to upload your K8s kubeconfig file. Select upload and it will validate. Should you get a warning the the K8s version is newer than the kubectl client onboard the NSX manager upload a newer client from my.vmware.com The Cluster Type is only Standard for now. Storage Class is what you defined in K8s with the CSI (Persistent Volumes) Service Name (FQDN) registered in DNS Form Factor - Here you will have three choices: Standard, Advanced and Evaluation: I have gone with the Advanced form factor. The result should look like this: Now you should be able to click next to do the Precheck Platform for validation:\nFinally Review \u0026amp; Update\nInstallation starts\nAnd after some eager waiting the end result should look like this:\nA brief summary from the K8s cluster:\nA bunch of pods - nice!\nThats it - next time I will continue with the features NAPP brings to the table: NSX Intelligence, Network Detection and Response and NSX Malware Prevention\n","link":"https://yikes.guzware.net/2022/01/18/vmware-nsx-application-platform/","section":"post","tags":["deployment","configuration"],"title":"VMware NSX Application Platform"},{"body":"This page will explain my lab environment, which is used in all the examples, tutorials in this blog.\nLab overview/connectivity - physical, logical and hybrid It is nice to have an overview of how the underlying hardware looks like and when reading my different articles. So I decided to create some diagrams to illustrate this. Which hopefully will help understanding my blog posts further. First out is the physical components (which is relevant for the posts in this blog).\nPhysical hardware My lab consist of two ESXi hosts, one ToR switch (enterprise dc switch with many capabilities) and a fibrechannel SAN (storage).\nLogical overview To make possible all the things I want to do in my lab I am running most of the networking and other features virtually on top of my physical hardware. This includes NSX-T, virtual routers (VM based) in additition to the router functionality in NSX-T and VM based \u0026quot;perimeter\u0026quot; firewall which is based on PfSense. Below is an logical overview of the network topology in my lab.\n","link":"https://yikes.guzware.net/2021/10/19/my-lab/","section":"post","tags":null,"title":"My LAB"},{"body":"This post will go through the IDS/IPS built-in feature of the NSX distributed firewall.\nAbbreviations used in this article:\nIDS = Intrusion Detection System IPS = Intrusion Prevention System Introduction to VMware NSX distributed IDS \u0026amp; IPS Before we dive into how to configure and use the distributed IDS and IPS feature in NSX let me just go through the basics where I compare the traditional approach with IDS/IPS and the NSX distributed IDS/IPS. This article is a continuation on the article Microsegmentation with VMware NSX\u0026quot; where I talk about east/west and north/south traffic pattern and being in context with the workload its supposed to protect. Where being in context is a key thing, especially when it comes to security policies and IDS/IPS. Know what you are protecting, make the inspection as relevant as possible, inspection done optimal (reduce false positives, maintain performance) and at the right place.\nThe traditional way of using IDS/IPS In a more traditional infrastructure we have the perimeter firewall that is responsible for the \u0026quot;environment\u0026quot; policies, enforcing policies between the environments and allowing/blocking different types of the services from each environment to communicate. In such an scenario it is often also the same perimeter firewall that is enabled with IDS/IPS. In a datacenter full of virtualized workload this leads to hairpinning the traffic to a centralized appliance for inspection with the consequence of reducing performance, a lot of unnecessary traffic is sent out to the physical infrastructure to reach the perimeter firewall and sent back again. The appliance is not in context of the workload its analyzing traffic from/to so its hard to be very specific enough when it comes to the right signatures etc. The picture below illustrates this:\nIDS/IPS with a centralized appliance\nNSX Distributed IDS and IPS To overcome the challenges of hairpinning traffic in an virtualized environment, we need to have the firewall, IDS and IPS enforced where the workload actually resides. This saves unnecessary traffic being sent out on the physical infrastructure if its not meant to go out and it also gives the network logics (firewall/IDS/IPS) to be part of the dataplane where the actual workload its supposed to protect resides and can have much more insight (being in context of) in whats going on. Things as knowing its a Ubuntu 20.04 and MySQL server you are protecting, makes it much easier to create the firewall policies but also much more pinpointed/granular IDS/IPS policies. This leads to very specific IDS/IPS rules, no false positives, better performance. This is where NSX Distributed IDS and IPS comes into play. Both the NSX Distributed Firewall and IDS/IPS runs on the same host as the virtual workload you are protecting. Its not necessary to redirect traffic, no need to change anything in the infrastructure, its as simple as just enabling the feature and create policies. Those policies can be created with an application centric perspective, as we have the ability to know the workload we are protecting as the below illustration:\nIDS/IPS polices with only workload relevant signatures\nIDS/IPS available on each hypervisor host\nHow to use IDS \u0026amp; IPS in VMware NSX-T To get started with IDPS in NSX is very easy, its already installed on your transport nodes when you have them enabled with NSX. In the following sections I will go through the different parts in the NSX gui that involves the IDPS part and finish up with an example of how to create policies.\nEnable IDPS, settings and signature updates When one log in to the NSX manager GUI one will see it is divided into different categories such as Networking, Security, and Inventory. IDPS is certainly a security feature of NSX so we will head over there.\nAfter clicking on the Security tab, it will take us to the Security Overview page:\nNSX Security Overview\nAs one can see this gives us a great summarized view over the different security parts in NSX, the IDPS, URL Analysis, the DFW, Anomalies. To see more details in the specific area click on the respective feature on the left side menu. In our case, this is the Distributed IDS/IPS menu.\nWhen inside the Distributed IDS/IPS section, head over to the settings page:\nSettings page of IDPS\nOn this page we can manage the signatures (versions), see the status on the signatures version, whether there is an update on the signature database, update and or adjust whether updates are done automatically. If we want to view the complete list of available signatures click on \u0026quot;View and Manage global signature set\u0026quot;. It should present us a list of all signatures:\nGlobal signature set\nHere we can search for a specific signature, or signatures based on the filter you choose in the top right corner. Say I want to search for signatures relevant to MySQL, I type in \u0026quot;mysql\u0026quot;:\nMysql filter\nBut I can also search for a specific CVE ID (one that we have recently been alerted on maybe):\nCVE-2017-12636\nOr a filter based on CVSS score, in this example 7.5:\nCVSS 7.5\nWe can also adjust the Global default action on specific signatures from Alert, Drop and Reject:\nBy hovering over the blue (!) we will be presented with an explanation of how this works:\nInstead of overriding the global setting for a set of signatures here, we will do this in the next section \u0026quot;IDPS Profiles\u0026quot;.\nFurther down on the same page is where we enable or disable the IDPS feature. It can be enabled on a vSphere cluster (a set of hosts managed by a vCenter) or standalone ESXi hosts. And its just as simple as clicking the enable button on the right side. It should turn green when enabled.\nNow that IDPS is enabled lets head over to Profiles.\nIDPS profiles The profiles section is where we create our application specific signatures we want to use in our IDPS policies (later). We want to adjust and narrow down the total amount of signatures to be used when we create our idps policies for our workload. If I want to create an IDPS policy for a specific application I should create a profile that matches this to reduce false positives, and maintain an optimal inspection with IDS/IPS as an added security feature on top of the Distributed Firewall. In my demo I am interested in only vulnerabilities affecting product \u0026quot;Linux\u0026quot;.\nLets start out by clicking \u0026quot;Add Profile\u0026quot; and create the profile.\nNew profile\nGive the profile a name and start adjusting the signatures we want to use, we start by deciding the Severity Category (Critical, High, Medium and Low). For more information on these categories look here: https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.1/administration/GUID-4343E565-7AC2-40C2-8B12-5FC14893A607.html\nAfter the severity category has been decided we can go ahead and further adjust the specifics we are looking for, please also make note of the IDS Signatures Included number as we proceed in our selection.\nBefore any selections done:\nDefault\nI will go ahead with Severity Critical \u0026amp; High\nThen I can proceed with a selection based Attack Types, Attack Targets, CVSS and Products Affected. I will just post a screenshot from each four options:\nSearch is possible\nSearch is possible\nI will adjust my profile with only Products Affected (this is done to justify the demo of IDS):\nI am satisfied with my selection from now. Let look at the profile now:\nNow I am down to 217 signatures in this specific profile. I also have an option now to override the default action when/if IDPS detects anything and how to respond. Click on the \u0026quot;Manage signatures for this profile\u0026quot; and we should be presented with the signatures relevant only for this profile (after the selection is done):\nThere is an important note to take here. If we only have Alert on the signature, and we want to create an IDPS policy with Detect \u0026amp; Prevent one must have the signature action to either Drop or Reject also. Its not sufficient to just create a policy with Detect \u0026amp; Prevent. That is brilliant if we have signatures in the same profile we don't want to be dropped, but we only want to be notified. Then we can have one rule with Detect \u0026amp; Prevent where some traffic is being dropped (Prevent) while the other is just notified (Detect). So if there is one CVE that you really should drop here you have the option to select just the few of those.\nLets apply the profile to a policy/rule in IDPS so it can act upon it.\nIDPS Policy Under rules in the Distributed IDPS section in NSX:\nClick + Add Policy, this creates a new section:\nName it and then click on the three dots on the left side to create a rule:\nShould now look like this:\nNow we need to fill in source, destination, our IDS Profile created earlier, where to apply the policy and action (mode). In the example below I have already created a couple of security groups (security groups explained in the NSX Distributed firewall article) so I just need to add them to the respective source/destination:\nIDS rule applied to groups used in source and destination\nClick publish and your rule is in effect immediately.\nAlso notice that I did not specify a Service in my rule, here you can be more specific by add the service also, if you know its HTTP for example.\nNow let us check if it detects something....\nDistributed IDS/IPS Events To view detected events, and drill down into the occured events, we must head over to the Events section in our Distributed IDS/IPS section.\nHere we can get a an overview with a timeline on when and where the events have occured.\nIf we look at the overview we can quickly notice if there has been any event in the last 24 hours, 48 hours, last 7 days or 14 days. Lets go through the Events page. Below is a screenshot showing last 24 hours.\nOverview\nTo change the timeline from default 24 hours, take a look at the top right corner and click on the arrow to get the drop down menu.\nNow look at the timeline view using Last 24 hours I can see that there has been 1 event represented by a an orange dot.\nThat tells me very quickly that there has atleast been detected an event. The color code tells me that the event is of severity High.\nIf there are many events in the timeline view, I can choose to filter out the severity I am interested in by unchecking the others.\nLegend color codes\nBy hovering over the orange dot I can get more information on the event:\nI can see what kind of event, and how many attempts of the same kind. If you look closer on the timeline view there are several dots represented. Those represent the other attempts of the same kind. The colored dots will only represent unique occurences within the given timeline. Its also possible to adjust your timeline further if you want to inspect events happening at a certain time within the 24 hours timeline by adjusting the blue sliders:\nAdjusted timeline view\nThen if I adjust it to say just before 18:10 and just after (where there is a dot) the orange dot will appear again as this event suddenly will be unique for this specific time. The bigger timeline view will be updated accordingly. Now I want to know more of this specific event. Look further down, it will be a list (if several unique events has occured represented, again within the timeline give above). The detailed list below will also update according to the adjusted timeline.\nIf one take a look at the below event it says under occurence \u0026quot;Single Attempt\u0026quot;, but I know there are multiple attempts, as I saw before I adjusted the timeline. I \u0026quot;reset\u0026quot; the timeline view back to the full 24 hours view it will be updated to multiple attempts.\nSingle attempt at the given timeline\nMultiple attempts over a longer timeline\nNow if one click on the arrow on the left side more information will be revealed.\nIn this view I can see the source (the attacker, where it is initiated from) and the destination (the target, victim of the \u0026quot;attack\u0026quot;). Intrusion activity, detected and prevented. The number of VMs affected. If one click on the number below VMs affected we will also see a list of VM(s) affected with names:\nIf I now go back to my profile defined earlier, I want to change the signature ID 2023995 to drop and also update my policy to Detect \u0026amp; Prevent. Lets see how this affects the detailed view.\nUpdated the profile\nUpdated the policy\nPrevented events\nWith the profile on this specific signature ID sat do drop and the policy sat to Detect \u0026amp; Prevent it also drops the specific attempt. Meaning I can have a good night sleep, or can I....?\nI should probably do something with the source also. But that should be easy now that we know what the source is.\n","link":"https://yikes.guzware.net/2021/10/19/vmware-nsx-ids-ips/","section":"post","tags":["informational"],"title":"VMware NSX IDS \u0026 IPS"},{"body":"","link":"https://yikes.guzware.net/categories/homeautomation/","section":"categories","tags":null,"title":"homeautomation"},{"body":"When I finish up the other posts I have started on there will be content coming here also\n","link":"https://yikes.guzware.net/2021/07/14/the-home-automation-category/","section":"post","tags":["tips\u0026tricks"],"title":"The Home Automation category"},{"body":"","link":"https://yikes.guzware.net/tags/tipstricks/","section":"tags","tags":null,"title":"tips\u0026tricks"},{"body":"","link":"https://yikes.guzware.net/tags/configurations/","section":"tags","tags":null,"title":"configurations"},{"body":"NSX ALB has a very useful feature built-in, to function as DNS server for your domains defined in your NSX-ALB environment. Meaning that all host-records will be automatically resolved by fqdn as soon as the service is created.\nIf you have followed my other post about how to configure the AKO (Avi Kubernetes Operator) http://yikes.guzware.net/2020/10/08/ako-with-antrea-on-native-k8s-cluster/ you are familiar with creating DNS profiles in NSX-ALB. The first step in configuring NSX-ALB as DNS provider is to configure one or more domain names in NSX-ALB.\nLog in to the NSX-ALB controller GUI: -\u0026gt; Templates -\u0026gt; IPAM/DNS Profiles\nCreate a profile (if you dont already have one) give it a name and add one or more domain names:\nAfter you have configured a DNS profile head over to -\u0026gt; Administration -\u0026gt; Settings -\u0026gt; DNS Service in the controller GUI to create the DNS Virtual Service:\nFrom here one can click \u0026quot;Add Virtual Service\u0026quot; and configure the DNS VS. Go to the empty drop-down list (if you don't already have DNS VS configured) and click Create Virtual Service. Choose your cloud and VRF context.\nOne can also create a DNS VS directly from the Application menu, but by going this way some fields are automatically decided for the use of DNS Service.\nGive the service a name, and adjust accordingly. I have done some adjustment to the service in my environment such as Service port where I add 53 twice and choose Override TCP/UDP on the last one to get DNS on UDP port 53 also. I have also added my backend DNS servers as a pool to this VS to have them do lookup against those if the record is not found locally (not obligatory). Application-Domain-Name should have the same domain name as defined in your DNS Profile attached to your cloud.\nLeave Policies and Analytics as is. Under Advanced you choose your SE pool where your DNS VS should live. As a best practice the DNS SE should not be shared with other VS'es. So create a dedicated pool for the DNS-VS and if resources are scarce you can defined the SE group to only contain one SE (no redundancy for DNS VS though).\nIn my environment I have also created a Conditional forwarder on my backend DNS servers to look for DNS records in my domains defined in the N-ALB environment. Using NSX-ALB DNS provider service is a brilliant feature as I don't have to manually register any applications/services created in N-ALB or from K8s through AKO as this is all handled by the DNS service in N-ALB. My K8s applications can be spun up/down, without having to care about their dns records as this is all handled automatically by the NSX-ALB.\nDemo:\nTake an application created in NSX-ALB\nPing the dns name\nThat's it. Now NSX-ALB handles all your DNS records for you. If you want your backend DNS servers to forward the request to NSX-ALB head over to your DNS servers and either add a Conditional forwarder for your domains or add a Delegated zone as a sub-domain and point to your DNS-VS VIP.\n","link":"https://yikes.guzware.net/2021/07/12/configure-nsx-advanced-load-balancer-nsx-alb-as-dns-provider/","section":"post","tags":["informational","configurations"],"title":"Configure NSX Advanced Load Balancer (NSX-ALB) as DNS provider"},{"body":"Use NFS for your PVC needs If you are running vShere with Tanzu, TKG on vSphere or are using vSphere as your hypervisor for your worker-nodes you have the option to use the vSphere CSI plugin here. In Tanzu this is automatically configured and enabled. But if you are not so privileged to have vSphere as your foundation for your environment one have to look at other options. Thats where NFS comes in. To use NFS for your persistent volumes is quite easy to enable in your environment, but there are some pre-reqs that needs to be placed on your workers (including control plane nodes). I will go through the installation steps below.\nPre-reqs A NFS server available, already configured with shares exported. This could be running on any Linux machine in your environment that has sufficient storage to cover your storage needs for your PVCs. Or any other platform that can export NFS shares.\nInstall and configure This post is based on Ubuntu 20.04 as operating system for all the workers.\nThe first package that needs to be in place is the nfs-common package in Ubuntu. This is installed with the below command, and on all your workers (control-plane and workers):\n1sudo apt install nfs-common -y Now that nfs-common is installed on all workers we are ready deploy the NFS subdir external provisioner link. The below commands is done from your controlplane nodes, if not stated otherwise. I prefer to use Helm. If you dont have Helm installed head over here for how to install Helm. With Helm in place execute the following command to add the NFS Subdir External Provisioner chart:\n1helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ Then we need to install the NFS provisioner like this:\n1helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ 2 --set nfs.server=x.x.x.x \\ 3 --set nfs.path=/exported/path Its quite self-explanatory but I will quickly to through it. --set nfs.server=x.x.x.x \\ needs to be updated with the IP address to your NFS server. --set nfs.path=/exported/path needs to be updated to reflect the path your NFS server exports.\nThats it actually, you know have a storageclass available in your cluster using NFS. The default values for the storageclass deployed without editing the NFS subdir external provisioner helm values looks like this:\n1$kubectl get storageclasses.storage.k8s.io 2NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE 3nfs-client cluster.local/nfs-subdir-external-provisioner Delete Immediate true 51d Values, additional storageclasses If you want to change the default values get the values file, edit it before you deploy the NFS provisioner or get the file, edit it and update your deployment with helm upgrade To grab the values file run this command:\n1helm show values nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \u0026gt; nfs-prov-values.yaml If you want to add additional storageclasses, say with accessmode to RWX, you deploy NFS provisioner with a value file that has the settings you want. Remember to change the class name.\n1 name: XXXXX 2 3 # Allow volume to be expanded dynamically 4 allowVolumeExpansion: true 5 6 # Method used to reclaim an obsoleted volume 7 reclaimPolicy: Delete 8 9 # When set to false your PVs will not be archived by the provisioner upon deletion of the PVC. 10 archiveOnDelete: true 11 12 # If it exists and has \u0026#39;delete\u0026#39; value, delete the directory. If it exists and has \u0026#39;retain\u0026#39; value, save the directory. 13 # Overrides archiveOnDelete. 14 # Ignored if value not set. 15 onDelete: 16 17 # Specifies a template for creating a directory path via PVC metadata\u0026#39;s such as labels, annotations, name or namespace. 18 # Ignored if value not set. 19 pathPattern: 20 21 # Set access mode - ReadWriteOnce, ReadOnlyMany or ReadWriteMany 22 accessModes: XXXXXXX Above is a snippet from the values file.\nDeploying pods with special privileges If you need to deploy pods with special privileges, often mysql containers, you need to prepare your NFS server and filesystem permission for that. Otherwise they will not be able to write correctly to their PV when they are deployed. The example below is a mysql container that needs to create its own permission on the filesystem it writes to:\n1 spec: 2 securityContext: 3 runAsUser: 999 4 fsGroup: 999 So what I did to solve this is the following: I changed my NFS export to look like this on my NFS server: /path/to/share -alldirs -maproot=\u0026quot;root\u0026quot;:\u0026quot;wheel\u0026quot; Then I need to update the permissions on the NFS server filesystem with these commands and permissions:\n1$chown nobody:nogroup /shared/folder 2$chmod 777 /shared/folder ","link":"https://yikes.guzware.net/2021/07/12/kubernetes-persistent-volumes-with-nfs/","section":"post","tags":["configurations"],"title":"Kubernetes Persistent Volumes with NFS"},{"body":"","link":"https://yikes.guzware.net/categories/nsx-advanced-loadbalancer/","section":"categories","tags":null,"title":"nsx-advanced-loadbalancer"},{"body":"","link":"https://yikes.guzware.net/categories/ako/","section":"categories","tags":null,"title":"ako"},{"body":"","link":"https://yikes.guzware.net/categories/avi/","section":"categories","tags":null,"title":"avi"},{"body":"Abbreviations used in this article:\nNSX Advanced Load Balancer = NSX-ALB K8s = Kubernetes (8 letters between the K and s in Kubernetes) SSL = Secure Sockets Layer AKO = Avi Kubernetes Operator (AVI now a VMware product called NSX Advanced Load Balancer) In one of my previous posts I wrote about how to install and configure AKO (Avi Kubernetes Operator) to use as Service type LoadBalancer.\nThis post will try to cover the basics of how to use NSX Advanced LoadBalancer by using AKO to handle our Ingress requests (ingress-controller).\nFor more information on Ingress in Kubernetes\nAn API object that manages external access to the services in a cluster, typically HTTP.\nIngress may provide load balancing, SSL termination and name-based virtual hosting.\nIngress Load Balancer Kubernetes Definition Within Kubernetes or K8s, a collection of routing rules that control how Kubernetes cluster services are accessed by external users is called ingress. Managing ingress in Kubernetes can take one of several approaches.\nAn application can be exposed to external users via a Kubernetes ingress resource; a Kubernetes NodePort service which exposes the application on a port across each node; or using an ingress load balancer for Kubernetes that points to a service in your cluster.\nAn external load balancer routes external traffic to a Kubernetes service in your cluster and is associated with a specific IP address. Its precise implementation is controlled by which service types the cloud provider supports. Kubernetes deployments on bare metal may require custom load balancer implementations.\nHowever, properly supported ingress load balancing for Kubernetes is the simplest, more secure way to route traffic. link\nGetting AKO ready While this post assumes AKO is already in place and working in your k8s clusters I will get straight to the parts that involve Ingress. If not head over here to read the official docs how to install Avi Kubernetes Operator (AKO). To verify that you AKO is ready to handle Ingress request, type in this and notice the output:\n1$ kubectl get ingressclasses.networking.k8s.io 2NAME CONTROLLER PARAMETERS AGE 3avi-lb ako.vmware.com/avi-lb \u0026lt;none\u0026gt; 50d Default secret for TLS Ingress As AKO expects all ingresses with TLS termination to have a key and certificate specified, there is a couple of ways this can be done. We can go with a \u0026quot;pr service\u0026quot;, meaning a dedicated set of keys and certs pr service or a default/common set of keys and certificates that AKO can use if nothing else is specified. To apply the common approach, one common key and certificate for one or more applications we need to add a secret for the AKO. Prepare your router-default.yaml definition file like this (the official docs wants you to put in your cert as is, that does not work so you need to base64 encode both keys and certs and paste in below):\n1apiVersion: v1 2kind: Secret 3metadata: 4 name: router-certs-default 5 namespace: avi-system 6type: kubernetes.io/tls 7data: 8 tls.key: \u0026#34;base64 encoded\u0026#34; 9 tls.crt: \u0026#34;base64 encoded\u0026#34; 10 alt.key: \u0026#34;base64 encoded\u0026#34; 11 alt.crt: \u0026#34;base64 encoded\u0026#34; 12 To base64 encode your keys and certs this can be done like this:\nIf you have the keys and certs in a file, from whatever linux terminal type in:\n1cat cert.pem | base64 -w 0 2cat key.pem | base64 -w 0 Then paste into the above yaml accordingly (tls.key:key, tls.crt:crt) If you have both ECDSA and RSA certs use the alt.key and alt.crt to apply both. As soon as everything is pasted, apply the yaml file kubectl apply -f router-defaults.yaml\nApply your ingress service To create an ingress service you need to define this in yaml. An example below:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: \u0026#34;NameOfIngress-Service\u0026#34; 5 namespace: \u0026#34;Namespaceofwhereyour-service-resides\u0026#34; 6 labels: 7 app: \u0026#34;ifyouwant\u0026#34; 8 annotations: 9 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; #\u0026#34;Indicates to Avi that you want to use TLS\u0026#34; 10spec: 11 ingressClassName: avi-lb #\u0026#34;The default class name for AVI\u0026#34; 12 rules: 13 - host: \u0026#34;FQDN\u0026#34; 14 http: 15 paths: 16 - pathType: Prefix 17 path: / 18 backend: 19 service: 20 name: \u0026#34;which-service-to-point-to\u0026#34; 21 port: 22 number: 80 Hostrules and HTTPrules As I mentioned earlier, you can also define specific rules pr server such as certificates. Here we can use Hostrules and HTTPrules to further adjust granular settings pr service. One Hostrule example below:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: HostRule 3metadata: 4 name: name-of-your-rule 5 namespace: namespace-of-your-service 6spec: 7 virtualhost: 8 fqdn: must-match-hostname-above # mandatory 9 fqdnType: Exact 10 enableVirtualHost: true 11 tls: # optional 12 sslKeyCertificate: 13 name: \u0026#34;name-of-certificate\u0026#34; # This must be already defined in your AVI controller 14 type: ref 15 alternateCertificate: 16 name: \u0026#34;name-of-alternate-cert\u0026#34; # This must be already defined in your AVI controller 17 type: ref 18 sslProfile: System-Standard-PFS 19 termination: edge To get all features available head over to the official docs site here\nHostrule example from the official docs:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: HostRule 3metadata: 4 name: my-host-rule 5 namespace: red 6spec: 7 virtualhost: 8 fqdn: foo.region1.com # mandatory 9 fqdnType: Exact 10 enableVirtualHost: true 11 tls: # optional 12 sslKeyCertificate: 13 name: avi-ssl-key-cert 14 type: ref 15 alternateCertificate: 16 name: avi-ssl-key-cert2 17 type: ref 18 sslProfile: avi-ssl-profile 19 termination: edge 20 gslb: 21 fqdn: foo.com 22 includeAliases: false 23 httpPolicy: 24 policySets: 25 - avi-secure-policy-ref 26 overwrite: false 27 datascripts: 28 - avi-datascript-redirect-app1 29 wafPolicy: avi-waf-policy 30 applicationProfile: avi-app-ref 31 analyticsProfile: avi-analytics-ref 32 errorPageProfile: avi-errorpage-ref 33 analyticsPolicy: # optional 34 fullClientLogs: 35 enabled: true 36 throttle: HIGH 37 logAllHeaders: true 38 tcpSettings: 39 listeners: 40 - port: 8081 41 - port: 6443 42 enableSSL: true 43 loadBalancerIP: 10.10.10.1 44 aliases: # optional 45 - bar.com 46 - baz.com Httprule example from the official docs:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: HTTPRule 3metadata: 4 name: my-http-rule 5 namespace: purple-l7 6spec: 7 fqdn: foo.avi.internal 8 paths: 9 - target: /foo 10 healthMonitors: 11 - my-health-monitor-1 12 - my-health-monitor-2 13 loadBalancerPolicy: 14 algorithm: LB_ALGORITHM_CONSISTENT_HASH 15 hash: LB_ALGORITHM_CONSISTENT_HASH_SOURCE_IP_ADDRESS 16 tls: ## This is a re-encrypt to pool 17 type: reencrypt # Mandatory [re-encrypt] 18 sslProfile: avi-ssl-profile 19 destinationCA: |- 20 -----BEGIN CERTIFICATE----- 21 [...] 22 -----END CERTIFICATE----- ","link":"https://yikes.guzware.net/2021/07/11/k8s-ingress-with-nsx-advanced-load-balancer/","section":"post","tags":["configurations"],"title":"K8s Ingress with NSX Advanced Load Balancer"},{"body":" This is an introduction post to Antrea, what it is and which features it has.\nFor more details head over to:\nhttps://antrea.io/ and https://github.com/antrea-io/antrea\nFirst of, Antrea is a CNI. CNI stands for Container Network Interface. As the world moves into Kubernetes more and more, we need a good CNI to support everything from network to security within Kubernetes. Thats where Antrea comes into play.\nAntrea has a rich set of features such as:\nKubernetes-native: Antrea follows best practices to extend the Kubernetes APIs and provide familiar abstractions to users, while also leveraging Kubernetes libraries in its own implementation. Powered by Open vSwitch: Antrea relies on Open vSwitch to implement all networking functions, including Kubernetes Service load-balancing, and to enable hardware offloading in order to support the most demanding workloads. Run everywhere: Run Antrea in private clouds, public clouds and on bare metal, and select the appropriate traffic mode (with or without overlay) based on your infrastructure and use case. Windows Node support: Thanks to the portability of Open vSwitch, Antrea can use the same data plane implementation on both Linux and Windows Kubernetes Nodes. Comprehensive policy model: Antrea provides a comprehensive network policy model, which builds upon Kubernetes Network Policies with new features such as policy tiering, rule priorities and cluster-level policies. Troubleshooting and monitoring tools: Antrea comes with CLI and UI tools which provide visibility and diagnostics capabilities (packet tracing, policy analysis, flow inspection). It exposes Prometheus metrics and supports exporting network flow information which can be visualized in Kibana dashboards. Encryption: Encryption of inter-Node Pod traffic with IPsec tunnels when using an overlay Pod network. Easy deployment: Antrea is deployed by applying a single YAML manifest file. As this blog page evolves, it will cover in more technical posts how to use and configure Antrea with examples. As how this webpage is both handled by Antrea network and security features (yes, this wordpress page is hosted on a native K8s cluster with Antrea as CNI)\n","link":"https://yikes.guzware.net/2021/07/10/antrea-kubernetes-cni/","section":"post","tags":["informational"],"title":"Antrea - Kubernetes CNI"},{"body":"","link":"https://yikes.guzware.net/categories/kubernetesnetworking/","section":"categories","tags":null,"title":"kubernetesnetworking"},{"body":"This post will go through one way of securing your workloads with VMware NSX. It will cover the different tools and features built into NSX to achieve a robust and automated way of securing your workload. It will go through the use of Security Groups, how they can be utilized, and how to create security policies in the distributed firewall section of NSX-T with the use of the security groups.\nIntroduction to NSX Distributed Firewall If we take a look inside a modern datacenter we will discover very soon that there is not so much bare metal anymore (physical server with one operating system and often many services to utilize the resources), most workload today is virtualized. From a network perspective the traffic pattern has shifted from being very much north/south to very much east/west. A typical traffic distribution today between north/south and east/west is a 10% (+/-) north/south and 90%(+/-) east/west. When the traffic pattern consisted of a high amount north/south it made sense to have our perimeter firewall regulate and enforce firewall rules in and out of the DC and between server workload. Due to server virtualization a major part of the DC the workload consist of many virtual machine instances with very specific services and \u0026quot;intra\u0026quot; communication (east/west) is a large part. It is operationally a tough task to manage a perimeter firewall to be the \u0026quot;policy enforcer\u0026quot; between workload in the east/west \u0026quot;zone\u0026quot;. It is also very hard for a discrete appliance to be part of the context (it is outside of the dataplane/context of the workload it is trying to protect).. Will delve into this in more detail later in the article. Will also illustrate east/west and north/south traffic pattern.\n","link":"https://yikes.guzware.net/2021/07/10/microsegmentation-with-vmware-nsx/","section":"post","tags":["informational"],"title":"Microsegmentation with VMware NSX"},{"body":"When starting out a microsegmentation journey with VMware NSX it will be very important to have a tool that gives you all the visibility and insight you need. This is crucial if you dont know your applications requirements in detail and to make the right decisions in defining your NSX security policies.\nNSX Intelligence is your tool for that. This post is just to show a couple of screenshots of how it looks, and the next post will go more into detail how it works and how to use it.\n","link":"https://yikes.guzware.net/2021/07/10/nsx-intelligence-quick-overview/","section":"post","tags":["informational"],"title":"NSX Intelligence - quick overview"},{"body":"","link":"https://yikes.guzware.net/categories/nsx-t/","section":"categories","tags":null,"title":"nsx-t"},{"body":"","link":"https://yikes.guzware.net/categories/security/","section":"categories","tags":null,"title":"security"},{"body":"","link":"https://yikes.guzware.net/categories/vmare-nsx-intelligence/","section":"categories","tags":null,"title":"vmare nsx intelligence"},{"body":"This post will cover the steps to bring up a Kubernetes cluster in Ubuntu, then how to install and configure Antrea as CNI and how to install and configure NSX Advanced Load Balancer to use as a servicetype Load Balancer in the k8s environment with the use of Avi Kubernetes Operator.\nAbbreviations used in this post:\nNSX Advanced Load Balancer = NSX ALB Avi Kubernetes Operator = AKO Kubernetes = k8s Container Network Interface = CNI Load Balancer = lb Introduction to this post When working with pods in a k8s cluster there is often the use of nodePort, clusterIP and LoadBalancer. In a three tiered application very often only one of the tier is necessary to expose to the \u0026quot;outside\u0026quot; (outside of the k8s cluster) world so clients can reach the application. There are several ways to do this, and one of them is using the service Load Balancer in k8s. The point of this post is to make use of NSX ALB to be the load balancer when you call for the service load balancer in your application. There are some steps needed to be done to get this up and running, as will be covered in this post, and when done you can enjoy the beauty of automatically provisioned lb's by just stating in your yaml that you want a lb for the specific service/pod/application and NSX ALB will take care of all the configuration needed to make your application available through a VIP. One of the steps involved in making this happen is deploying the AKO in your k8s cluster. AKO runs as a pod and is very easy to deploy and configure.\nDiagram over topology Deploy Kubernetes on Ubuntu 20.04 Prepare the Worker and Master nodes. For this part the initial process was taken from here with modifications from my side.\nThis will be a small 3-node cluster with 1 Master and 2 Worker Nodes.\nInstall 3 Ubuntu VMs, or one and clone from that.\n1sudo hostnamectl set-hostname \u0026lt;hostname\u0026gt; By using Ubuntu Server as image make sure openssh is installed.\nDisable Swap\n1sudo swapoff -a Edit fstab and comment out the swap entry.\nVerify with the command:\n1free -h Install packages on the master and worker nodes: On all nodes: 1sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl Add Kubernetes repository:\nAdd key to Apt:\n1sudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - Add repo:\n1cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list 2deb https://apt.kubernetes.io/ kubernetes-xenial main 3EOF Install Kubeadm, Kubelet and Kubectl\n1sudo apt-get update 2 3sudo apt-get install -y kubelet kubeadm kubectl Note, this will install default Kubernetes release in apt repo from your distro, if you want to install a specific version, follow this:\nOverview of differente Kubernets versions and dependices:\n_https://packages.cloud.google.com/apt/dists/kubernetes-xenial/main/binary-arm64/Packages\n_Lets say we want to install Kubeadmin 1.18.9:\n1sudo apt-get install kubelet=1.18.9-00 kubectl kubeadm=1.18.9-00 Check the below link if you can install a later version of kubectl than kubeadmin\nhttps://kubernetes.io/docs/setup/release/version-skew-policy/\nThe latter apt-get install command install kubelet version 1.18.9 and kubeadm 1.18.9 but kubectl will be the default release in apt (1.19.x for me).\nAfter the the three packages have been installed, set them on hold:\n1sudo apt-mark hold kubelet kubeadm kubectl 2 3kubelet set on hold. 4kubeadm set on hold. 5kubectl set on hold. Install Docker container runtime:\n1sudo apt-get install docker.io -y On the master-node init the Kubernetes master worker: 1sudo kubeadm init --pod-network-cidr=10.162.0.0/16 --apiserver-cert-extra-sans apihost.corp.local Change the CIDR accordingly to match your defined pod-network. It comes down to if you want to do a routable or nat’ed toplogy. And by using the –apiserver-cert-extra variable it will generate the certs to also reflect the dns-name, making it easier to expose this with a name instead of just the ip of the master worker.\nOn the worker-nodes, join the control plane of the master worker: 1sudo kubeadm join apihost.corp.local:6443 --token \u0026lt;toke\u0026gt; --discovery-token-ca-cert-hash \u0026lt;hash\u0026gt; The token and hash is presented to you after you have init’d the master-node in the previous step above.\nAfter you have joined all you worker nodes to the control-plane you have a working Kubernetes cluster, but without any pod-networking plugin (CNI), this will be explained a bit later and is also the reason for creating this guide.\nTo make it easier to access your cluster copy the kube config to your $HOME/.kube folder:\n1mkdir -p $HOME/.kube 2 3sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 4sudo chown $(id -u):$(id -g) $HOME/.kube/config You can now to a kubectl get pods --all-namespaces and notice that the coredns pods have status pending. That is because there is not pod networking up and running yet.\nInstall Antrea as the CNI **To read more about Antrea, go here:\n**https://github.com/vmware-tanzu/antrea\nTo get started with Antrea is very easy.\nhttps://github.com/vmware-tanzu/antrea/blob/master/docs/getting-started.md\nI will post a dedicated post for Antrea, covering Antrea Policies and other features in Antrea.\nOn the master worker:\nDecide first which version/release of Antrea you want (latest as of this writing is v1.1.1), this could depend on certain dependencies with other solutions you are going to be using. In my case I am using VMware Advanced LoadBalancer AKO v1.4.2 (Previously AVI Networks) as Service LB for my pods/apps. See this for compatibility guide: https://avinetworks.com/docs/ako/1.4/ako-compatibility-guide/\nTag the release you want to be installed of Antrea:\n_TAG=v1.1.1\n_Again, this is all done on the master-worker\nThen apply:\nkubectl apply -f [https://github.com/vmware-tanzu/antrea/releases/download/$TAG/antrea.yml](https://github.com/vmware-tanzu/antrea/releases/download/$TAG/antrea.yml)\nThis will automatically create the needed interfaces on all the workers. After a short time check your pods:\n1Kubectl get pods --all-namespaces 2kube-system antrea-agent-bzdkx 2/2 Running 0 23h 3kube-system antrea-agent-lvdxk 2/2 Running 0 23h 4kube-system antrea-agent-zqp6d 2/2 Running 0 23h 5kube-system antrea-controller-55946849c-hczkw 1/1 Running 0 23h 6kube-system antrea-octant-f59b76dd9-6gj82 1/1 Running 0 10h 7kube-system coredns-66bff467f8-6qz2q 1/1 Running 0 23h 8kube-system coredns-66bff467f8-cd6dw 1/1 Running 0 23h Coredns is now running and antrea controller/agents have been installed.\nIf you look at your worker nodes, they have also been configured with some extra interfaces:\n4: ovs-system:\n5: genev_sys_6081:\n6: antrea-gw0:\n**You now have a working container network interface, what now?\n**Wouldn’t it be cool to utilize a service load balancer to easily scale and expose your frontends?\nInstall AVI Kubernetes Operator Here comes AKO (Avi Kubernetes Operator), VMware Advanced LoadBalancer\nTo read more about AKO visit: https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes and https://avinetworks.com/docs/ako/1.4/ako-release-notes/\nTo be able to install AKO there are some prereqs that needs to be done on both your k8s cluster and NSX-ALB controller.\nPrepare NSX-ALB for AKO Lets start with the pre-reqs on the NSX-ALB controller side by logging into the controller GUI.\nDisclaimer, I have followed some of the official documentation here: https://avinetworks.com/docs/ako/1.2/ako-installation/ but I couldn’t follow everything there as it did not work in my environment.\nThis guide assumes NSX-ALB has been configured with a default-cloud with vCenter and the networks for your VIP subnet, node-network already has been defined as vDS portgroups in vCenter. In additon to basic knowledge of NSX-ALB.\nIn the NSX-ALB Controller\nCreate a dedicated SE group for the K8s cluster you want to use with AVI, define it the way you would like:\nCreate DNS and IPAM profiles\nIn IPAM define this:\nName, cloud and the VIP network for the SEs frontend facing IP. The network that should be reachable outside the node-network.\nGo to infrastructure and select your newly created IPAM/DNS profiles in your default-cloud:\nWhile you are editing the default-cloud also make sure you have configured these settings:\nDHCP is for the management IP interface on the SEs. The two other options “Prefer Static” \u0026amp; Use Static” I have to use otherwise it will not work, the AKO pod refuses to start, crashloopbackoff. And I am not doing BGP on the Avi side, only from the T0 of NSX. One can use the default global routing context as it can be shared, no need to define a custom routing context.\nUnder network select the management network and your SE group created for AKO SEs. And ofcourse enable DHCP:\nNow we are done on the NSX-ALB controller side. Lets jump back to our K8s Master worker to install AKO.\nThe initial steps here I have taken from:\nhttps://www.vrealize.it/2020/09/15/installing-antrea-container-networking-and-avi-kubernets-operator-ako-for-ingress/\nInstall AKO: AKO is installed with helm, so we need to install helm on our master worker:\nsudo snap install helm --classic\nOr whatever preferred way to install packages in Ubuntu.\n**Create a namespace for the AKO pod:\n**\n1 kubectl create ns avi-system **Add AKO incubator repository\n**\n1 helm repo add ako https://projects.registry.vmware.com/chartrepo/ako **Search for available charts and find the version number:\n**\n1helm search repo 2 3 NAME CHART VERSION\tAPP VERSION\tDESCRIPTION 4 ako/ako 1.4.2 1.4.2 A helm chart for Avi Kubernetes Operator 5 Download AKO values.yaml\n1 helm show values ako/ako --version 1.4.2 \u0026gt; values.yaml This needs to be edited according to your needs. See the comments in the value.yaml and you will se whats needed to be updated.\nI will publish my value.yaml file here and comment what I edited.\nI will remove the default comments and replace them with my own, just refer to de default comments by looking at the original yaml file here:\nhttps://raw.githubusercontent.com/avinetworks/avi-helm-charts/master/charts/stable/ako/values.yaml\n1# Default values for ako. 2# This is a YAML-formatted file. 3# Declare variables to be passed into your templates. 4 5replicaCount: 1 6 7image: 8 repository: avinetworks/ako 9 pullPolicy: IfNotPresent 10 11 12### This section outlines the generic AKO settings 13AKOSettings: 14 logLevel: \u0026#34;INFO\u0026#34; #enum: INFO|DEBUG|WARN|ERROR 15 fullSyncFrequency: \u0026#34;1800\u0026#34; # This frequency controls how often AKO polls the Avi controller to update itself with cloud configurations. 16 apiServerPort: 8080 # Specify the port for the API server, default is set as 8080 // EmptyAllowed: false 17 deleteConfig: \u0026#34;false\u0026#34; # Has to be set to true in configmap if user wants to delete AKO created objects from AVI 18 disableStaticRouteSync: \u0026#34;false\u0026#34; # If the POD networks are reachable from the Avi SE, set this knob to true. 19 clusterName: \u0026#34;GuzK8s\u0026#34; # A unique identifier for the kubernetes cluster, that helps distinguish the objects for this cluster in the avi controller. // MUST-EDIT 20 cniPlugin: \u0026#34;\u0026#34; # Set the string if your CNI is calico or openshift. enum: calico|canal|flannel|openshift 21 22### This section outlines the network settings for virtualservices. 23NetworkSettings: 24 ## This list of network and cidrs are used in pool placement network for vcenter cloud. 25 ## Node Network details are not needed when in nodeport mode / static routes are disabled / non vcenter clouds. 26 # nodeNetworkList: [] 27 nodeNetworkList: 28 - networkName: \u0026#34;Native-K8s-cluster\u0026#34; 29 cidrs: 30 - 192.168.0.0/24 # NODE network 31 subnetIP: \u0026#34;10.150.4.0\u0026#34; # Subnet IP of the vip network 32 subnetPrefix: \u0026#34;255.255.255.0\u0026#34; # Subnet Prefix of the vip network 33 networkName: \u0026#34;NativeK8sVIP\u0026#34; # Network Name of the vip network 34 35### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO. 36L7Settings: 37 defaultIngController: \u0026#34;true\u0026#34; 38 l7ShardingScheme: \u0026#34;hostname\u0026#34; 39 serviceType: \u0026#34;ClusterIP\u0026#34; #enum NodePort|ClusterIP 40 shardVSSize: \u0026#34;SMALL\u0026#34; # Use this to control the layer 7 VS numbers. This applies to both secure/insecure VSes but does not apply for passthrough. ENUMs: LARGE, MEDIUM, SMALL 41 passthroughShardSize: \u0026#34;SMALL\u0026#34; # Control the passthrough virtualservice numbers using this ENUM. ENUMs: LARGE, MEDIUM, SMALL 42 43### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO. 44L4Settings: 45 defaultDomain: \u0026#34;\u0026#34; # If multiple sub-domains are configured in the cloud, use this knob to set the default sub-domain to use for L4 VSes. 46 47### This section outlines settings on the Avi controller that affects AKO\u0026#39;s functionality. 48ControllerSettings: 49 serviceEngineGroupName: \u0026#34;k8s2se\u0026#34; # Name of the ServiceEngine Group. 50 controllerVersion: \u0026#34;20.1.1\u0026#34; # The controller API version 51 cloudName: \u0026#34;Default-Cloud\u0026#34; # The configured cloud name on the Avi controller. 52 controllerIP: \u0026#34;172.18.5.50\u0026#34; 53 54nodePortSelector: # Only applicable if serviceType is NodePort 55 key: \u0026#34;\u0026#34; 56 value: \u0026#34;\u0026#34; 57 58resources: 59 limits: 60 cpu: 250m 61 memory: 300Mi 62 requests: 63 cpu: 100m 64 memory: 75Mi 65 66podSecurityContext: {} 67 # fsGroup: 2000 68 69 70avicredentials: 71 username: \u0026#34;admin\u0026#34; 72 password: \u0026#34;PASSWORD\u0026#34; 73 74 75service: 76 type: ClusterIP 77 port: 80 78 79 80persistentVolumeClaim: \u0026#34;\u0026#34; 81mountPath: \u0026#34;/log\u0026#34; 82logFile: \u0026#34;avi.log\u0026#34; Remember that YAML’s are indentation sensitive.. so watch your spaces 😉\n**Deploy the AKO controller:\n**helm install ako/ako --generate-name --version 1.4.2 -f avi-values.yaml --namespace=avi-system\nOne can verify that it has been installed with the following command:\nhelm list --namespace=avi-system\nand\nKubectl get pods --namespace=avi-system\nNAME READY STATUS RESTARTS AGE\nako-0 1/1 Running 0 15h\nOne can also check the logs of the pod:\nkubectl logs --namespace=avi-system ako-0\nIf you experience a lot of restarts, go through your config again, I struggled a lot to get it running in my lab the first time after I figured out there were some configs I had to to. I suspected some issues with the pod itself, but the problem was on the NSX-ALB controller side and values.yaml parameters.\nNow to test out the automatic creation of SE and ingress for your frontend install an application and change the service type to use loadBalancer. Everything is automagically created for you. Monitor progress in the NSX-ALB Controller and vCenter.\nNSX-ALB deploys the SE OVAs, as soon as they are up and running they will be automatically configured and you can access your application through the NSX-ALB VIP IP. You can of ofcourse scale the amount of SEs and so on from within the Avi controller.\nThe ips on the right side is the pods, and my frontend/public facing IP is:\nWhich can also be received by using:\nkubectl get service --namespace=app\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nfrontend-external LoadBalancer 10.96.131.185 10.150.4.2 80:31874/TCP 15h\nNow we have all the power and features from NSX Advanced LoadBalancer, with full visibility and logging. What about monitoring and features of Antra, the CNI plugin?\nInstall Octant - opensource dashboard for K8s https://github.com/vmware-tanzu/octant\nOctant with Antrea Plugin as a POD\nhttps://github.com/vmware-tanzu/antrea/blob/master/docs/octant-plugin-installation.md\nUpgrading the components used in this blog Upgrade Antrea Say you are running Antrea version v0.9.1 and want to upgrade to v0.10.1. Do a rolling upgrade like this:\nFirst out is the Antrea-Controller!\nkubectl set image deployments/antrea-controller antrea-controller=antrea/antrea-ubuntu:v0.10.1 --namespace=kube-system\nCheck status on upgrade:\nkubectl rollout status deployments/antrea-controller --namespace=kube-system\nThis upgrades the Antrea Controller, next up is the Antrea Agents.\nUpgrading the Antrea-agents:\nkubectl set image daemonset/antrea-agent antrea-agent=antrea/antrea-ubuntu:v0.10.1 --namespace=kube-system\nAs you are doing the rolling upgrades, one can monitor the process by following the pods or use Octant and get real-time updates when the agents have been upgraded.\nDoing changes in the antrea yaml file, if changes is not updated, do a rollut restart:\nkubectl rollout restart daemonset --namespace=kube-system antrea-agent\n","link":"https://yikes.guzware.net/2020/10/08/nsx-advanced-loadbalancer-with-antrea-on-native-k8s/","section":"post","tags":["deployment","howto"],"title":"NSX Advanced LoadBalancer with Antrea on Native K8s"},{"body":"To be able to deploy an Edge node or nodes in your lab or other environment where you only have 2 physical nic you must be able to deploy it on a N-VDS switch as you have already migrated all your kernels etc to this one N-VDS switch.\nBut trying to do this from the NSX-T 2.4 manager GUI you will only have the option to deploy it to VSS or VDS portgroups, the N-VDS portgroups are not visible at all.\nSo, I followed this blog by Amit Aneja which explains how this works.\nSo after I read this blog post I sat out to try this. I had to write down the api-script he used by hand because I could not find it when I searched for a example I could use. By using PostMan I filled out this:\n1{ \u0026#34;resource\\_type\u0026#34;: \u0026#34;EdgeNode\u0026#34;, 2 3\u0026#34;display\\_name\u0026#34;: \u0026#34;YourEdgevCenterInventoryName\u0026#34;, \u0026#34;tags\u0026#34;: \\[\\], \u0026#34;ip\\_addresses\u0026#34;: \\[ \u0026#34;X.X.X.X\u0026#34; (Your edge MGMT IP adress) \\], \u0026#34;deployment\\_config\u0026#34;: { \u0026#34;vm\\_deployment\\_config\u0026#34;: { \u0026#34;placement\\_type\u0026#34;: \u0026#34;VsphereDeploymentConfig\u0026#34;, \u0026#34;vc\\_id\u0026#34;: \u0026#34;YourvCenterIDFromNSXTManager\u0026#34;, \u0026#34;management\\_network\\_id\u0026#34;: \u0026#34;YourLSPortGroupIDFromNSXTManager\u0026#34;, \u0026#34;default\\_gateway\\_addresses\u0026#34;: \\[ \u0026#34;X.X.X.X\u0026#34; \\], \u0026#34;compute\\_id\u0026#34;: \u0026#34;YourClusterIDFromNSXTManager\u0026#34;, \u0026#34;allow\\_ssh\\_root\\_login\u0026#34;: true, \u0026#34;enable\\_ssh\u0026#34;: true, \u0026#34;hostname\u0026#34;: \u0026#34;yourEdge\\_FQDNName\u0026#34;, \u0026#34;storage\\_id\u0026#34;: \u0026#34;YourDataStoreIDfromNSXTManager\u0026#34;, \u0026#34;management\\_port\\_subnets\u0026#34;: \\[ { \u0026#34;ip\\_addresses\u0026#34;: \\[ \u0026#34;YourEdgeIPMGMT\\_AddressAgain\u0026#34; \\], \u0026#34;prefix\\_length\u0026#34;: 24 } \\], 4 5\u0026#34;data\\_network\\_ids\u0026#34;: \\[ \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_OverLayVLAN(NotTheHostOverlayVLAN)\u0026#34;, \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_VLANUplink1\u0026#34;, \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_VLANUplink1\u0026#34; \\] }, \u0026#34;form\\_factor\u0026#34;: \u0026#34;SMALL\u0026#34;, 6 7\u0026#34;node\\_user\\_settings\u0026#34;: { \u0026#34;cli\\_username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;root\\_password\u0026#34;:\u0026#34;YourPasswordHere\u0026#34;, \u0026#34;cli\\_password\u0026#34;:\u0026#34;YourPasswordHere\u0026#34; 8 9} } 10 11} Then POST it to your NSX-T manager from Postman and after a short blink, the Edge is deployed, and you have to add it as a transport node in the NSX-T manager. Here it is important that you do this right at once, because (as I found out) this is a one-time config GUI where the first time you will be able to choose the right fp-eth nics. If you try to edit the edge deployment a second time it switches back to only showing the VDS/VSS portgroups. Then you have to redeploy.\nExample screenshots:\nRemember that the Uplink VLANs will belong to their own N-VDS (which you have already defined in their respective Transport Zone) which will not be created on the host, but the Edges. The first N-VDS are already in place on the Hosts. Its only the last two NICs which will be on their own Edge N-VDS switches.\nI am not saying this is a best practice or the right way to do this, but it works in my lab environment so I can fully test out the latest NSX-T 2.4 and continue playing with PKS (when we get CNI plugins for NSX-T 2.4... are we there yet? are we there yet, are we there yet..... its hard to wait ;-) )\n","link":"https://yikes.guzware.net/2019/03/09/deploy-nsx-t-2.4-edge-nodes-on-a-n-vds-logical-switch/","section":"post","tags":["configurations"],"title":"Deploy NSX-T 2.4 Edge Nodes on a N-VDS Logical Switch"},{"body":"If you have missing objects in the firewall section before upgrading from NSX-T 2.1 to 2.2 you will experience a General Error in the GUI, on the Dashboard, and in the Firewall section of the GUI. You will even get general error when doing API calls to list the DFW sections https://NSXMGRIP/api/v1/firewall/sections: { \u0026quot;module\\_name\u0026quot; : \u0026quot;common-services\u0026quot;, \u0026quot;error\\_message\u0026quot; : \u0026quot;General error has occurred.\u0026quot;, \u0026quot;details\u0026quot; : \u0026quot;java.lang.NullPointerException\u0026quot;, \u0026quot;error\\_code\u0026quot; : \u0026quot;100\u0026quot; }\nIf you have upgraded the fix is straight forward. Go to the following KB and dowload the attached jar file.\nUpload this jar file to the NSX-T manager by logging in with root and do a scp command from where you downloaded it. ex: \u0026quot;scp your\\_username@remotehost:nsx-firewall-1.0.jar /tmp\u0026quot;\nThen replace the existing file with the one from the kb article placed here: /opt/vmware/proton-tomcat/webapps/nsxapi/WEB-INF/lib#\nReboot\n","link":"https://yikes.guzware.net/2018/09/05/general-error-nsx-t-manager-firewall-section/","section":"post","tags":["issues","troubleshooting"],"title":"\"General Error\" NSX-T Manager Firewall Section"},{"body":"","link":"https://yikes.guzware.net/tags/issues/","section":"tags","tags":null,"title":"issues"},{"body":"","link":"https://yikes.guzware.net/tags/troubleshooting/","section":"tags","tags":null,"title":"troubleshooting"},{"body":"","link":"https://yikes.guzware.net/tags/fails/","section":"tags","tags":null,"title":"fails"},{"body":"","link":"https://yikes.guzware.net/categories/vsphere/","section":"categories","tags":null,"title":"vSphere"},{"body":"After upgrading vCenter (VCSA) to 6.0 U3 and upgrading the vSphere Replication Appliance to 6.1.2 the plugin in vCenter stops working. Follow this KB\nAnd this KB to enable SSH (for those who are unfamiliar with how to enable SSH in a *nix environment):\n","link":"https://yikes.guzware.net/2017/04/10/vsphere-replication-6.1.2-vcenter-plugin-fails-after-upgrade-vcenter-6.0-u3/","section":"post","tags":["troubleshooting","issues","fails"],"title":"vSphere Replication 6.1.2 vCenter plugin fails after upgrade (vCenter 6.0 U3)"},{"body":"Lead Solution Engineer @VMware. Been working as an IT consultant since 2004. Where my key areas were delivering professional services on VMware solutions specializing in VMware vSphere, VMware NSX, VSAN, DR-solutions and Hybrid-Cloud. I think I have touched all products coming from VMware since 2003. Some of my certifications are VCIX-DCV, VCIX-NV.\nAreas of interest: Virtulization, Neworking, Security, Linux, OpenSource solutions, BSD, DiY projects, home-automation etc..\nI created this blog page for many years ago where one of the purposes was to use it as my \u0026quot;knowledge\u0026quot; base for different stuff I deploy in my lab. It primarily focuses around my line of work (solutions such as VMware NSX and Tanzu), but will also add other topics out of work scope (different DiY projects, home automation).\n* Wikis, howtos, tips/trick and troubleshooting issues mainly within my own field of competence, as a contribution back to the community for all the help they have given me over the years. Which has been, and still is, very appreciated. Sharing is actually caring.\nAll views expressed on this site is solely my own personal views, not my employer or partners.\n","link":"https://yikes.guzware.net/about/","section":"","tags":null,"title":"About"},{"body":"Lead Solution Engineer @VMware. Been working as an IT consultant since 2004. Where my key areas were delivering professional services on VMware solutions specializing in VMware vSphere, VMware NSX, VSAN, DR-solutions and Hybrid-Cloud. I think I have touched all products coming from VMware since 2003. Some of my certifications are VCIX-DCV, VCIX-NV.\nAreas of interest: Virtulization, Neworking, Security, Linux, OpenSource solutions, BSD, DiY projects, home-automation etc..\nI created this blog page for many years ago where one of the purposes was to use it as my \u0026quot;knowledge\u0026quot; base for different stuff I deploy in my lab. It primarily focuses around my line of work (solutions such as VMware NSX and Tanzu), but will also add other topics out of work scope (different DiY projects, home automation).\n* Wikis, howtos, tips/trick and troubleshooting issues mainly within my own field of competence, as a contribution back to the community for all the help they have given me over the years. Which has been, and still is, very appreciated. Sharing is actually caring.\nAll views expressed on this site is solely my own personal views, not my employer or partners.\n","link":"https://yikes.guzware.net/page/2016-07-14-about/","section":"page","tags":null,"title":"About"},{"body":"","link":"https://yikes.guzware.net/page/","section":"page","tags":null,"title":"Pages"},{"body":"","link":"https://yikes.guzware.net/search/","section":"","tags":null,"title":"Search"},{"body":"","link":"https://yikes.guzware.net/series/","section":"series","tags":null,"title":"Series"}]